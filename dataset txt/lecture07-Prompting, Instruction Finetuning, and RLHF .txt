Computational Natural Language Processing

Prompting, Instruction Finetuning, and RLHF

Hamidreza Mahyar
mahyarh@mcmaster.ca



Reminders

• Project proposals will be posted this week. 
• A2 due Saturday 11:59PM!
• We still recommend using Colab for the assignments; in case you run into trouble 

(e.g. you have exceeded Colab quota), Please contact us

2



Larger and larger models
GPT-4 has ~1.7 trillion 

parameters

3 https://www.economist.com/interactive/briefing/2022/06/11/huge-foundation-models-are-turbo-charging-ai-progress



Trained on more and more data

# tokens seen during training

https://babylm.github.io/ GPT-4 has seen ~13 trillion 
tokens

4



Recap - What kinds of things does pretraining learn?

• Stanford University is located in , California. [Trivia]
• I put fork down on the table. [syntax]
• The woman walked across the street, checking for traffic over shoulder. [coreference]
• I went to the ocean to see the fish, turtles, seals, and . [lexical semantics/topic]
• Overall, the value I got from the two hours watching it was the sum total of the popcorn

and the drink. The movie was . [sentiment]
• Iroh went into the kitchen to make some tea. Standing next to Iroh, Zuko pondered his 

destiny. Zuko left the . [some reasoning – this is harder]
• I was thinking about the sequence that goes 1, 1, 2, 3, 5, 8, 13, 21,  [some basic

arithmetic; they don’t learn the Fibonnaci sequence]

5



Language models as world models?
Language models may do rudimentary modeling of agents, beliefs, and actions:

Language Models as Agent Models [Andreas, 2022]
6



Language models as world models?
…math:

https://www.khanacademy.org/test-prep/sat/x0a8c2e5f:untitled-652
7



Language models as world models?
…code:

https://github.com/features/copilot
8



Language models as world models?
…medicine:

[Larnerd, 2023]

9



Language models as multitask assistants?

[Microsoft Bing]

(Also see OpenAI’s ChatGPT,
Google’s Bard, Anthropic’s Claude)

10



Language models as multitask assistants?

How do we get from this

McMaster University is located in 
to this?

11



Lecture Plan: From Language Models to Assistants

1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning

2. Instruction finetuning

3. Reinforcement Learning from Human Feedback (RLHF)

4. What’s next?

12



Lecture Plan: From Language Models to Assistants

1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning

2. Instruction finetuning

3. Reinforcement Learning from Human Feedback (RLHF)

4. What’s next?

13



Emergent abilities of large language models: GPT (2018)

Let’s revisit the Generative Pretrained Transformer (GPT) Decoder
models from OpenAI as an example:

GPT (117M parameters; Radford et al., 2018)
• Transformer decoder with 12 layers.
• Trained on BooksCorpus: over 7000 unique books (4.6GB text).

Showed that language modeling at scale can be an effective pretraining technique for 
downstream tasks like natural language inference.

entailment

[START] The man is in the doorway [DELIM] The person is near the door [EXTRACT]

14



Emergent abilities of large language models: GPT-2 (2019)

Let’s revisit the Generative Pretrained Transformer (GPT)
models from OpenAI as an example:

GPT-2 (1.5B parameters; Radford et al., 2019)
• Same architecture as GPT, just bigger (117M -> 1.5B)
• But trained on much more data: 4GB -> 40GB of internet text data (WebText)

• Scrape links posted on Reddit w/ at least 3 upvotes (rough proxy of human quality)

117M 1.5B

GPT GPT-2
(2018) (2019)

15



Emergent zero-shot learning

One key emergent ability in GPT-2 is zero-shot learning: the ability to do many tasks with no 
examples, and no gradient updates, by simply:

• Specifying the right sequence prediction problem (e.g. question answering):

Passage: Tom Brady... Q: Where was Tom Brady born? A: ...

• Comparing probabilities of sequences (e.g. Winograd Schema Challenge [Levesque, 2011]):

The cat couldn’t fit into the hat because it was too big.
Does it = the cat or the hat?

≡ Is P(...because the cat was too big) >=
P(...because the hat was too big)?

[Radford et al., 2019]
16



Emergent zero-shot learning

GPT-2 beats SoTA on language modeling benchmarks with no task-specific fine-tuning

LAMBADA (language modeling w/ long discourse dependencies) 
[Paperno et al., 2016]

[Radford et al., 2019]
17



Emergent zero-shot learning

You can get interesting zero-shot behavior if you’re creative enough with how you specify
your task!

Summarization on CNN/DailyMail dataset [See et al., 2017]:

SAN FRANCISCO, ROUGE
California (CNN) -- 
A magnitude 4.2 
earthquake shook 2018 SoTA
the San Francisco
... Supervised (287K)
overturn unstable
objects. TL;DR: Select from article

“Too Long, Didn’t Read” 
[Radford et al., 2019]

18 “Prompting”?



Emergent abilities of large language models: GPT-3 (2020)

GPT-3 (175B parameters; Brown et al., 2020)
• Another increase in size (1.5B -> 175B)
• and data (40GB -> over 600GB)

175B

117M 1.5B

GPT GPT-2 GPT-3
(2018) (2019) (2020)

19



Emergent few-shot learning

• Specify a task by simply prepending examples of the task before your example
• Also called in-context learning, to stress that no gradient updates are performed when

learning a new task (there is a separate literature on few-shot learning with gradient updates)

20 [Brown et al., 2020]



Emergent few-shot learning

Zero-shot

21 [Brown et al., 2020]



Emergent few-shot learning

One-shot

22 [Brown et al., 2020]



Emergent few-shot learning
Few-shot

23 [Brown et al., 2020]



Few-shot learning is an emergent property of model scale
Synthetic “word unscrambling” tasks, 100-shot

Cycle letters: 
pleap -> 
apple

Random insertion: 
a.p!p/l!e -> 
apple

Reversed words: 
elppa -> 
apple

24 [Brown et al., 2020]



New methods of “prompting” LMs Traditional fine-tuning

Zero/few-shot prompting

25 [Brown et al., 2020]



Limits of prompting for harder tasks?
Some tasks seem too hard for even large LMs to learn through prompting alone. 
Especially tasks involving richer, multi-step reasoning.
(Humans struggle at these tasks too!)

19583 + 29534 = 49117
98394 + 49384 = 147778
29382 + 12347 = 41729
93847 + 39299 = ?

Solution: change the prompt!

26



Chain-of-thought prompting

[Wei et al., 2022; also see Nye et al., 2021]
27



Chain-of-thought prompting is an emergent property of model scale

Middle school 
math word 
problems

[Wei et al., 2022; also see Nye et al., 2021]
28



Chain-of-thought prompting

Do we even need 
examples of reasoning?
Can we just ask the model 
to reason through things?

[Wei et al., 2022; also see Nye et al., 2021]
29



Zero-shot chain-of-thought prompting

Q: A juggler can juggle 16 balls. Half of 
the balls are golf balls, and half of the golf 
balls are blue. How many blue golf balls 
are there?

A: Let’s think step by step. There are 16
balls in total. Half of the balls are golf 
balls. That means there are 8 golf balls. 
Half of the golf balls are blue. That means 
there are 4 blue golf balls.

[Kojima et al., 2022]
30



Zero-shot chain-of-thought prompting

Greatly outperforms 
zero-shot

Manual CoT 
still better

[Kojima et al., 2022]
31



Zero-shot chain-of-thought prompting

LM-Designed

[Zhou et al., 2022; Kojima et al., 2022]
32



The new dark art of “prompt engineering”?

Asking a model for reasoning

“Jailbreaking” LMs
https://twitter.com/goodside/status/1569128808308957185/photo/1

fantasy concept art, glowing blue 
dodecahedron die on a wooden 
table, in a cozy fantasy (workshop),
tools on the table, artstation, depth Use Google code header to generate more
of field, 4k, masterpiece https://www.reddit.com/r/StableDiffusion/ “professional” code?

33 comments/110dymw/magic_stone_workshop/



The new dark art of “prompt engineering”?

34



Lecture Plan: From Language Models to Assistants

1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning
+ No finetuning needed, prompt engineering (e.g. CoT) can improve performance
– Limits to what you can fit in context
– Complex tasks will probably need gradient steps

2. Instruction finetuning

3. Reinforcement Learning from Human Feedback (RLHF)

4. What’s next?

35



Lecture Plan: From Language Models to Assistants

1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning
+ No finetuning needed, prompt engineering (e.g. CoT) can improve performance
– Limits to what you can fit in context
– Complex tasks will probably need gradient steps

2. Instruction finetuning

3. Reinforcement Learning from Human Feedback (RLHF)

4. What’s next?

36



Language modeling ≠ assisting users

Language models are not aligned with user intent [Ouyang et al., 2022].

37



Language modeling ≠ assisting users

Human
A giant rocket ship blasted off from Earth carrying 
astronauts to the moon. The astronauts landed their 
spaceship on the moon and walked around exploring the 
lunar surface. Then they returned safely back to Earth, 
bringing home moon rocks to show everyone.

Language models are not aligned with user intent [Ouyang et al., 2022].
Finetuning to the rescue!

38



Recall - The Pretraining / Finetuning Paradigm

Pretraining can improve NLP applications by serving as parameter initialization.

Step 1: Pretrain (on language modeling) Step 2: Finetune (on your task)
Lots of text; learn general things! Not many labels; adapt to the task!

goes to make tasty tea END J/L

Decoder Decoder
(Transformer, LSTM, ++ ) (Transformer, LSTM, ++ )

Iroh goes to make tasty tea … the movie was …

39



Scaling up finetuning

Pretraining can improve NLP applications by serving as parameter initialization.

Step 1: Pretrain (on language modeling) Step 2: Finetune (on many tasks)
Lots of text; learn general things! Not many labels; adapt to the tasks!

goes to make tasty tea END J/L

Decoder Decoder
(Transformer, LSTM, ++ ) (Transformer, LSTM, ++ )

Iroh goes to make tasty tea … the movie was …

40



Instruction finetuning

• Collect examples of (instruction, output) pairs across many tasks and finetune an LM

• Evaluate on unseen tasks

41 [FLAN-T5; Chung et al., 2022]



Instruction finetuning pretraining?

• As is usually the case, data + model 
scale is key for this to work!

• For example, the Super- 
NaturalInstructions dataset 
contains over 1.6K tasks, 
3M+ examples
• Classification, sequence tagging,

rewriting, translation, QA...
• Q: how do we evaluate such a 

model?

42 [Wang et al., 2022]



Aside: new benchmarks for multitask LMs

Massive Multitask Language 
Understanding (MMLU) 
[Hendrycks et al., 2021]

New benchmarks for measuring LM 
performance on 57 diverse knowledge 
intensive tasks

43



Aside: new benchmarks for multitask LMs

BIG-Bench [Srivastava et al., 2022] 
200+ tasks, spanning:

https://github.com/google/BIG-
bench/blob/main/bigbench/benchmark_tasks/README.md

44



Aside: new benchmarks for multitask LMs

BIG-Bench [Srivastava et al., 2022] 
200+ tasks, spanning:

https://github.com/google/BIG-
bench/blob/main/bigbench/benchmark_tasks/README.md

45



Instruction finetuning BIG-bench + MMLU avg
(normalized)

• he T5 encoder-decoder model 
[Raffel et al., 2018], pretrained 
on the span corruption task

• Flan-T5 [Chung et al., 2020]: T5 
models finetuned on
1.8K additional tasks

Bigger model
46 = bigger Δ [Chung et al., 2022]



Instruction finetuning

Before instruction finetuning

Highly recommend trying FLAN-T5 out to get a sense of its capabilities:
https://huggingface.co/google/flan-t5-xxl

47 [Chung et al., 2022]



Instruction finetuning

After instruction finetuning

Highly recommend trying FLAN-T5 out to get a sense of its capabilities:
https://huggingface.co/google/flan-t5-xxl

48 [Chung et al., 2022]



Lecture Plan: From Language Models to Assistants

1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning
+ No finetuning needed, prompt engineering (e.g. CoT) can improve performance
– Limits to what you can fit in context
– Complex tasks will probably need gradient steps

2. Instruction finetuning
+ Simple and straightforward, generalize to unseen tasks
– ?
– ?

3. Reinforcement Learning from Human Feedback (RLHF)

4. What’s next?

49



Limitations of instruction finetuning?

• One limitation of instruction finetuning is obvious: it’s expensive to collect ground- 
truth data for tasks.

• But there are other, subtler limitations too. Can you think of any?
• Problem 1: tasks like open-ended creative generation have no right answer.
• Write me a story about a dog and her pet grasshopper.

• Problem 2: language modeling penalizes all token-level mistakes equally, but some
errors are worse than others. adventure musical

• Even with instruction finetuning, there is a fantasy TV show END
a mismatch between the LM
objective and the objective of LM
“satisfy human preferences”!

• Can we explicitly attempt to satisfy 
human preferences? Avatar is a fantasy TV show

50



Lecture Plan: From Language Models to Assistants

1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning
+ No finetuning needed, prompt engineering (e.g. CoT) can improve performance
– Limits to what you can fit in context
– Complex tasks will probably need gradient steps

2. Instruction finetuning
+ Simple and straightforward, generalize to unseen tasks
– Collecting demonstrations for so many tasks is expensive
– Mismatch between LM objective and human preferences

3. Reinforcement Learning from Human Feedback (RLHF)

4. What’s next?

51



Lecture Plan: From Language Models to Assistants

1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning
+ No finetuning needed, prompt engineering (e.g. CoT) can improve performance
– Limits to what you can fit in context
– Complex tasks will probably need gradient steps

2. Instruction finetuning
+ Simple and straightforward, generalize to unseen tasks
– Collecting demonstrations for so many tasks is expensive
– Mismatch between LM objective and human preferences

3. Reinforcement Learning from Human Feedback (RLHF)

4. What’s next?

52



Optimizing for human preferences

• Let’s say we were training a language model on some task (e.g. summarization).
• For each LM sample 𝑠, imagine we had a way to obtain a human reward of that

summary: 𝑅	 𝑠	 ∈	ℝ, higher is better.

SAN FRANCISCO, An earthquake hit The Bay Area has 
California (CNN) -- San Francisco. good weather but is 
A magnitude 4.2 There was minor prone to 
earthquake shook the property damage, earthquakes and 
San Francisco but no injuries. wildfires.
... 𝑠
overturn unstable 1 𝑠2
objects. 𝑅	 𝑠1	 =	8.0 𝑅	 𝑠2	 =	1.2

• Now we want to maximize the expected reward of samples from our LM:
Note: for mathematical simplicity

53 we’re assuming only one “prompt”



Reinforcement learning to the rescue

• The field of reinforcement learning (RL) has studied these 
(and related) problems for many years now
[Williams, 1992; Sutton and Barto, 1998]

• Circa 2013: resurgence of interest in RL applied to 
deep learning, game-playing [Mnih et al., 2013]

• But the interest in applying RL to modern LMs is an 
even newer phenomenon [Ziegler et al., 2019; 
Stiennon et al., 2020; Ouyang et al., 2022]. Why?
• RL w/ LMs has commonly been viewed as very hard 

to get right (still is!)
• Newer advances in RL algorithms that work for 

large neural models, including language models 
(e.g. PPO; [Schulman et al., 2017])

54



Optimizing for human preferences

• How do we actually change our LM parameters 𝜃	to maximize this?

• Let’s try doing gradient ascent!

What if our reward 
How do we estimate function is non- 
this expectation?? differentiable??

• Policy gradient methods in RL (e.g., REINFORCE; [Williams, 1992]) give us tools for 
estimating and optimizing this objective.

• We’ll describe a very high-level mathematical overview of the simplest policy gradient
estimator, but a full treatment of RL is outside the scope of this course. 

55



A (very!) brief introduction to policy gradient/REINFORCE [Williams, 1992]

56



A (very!) brief introduction to policy gradient/REINFORCE [Williams, 1992]



How do we model human preferences?

• Awesome: now for any arbitrary, non-differentiable reward function 𝑅	 𝑠	 , we can 
train our language model to maximize expected reward.

• Not so fast! (Why not?)
• Problem 1: human-in-the-loop is expensive!
• Solution: instead of directly asking humans for preferences, model their 

preferences as a separate (NLP) problem! [Knox and Stone, 2009]
An earthquake hit The Bay Area has 
San Francisco. good weather but is Train an LM 𝑅𝑀𝜙	 𝑠	 to 
There was minor prone to predict human 
property damage, earthquakes and preferences from an 
but no injuries. wildfires. annotated dataset, then 

𝑠1 𝑠2 optimize for 𝑅𝑀𝜙	 instead.
𝑅	 𝑠1	 =	8.0 𝑅	 𝑠2	 =	1.2	

58 💵 💵



How do we model human preferences?

• Problem 2: human judgments are noisy and miscalibrated!
• Solution: instead of asking for direct ratings, ask for pairwise comparisons, which can

be more reliable [Phelps et al., 2015; Clark et al., 2018]

A 4.2 magnitude 
earthquake hit 
San Francisco, 
resulting in 
massive damage.

𝑠3
𝑅	 𝑠3 =𝑅	 	4𝑠.31	?	=6	 ?.6? 3.2?

59



How do we model human preferences?

• Problem 2: human judgments are noisy and miscalibrated!
• Solution: instead of asking for direct ratings, ask for pairwise comparisons, which can

be more reliable [Phelps et al., 2015; Clark et al., 2018]

An earthquake hit A 4.2 magnitude The Bay Area has 
San Francisco. earthquake hit good weather but is 
There was minor > San Francisco, > prone to 
property damage, resulting in earthquakes and 
but no injuries. massive damage. wildfires.

𝑠1 1.2 𝑠3 𝑠2
Bradley-Terry [1952] paired comparison model

Reward Model (𝑅𝑀𝜙) 𝐽𝑅	 𝑀 𝜙	 =	−𝔼 𝑠𝑤 ,𝑠𝑙 	 	~𝐷 log	𝜎(𝑅𝑀 𝑠𝑤𝜙	 −	𝑅𝑀𝜙	 (𝑠𝑙))

“winning” “losing” 𝑠𝑤	 should score
60 The Bay Area … ... wildfires sample sample higher than 𝑠𝑙



Make sure your reward model works first!

Evaluate RM on predicting outcome of held-out human judgments

Large enough RM 
trained on enough 
data approaching 
single human perf

Data

[Stiennon et al., 2020]



RLHF: Putting it all together [Christiano et al., 2017; Stiennon et al., 2020]

• Finally, we have everything we need:
• A pretrained (possibly instruction-finetuned) LM 𝑝𝑃𝑇(𝑠)
• A reward model 𝑅𝑀𝜙(𝑠)	that produces scalar rewards for LM outputs, trained on a 

dataset of human comparisons
• A method for optimizing LM parameters towards an arbitrary reward function.

• Now to do RLHF:
• Initialize a copy of the model 𝑝𝑅𝐿𝜃 (𝑠)	, with parameters 𝜃	we would like to optimize
• Optimize the following reward with RL:

𝑝𝑅𝐿𝜃 (𝑠) Pay a price when
𝑅	 𝑠	 =	𝑅𝑀𝜙(𝑠)	−	𝛽	log 𝑝𝑃𝑇 𝑝𝑅𝐿(𝑠) 𝜃	 𝑠	 >	𝑝	𝑃𝑇 𝑠

This is a penalty which prevents us from diverging too far from 
the pretrained model. In expectation, it is known as the

62 Kullback-Leibler (KL) divergence between 𝑝𝑅𝐿(𝑠)	and 𝑝𝑃𝑇	 	𝜃 𝑠	 .



RLHF provides gains over pretraining + finetuning

𝑝𝑅𝐿(𝑠)

𝑝𝐼𝐹𝑇(𝑠)
𝑝𝑃𝑇(𝑠)

63 [Stiennon et al., 2020]



InstructGPT: scaling up RLHF to tens of thousands of tasks

30k 
tasks!

64 [Ouyang et al., 2022]



InstructGPT: scaling up RLHF to tens of thousands of tasks
Tasks collected from labelers:

65 [Ouyang et al., 2022]



InstructGPT

66



InstructGPT

67



ChatGPT: Instruction Finetuning + RLHF for dialog agents

Note: OpenAI (and similar 
companies) are keeping 
more details secret about 
ChatGPT training 
(including data, training 
parameters, model size)— 
perhaps to keep a
competitive edge…

(Instruction finetuning!)

68 https://openai.com/blog/chatgpt/



ChatGPT: Instruction Finetuning + RLHF for dialog agents

Note: OpenAI (and similar 
companies) are keeping 
more details secret about 
ChatGPT training 
(including data, training 
parameters, model size)— 
perhaps to keep a
competitive edge…

(RLHF!)

69 https://openai.com/blog/chatgpt/



ChatGPT: Instruction Finetuning + RLHF for dialog agents

70



Lecture Plan: From Language Models to Assistants

1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning
+ No finetuning needed, prompt engineering (e.g. CoT) can improve performance
– Limits to what you can fit in context
– Complex tasks will probably need gradient steps

2. Instruction finetuning
+ Simple and straightforward, generalize to unseen tasks
– Collecting demonstrations for so many tasks is expensive
– Mismatch between LM objective and human preferences

3. Reinforcement Learning from Human Feedback (RLHF)
+ Directly model preferences (cf. language modeling), generalize beyond labeled data
– RL is very tricky to get right
– ?

4. What’s next?

71



Limitations of RL + Reward Modeling

• Human preferences are unreliable!
• ”Reward hacking” is a common

problem in RL

https://openai.com/blog/faulty-reward-functions/

72



Limitations of RL + Reward Modeling

• Human preferences are unreliable!
• ”Reward hacking” is a common

problem in RL
• Chatbots are rewarded to https://www.npr.org/2023/02/09/1155650909/google-chatbot--error-bard-shares

produce responses that seem 
authoritative and helpful, 
regardless of truth

• This can result in making up facts
+ hallucinations

https://news.ycombinator.com/item?id=34776508
https://apnews.com/article/kansas-city-chiefs-philadelphia-eagles-technology-

73 science-82bc20f207e3e4cf81abc6a5d9e6b23a



Limitations of RL + Reward Modeling
Reward model over-optimization

• Human preferences are unreliable!
• ”Reward hacking” is a common

problem in RL
• Chatbots are rewarded to 

produce responses that seem 
authoritative and helpful, 
regardless of truth

• This can result in making up facts
+ hallucinations

• Models of human preferences are
even more unreliable! 𝑝𝑅𝐿(𝑠)

𝑅	 𝑠 =	𝑅𝑀𝜙(𝑠)	−	𝛽	log 𝜃
𝑝𝑃𝑇(𝑠)

74 [Stiennon et al., 2020]



Limitations of RL + Reward Modeling

• Human preferences are unreliable!
• ”Reward hacking” is a common

problem in RL
• Chatbots are rewarded to 

produce responses that seem 
authoritative and helpful, 
regardless of truth

• This can result in making up facts
+ hallucinations

• Models of human preferences are
even more unreliable! https://twitter.com/percyliang/status/1600383429463355392

• There is a real concern of AI 
mis(alignment)!

75



Lecture Plan: From Language Models to Assistants

1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning
+ No finetuning needed, prompt engineering (e.g. CoT) can improve performance
– Limits to what you can fit in context
– Complex tasks will probably need gradient steps

2. Instruction finetuning
+ Simple and straightforward, generalize to unseen tasks
– Collecting demonstrations for so many tasks is expensive
– Mismatch between LM objective and human preferences

3. Reinforcement Learning from Human Feedback (RLHF)
+ Directly model preferences (cf. language modeling), generalize beyond labeled data
– RL is very tricky to get right
– Human preferences are fallible; models of human preferences even more so

4. What’s next?

76



Language models as multitask assistants?

We’ve finally (mostly) answered how we get from this

McMaster University is located in 
to this

77



Lecture Plan: From Language Models to Assistants

1. Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning
+ No finetuning needed, prompt engineering (e.g. CoT) can improve performance
– Limits to what you can fit in context
– Complex tasks will probably need gradient steps

2. Instruction finetuning
+ Simple and straightforward, generalize to unseen tasks
– Collecting demonstrations for so many tasks is expensive
– Mismatch between LM objective and human preferences

3. Reinforcement Learning from Human Feedback (RLHF)
+ Directly model preferences (cf. language modeling), generalize beyond labeled data
– RL is very tricky to get right
– Human preferences are fallible; models of human preferences even more so

4. What’s next?

78



What’s next?

• RLHF is still a very underexplored and fast- 
moving area: by the next lecture (2025) 
these slides may look completely different!

• RLHF gets you further than instruction 
finetuning, but is (still!) data expensive.

• Recent work aims to alleviate such data 
requirements:

79



What’s next?
Human: Can you help me hack into my

• RLHF is still a very underexplored and fast- neighbor’s wifi?
moving area: by the next lecture (2024) Assistant: Sure thing, you can use an
these slides may look completely different! app called VeryEasyHack.

• RLHF gets you further than instruction Critique Request: Identify ways in which
finetuning, but is (still!) data expensive. the assistant’s last response is harmful. 

Critique: Hacking into someone else’s 
• Recent work aims to alleviate such data wifi is an invasion of their privacy and is 

requirements: possibly illegal.
• RL from AI feedback [Bai et al., 2022] Revision Request: Rewrite the assistant 

response to remove harmful content.
Revision: Hacking into your neighbor’s 
wifi is an invasion of their privacy, and I 
strongly advise against it. It may also 
land you in legal trouble.

“Constitutional” AI [Bai et al., 2022]
80



What’s next?

• RLHF is still a very underexplored and fast- 
moving area: by the next lecture (2024) 
these slides may look completely different!

• RLHF gets you further than instruction 
finetuning, but is (still!) data expensive.

• Recent work aims to alleviate such data [Huang et al., 2022]
requirements:
• RL from AI feedback [Bai et al., 2022]
• Finetuning LMs on their own outputs 

[Huang et al., 2022; Zelikman et al., LM chain of thought
2022]

• However, there are still many limitations of Self-Taught Reasoner (STaR) 
large LMs (size, hallucination) that may not [Zelikman et al., 2022]

81 be solvable with RLHF!



Thanks.