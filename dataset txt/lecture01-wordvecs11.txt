Computational Natural Language Processing

Overview of NLP and Word Vectors

Hamidreza Mahyar
mahyarh@mcmaster.ca



Lecture Plan

Lecture 1: Introduction and Word Vectors

1. The course
2. Human language and word meaning 
3. Word2vec introduction 
4. Word2vec objective function gradients 
5. Optimization basics 
6. Looking at word vectors 

Key learning today: The (astounding!) result that word meaning can be represented rather
well by a (high-dimensional) vector of real numbers

2



Course logistics in brief

• Instructor: Hamidreza Mahyar
• Head TA: Ali Shiraee
• Time: Wednesday 11:30am–2:30pm
• We put a lot of other important information on Avenue to Learn. Please read it!

3



What do we hope to teach? (A.k.a. “learning goals”)

1. The foundations of the effective modern methods for deep learning applied to NLP
• Basics first, then key methods used in NLP in 2024: Word vectors, feed-forward 

networks, recurrent networks, attention, encoder-decoder models, transformers, 
large pre-trained language models, etc.

2. A big picture understanding of human languages and the difficulties in understanding
and producing them via computers

3. An understanding of and ability to build systems (in PyTorch) for some of the major 
problems in NLP:
• Word meaning, dependency parsing, machine translation, question answering

4



Course work and grading policy

• 4 Assignments: 20%
• 4 Quizzes: 10%
• Midterm: 35%
• Final Group Project (1–3 people): 34%
• Late day policy

• NO free late days; 1% off course grade per day late
• Assignments not accepted more than 3 days late unless given permission in advance

• Collaboration policy: Understand allowed collaboration and how to document it: Don’t 
take code off the web; acknowledge working with other students; write your own 
assignment solutions

5









Trained on text data, neural machine translation is quite good!

https://kiswahili.tuko.co.ke/



GPT-3: A first step on the path to foundation models

The SEC said, “Musk, your tweets are a S: I broke the window.

blight. Q: What did I break?
S: I gracefully saved the day. 

They really could cost you your job,
Q: What did I gracefully save? 

if you don't stop all this tweeting at night.”
S: I gave John flowers.

Then Musk cried, “Why? Q: Who did I give flowers to?

The tweets I wrote are not mean, S: I gave her a rose and a guitar.

I don't use all-caps Q: Who did I give a rose and a guitar to?

and I'm sure that my tweets are clean.” How many users have signed up since the start of 2020?

“But your tweets can move markets SELECT count(id) FROM users 

and that's why we're sore. WHERE created_at > ‘2020-01-01’
What is the average number of influencers each user is 

You may be a genius and a billionaire, subscribed to?
but it doesn't give you the right to SELECT avg(count) FROM ( SELECT user_id, count(*) 
be a bore!” FROM subscribers GROUP BY user_id )

AS avg_subscriptions_per_user



ChatGPT: A recent, intriguing set of capabilities



ChatGPT: A recent, intriguing set of capabilities



ChatGPT: A recent, intriguing set of capabilities



How do we represent the meaning of a word?

Definition: meaning (Webster dictionary)
• the idea that is represented by a word, phrase, etc.
• the idea that a person wants to express by using words, signs, etc.
• the idea that is expressed in a work of writing, art, etc.

Commonest linguistic way of thinking of meaning:

signifier (symbol) ⟺ signified (idea or thing)

= denotational semantics

tree ⟺ { , , , …}
13



How do we have usable meaning in a computer?

Previously commonest NLP solution: Use, e.g., WordNet, a thesaurus containing lists of
synonym sets and hypernyms (“is a” relationships)

e.g., synonym sets containing “good”: e.g., hypernyms of “panda”:
from nltk.corpus import wordnet as wn from nltk.corpus import wordnet as wn 
poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'} for synset in 

panda = wn.synset("panda.n.01")
wn.synsets("good"):

print("{}: {}".format(poses[synset.pos()], hyper = lambda s: s.hypernyms() 
", ".join([l.name() for l in synset.lemmas()]))) list(panda.closure(hyper))

noun: good
[Synset('procyonid.n.01'), 

noun: good, goodness 
noun: good, goodness Synset('carnivore.n.01'), 

Synset('placental.n.01'), 
noun: commodity, trade_good, good adj: 
good Synset('mammal.n.01'), 

adj (sat): full, good adj: Synset('vertebrate.n.01'), 
Synset('chordate.n.01'), 

good
adj (sat): estimable, good, honorable, respectable Synset('animal.n.01'), 

Synset('organism.n.01'), 
adj (sat): beneficial, good adj 
(sat): good Synset('living_thing.n.01'), 

adj (sat): good, just, upright Synset('whole.n.02'),
Synset('object.n.01'), 

…
adverb: well, good Synset('physical_entity.n.01'), 

Synset('entity.n.01')]
adverb: thoroughly, soundly, good

14



Problems with resources like WordNet

• A useful resource but missing nuance:
• e.g., “proficient” is listed as a synonym for “good”

This is only correct in some contexts
• Also, WordNet list offensive synonyms in some synonym sets without any 

coverage of the connotations or appropriateness of words
• Missing new meanings of words:

• e.g., wicked, badass, nifty, wizard, genius, ninja, bombest
• Impossible to keep up-to-date!

• Subjective
• Requires human labor to create and adapt
• Can’t be used to accurately compute word similarity (see following slides)

15



Representing words as discrete symbols

In traditional NLP, we regard words as discrete symbols:
hotel, conference, motel – a localist representation

Means one 1, the rest 0s

Such symbols for words can be represented by one-hot vectors:

motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]

hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]

Vector dimension = number of words in vocabulary (e.g., 500,000+)

16



Sec. 9.2.2

Problem with words as discrete symbols

Example: in web search, if a user searches for “Seattle motel”, we would like to match 
documents containing “Seattle hotel”

But:
motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]
hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]

These two vectors are orthogonal
There is no natural notion of similarity for one-hot vectors!

Solution:
• Could try to rely on WordNet’s list of synonyms to get similarity?

• But it is well-known to fail badly: incompleteness, etc.
• Instead: learn to encode similarity in the vectors themselves
17



Representing words by their context

• Distributional semantics: A word’s meaning is given
by the words that frequently appear close-by

• “You shall know a word by the company it keeps” (J. R. Firth 1957: 11)

• One of the most successful ideas of modern statistical NLP!

• When a word w appears in a text, its context is the set of words that appear nearby 
(within a fixed-size window).

• We use the many contexts of w to build up a representation of w

…government debt problems turning into banking crises as happened in 2009…

…saying that Europe needs unified banking regulation to replace the hodgepodge…

…India has just given its banking system a shot in the arm…

21 These context words will represent banking



Word vectors

We will build a dense vector for each word, chosen so that it is similar to vectors of words 
that appear in similar contexts, measuring similarity as the vector dot (scalar) product

0.286 0.413
0.792 0.582

−0.177 −0.007
banking = −0.107 monetary = 0.247

0.109 0.216
−0.542 −0.718

0.349 0.147
0.271 0.051

Note: word vectors are also called (word) embeddings or (neural) word representations 
They are a distributed representation

19



Word meaning as a neural word vector – visualization

0.286
0.792

−0.177
−0.107

expect = 0.109
−0.542

0.349
0.271
0.487

20



3. Word2vec: Overview

Word2vec (Mikolov et al. 2013) is a framework for learning word vectors

Idea:
• We have a large corpus (“body”) of text: a long list of words
• Every word in a fixed vocabulary is represented by a vector
• Go through each position t in the text, which has a center word c and context

(“outside”) words o
• Use the similarity of the word vectors for c and o to calculate the probability of o given

c (or vice versa)
• Keep adjusting the word vectors to maximize this probability

21



Word2Vec Overview

Example windows and process for computing 𝑃 𝑤𝑡+𝑗 | 𝑤𝑡

𝑃 𝑤𝑡−2 | 𝑤𝑡 𝑃 𝑤𝑡+2 | 𝑤𝑡

𝑃 𝑤𝑡−1 | 𝑤𝑡 𝑃 𝑤𝑡+1 | 𝑤𝑡

… problems turning into banking crises as …

outside context words center word outside context words 
in window of size 2 at position t in window of size 2

22



Word2Vec Overview

Example windows and process for computing 𝑃 𝑤𝑡+𝑗 | 𝑤𝑡

𝑃 𝑤𝑡−2 | 𝑤𝑡 𝑃 𝑤𝑡+2 | 𝑤𝑡

𝑃 𝑤𝑡−1 | 𝑤𝑡 𝑃 𝑤𝑡+1 | 𝑤𝑡

… problems turning into banking crises as …

outside context words center word outside context words 
in window of size 2 at position t in window of size 2

23



Word2vec: objective function

For each position 𝑡 = 1, … , 𝑇, predict context words within a window of fixed size m, 
given center word 𝑤𝑡. Data likelihood:

sometimes called a cost or loss function

The objective function 𝐽 𝜃 is the (average) negative log likelihood:

Minimizing objective function ⟺ Maximizing predictive accuracy

24



Word2vec: objective function

• We want to minimize the objective function:

• Question: How to calculate 𝑃 𝑤𝑡+𝑗 | 𝑤𝑡; 𝜃 ?

• Answer: We will use two vectors per word w:
• 𝑣𝑤 when w is a center word
• 𝑢𝑤 when w is a context word

• Then for a center word c and a context word o:

25



Word2Vec with Vectors

• Example windows and process for computing 𝑃 𝑤𝑡+𝑗 | 𝑤𝑡

• 𝑃  𝑢𝑝𝑟𝑜𝑏𝑙𝑒𝑚𝑠 | 𝑣𝑖𝑛𝑡𝑜 short for P 𝑝𝑟𝑜𝑏𝑙𝑒𝑚𝑠 | 𝑖𝑛𝑡𝑜 ; 𝑢𝑝𝑟𝑜𝑏𝑙𝑒𝑚𝑠, 𝑣𝑖𝑛𝑡𝑜, 𝜃

All words vectors 𝜃
appear in denominator

𝑃 𝑢𝑝𝑟𝑜𝑏𝑙𝑒𝑚𝑠 | 𝑣𝑖𝑛𝑡𝑜 𝑃  𝑢𝑐𝑟𝑖𝑠𝑖𝑠 |𝑣𝑖𝑛𝑡𝑜

𝑃 𝑢𝑡𝑢𝑛𝑖𝑛𝑔 | 𝑣𝑖𝑛𝑡𝑜 𝑃 𝑢𝑏𝑎𝑛𝑘𝑖𝑛𝑔  |𝑣𝑖𝑛𝑡𝑜

… problems turning into banking crises as …

outside context words center word outside context words
in window of size 2 at position t in window of size 2

26



Word2vec: prediction function

• The softmax function maps arbitrary values 𝑥𝑖 to a probability distribution 𝑝𝑖
• “max” because amplifies probability of largest 𝑥𝑖

But sort of a weird name 
• “soft” because still assigns some probability to smaller 𝑥 𝑖 because it returns a distribution!
• Frequently used in Deep Learning

27



To train the model: Optimize value of parameters to minimize loss

To train a model, we gradually adjust parameters to minimize a loss

• Recall: 𝜃 represents all the 
model parameters, in one 
long vector

• In our case, with
d-dimensional vectors and
V-many words, we have ➔

• Remember: every word has 
two vectors

• We optimize these parameters by walking down the gradient (see right figure)
• We compute all vector gradients!
28



4.

29



30



31



32



5. Optimization: Gradient Descent

• We have a cost function 𝐽 𝜃 we want to minimize
• Gradient Descent is an algorithm to minimize 𝐽 𝜃
• Idea: for current value of 𝜃, calculate gradient of 𝐽 𝜃 , then take small step in direction 

of negative gradient. Repeat.

Note: Our 
objectives 
may not 
be convex 
like this 

But life turns 
out to be 
okay ☺

33






Gradient Descent

• Update equation (in matrix notation):

𝛼 = step size or learning rate

• Update equation (for single parameter):

• Algorithm:

35



Stochastic Gradient Descent

• Problem: 𝐽 𝜃 is a function of all windows in the corpus (potentially billions!) 

• So is very expensive to compute

• You would wait a very long time before making a single update!
• Very bad idea for pretty much all neural nets!

• Solution: Stochastic gradient descent (SGD)
• Repeatedly sample windows, and update after each one

• Algorithm:

36



Word2vec maximizes objective function bc 
putting similar words nearbc in space

37



How to evaluate word vectors?
• Related to general evaluation in NLP: Intrinsic vs. extrinsic
• Intrinsic:

• Evaluation on a specific/intermediate subtask
• Fast to compute
• Helps to understand that system
• Not clear if really helpful unless correlation to real task is established

• Extrinsic:
• Evaluation on a real task
• Can take a long time to compute accuracy
• Unclear if the subsystem is the problem or its interaction or other subsystems
• If replacing exactly one subsystem with another improves accuracy → Winning!

38



Meaning similaritc: Another intrinsic word vector evaluation
• Word vector distances and their correlation with human judgments
• Example dataset: WordSim353 http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/

Word 1 Word 2 Human (mean)
tiger cat 7.35
tiger tiger 10
book paper 7.46
computer internet 7.58
plane car 5.77
professor doctor 6.62
stock phone 1.62
stock CD 1.31
stock jaguar 0.92

39



Classification review and notation
• Supervised learning: we have a training dataset consisting of samples

{xi,yi}N
i=1

• xi are inputs, e.g., words (indices or vectors!), sentences, documents, etc.
• Dimension d

• yi are labels (one of C classes) we try to predict, for example:
• classes: sentiment (+/–), named entities, buy/sell decision
• other words
• later: multi-word sequences

40



Neural classification
• Typical ML/stats softmax classifier:
• Learned parameters θ are just elements

of W (not input representation x, which has sparse symbolic features)
• Classifier gives linear decision boundary, which can be limiting

• A neural network classifier differs in that:
• We learn both W and (distributed!) representations for words

• The word vectors x re-represent one-hot vectors, moving them 
around in an intermediate layer vector space, for easy classification 
with a (linear) softmax classifier
• Conceptually, we have an embedding layer: x = Le

• We use deep networks—more layers—that let us re-represent and But typically, it is linear

compose our data multiple times, giving a non-linear classifier relative to the pre-final
layer representation

41



Softmax classifier

Again, we can tease apart the prediction function into three steps:

1. For each row y of W, calculate dot product with x:

2. Apply softmax function to get normalized probability:

= softmax(𝑓𝑦)

3. Choose the y with maximum probability

• For each training example (x,y), our objective is to maximize the probability of the 
correct class y or we can minimize the negative log probability of that class:

42



Thanks.

43