Computational Natural Language Processing

Seq-to-Seq Models and Neural Machine Translation

Hamidreza Mahyar
mahyarh@mcmaster.ca



(One Type of) Conditional Language Model
Encoder

kono eiga ga kirai </s>

LSTM LSTM LSTM LSTM LSTM

I hate this movie

LSTM LSTM LSTM LSTM

argmax argmax argmax argmax argmax

I hate this movie </s>
Decoder



Machine Translation

Machine Translation (MT) is the task of translating a sentence x from one language (the 
source language) to a sentence y in another language (the target language).

x: L'homme est né libre, et partout il est dans les fers

y: Man is born free, but everywhere he is in chains

– Rousseau



The early history of MT: 1950s
• Machine translation research began in the early 1950s on machines less 

powerful than high school calculators (before term “A.I.” coined!)
• Concurrent with foundational work on automata, formal languages, 

probabilities, and information theory
• MT heavily funded by military, but basically just simple rule-based 

systems doing word substitution
• Human language is more complicated than that, and varies more across 

languages!
• Little understanding of natural language syntax, semantics, pragmatics
• Problem soon appeared intractable

1 minute video showing 1954 MT: 
https://youtu.be/K-HfpsHPmvw



The early history of MT: 1950s



1990s-2010s: Statistical Machine Translation
• Core idea: Learn a probabilistic model from data
• Suppose we’re translating French → English.
• We want to find best English sentence y, given French sentence x

• Use Bayes Rule to break this down into two components to be 
learned separately:

Translation Model Language Model

Models how words and phrases Models how to write 
should be translated (fidelity). good English (fluency).

Learned from parallel data. Learned from monolingual data.



What happens in translation isn’t trivial to model!

1519年600名西班牙人在墨西哥登陆，去征服几百万人口
的阿兹特克帝国，初次交锋他们损兵三分之二。
In 1519, six hundred Spaniards landed in Mexico to conquer the Aztec Empire with a 
population of a few million. They lost two thirds of their soldiers in the first clash.

translate.google.com (2009): 1519 600 Spaniards landed in Mexico, millions of people to 
conquer the Aztec empire, the first two-thirds of soldiers against their loss. 
translate.google.com (2013): 1519 600 Spaniards landed in Mexico to conquer the Aztec
empire, hundreds of millions of people, the initial confrontation loss of soldiers two-thirds.
translate.google.com (2015): 1519 600 Spaniards landed in Mexico, millions of people to 
conquer the Aztec empire, the first two-thirds of the loss of soldiers they clash.



1990s–2010s: Statistical Machine Translation

• SMT was a huge research field
• The best systems were extremely complex
• Hundreds of important details

• Systems had many separately-designed subcomponents
• Lots of feature engineering
• Need to design features to capture particular language phenomena

• Required compiling and maintaining extra resources
• Like tables of equivalent phrases

• Lots of human effort to maintain
• Repeated effort for each language pair!



What is Neural Machine Translation?

• Neural Machine Translation (NMT) is a way to do Machine Translation with a single 
end-to-end neural network

• The neural network architecture is called a sequence-to-sequence model (aka seq2seq) 
and it involves two RNNs (/LSTMs)



Neural Machine Translation (NMT)
The sequence-to-sequence model

Target sentence (output)
Encoding of the source sentence.

Provides initial hidden state
for Decoder RNN. he hit me with a pie <END>

il a m’ entarté <START> he hit me with a pie

Source sentence (input) Decoder RNN is a Language Model that generates 
target sentence, conditioned on encoding.

Encoder RNN produces Note: This diagram shows test time behavior: decoder 
an encoding of the output is fed in as next step’s input
source sentence.

Encoder RNN

argmax

argmax

argmax

argmax

argmax

argmax

argmax

Decoder RNN



Sequence-to-sequence is versatile!

• The general notion here is an encoder-decoder model
• One neural network takes input and produces a neural representation
• Another network produces output based on that neural representation
• If the input and output are sequences, we call it a seq2seq model

• Sequence-to-sequence is useful for more than just MT
• Many NLP tasks can be phrased as sequence-to-sequence:
• Summarization (long text → short text)
• Dialogue (previous utterances → next utterance)
• Parsing (input text → output parse as sequence)
• Code generation (natural language → Python code)



Neural Machine Translation (NMT)

• The sequence-to-sequence model is an example of a Conditional Language Model
• Language Model because the decoder is predicting the next word of the target sentence y
• Conditional because its predictions are also conditioned on the source sentence x

• NMT directly calculates :

Probability of next target word, given 
target words so far and source sentence x

• Question: How to train an NMT system?
• (Easy) Answer: Get a big parallel corpus…
• But there is now exciting work on “unsupervised NMT”, data augmentation, etc.



Training a Neural Machine Translation system
= negative log = negative log = negative log 
prob of “he” prob of “with” prob of <END>

= 𝐽1 + 𝐽2 + 𝐽3 + 𝐽$ + 𝐽5 + 𝐽& + 𝐽7

𝑦"1 𝑦"2 𝑦"3 𝑦"$ 𝑦"5 𝑦"& 𝑦"7

il a m’ entarté <START> he hit me with a pie

Source sentence (from corpus) Target sentence (from corpus)

Seq2seq is optimized as a single system. Backpropagation operates “end-to-end”.

Encoder RNNDecoder RNN



Multi-layer deep encoder-decoder machine translation net
[Sutskever et al. 2014; Luong et al. 2015] The hidden states from RNN layer i

are the inputs to RNN layer i+1

Translation  
The protests escalated over the weekend <EOS> generated

0.1 0.2 0.4 0.5 0.2 -0.1 0.2 0.2 0.3 0.4 -0.2 -0.4 -0.3
0.3 0.6 0.4 0.5 0.6 0.6 0.6 0.6 0.6 0.4 0.6 0.6 0.5
0.1 -0.1 0.3 0.9 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1

-0.4 -0.7 -0.2 -0.3 -0.5 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7
0.2 0.1 -0.3 -0.2 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1

Encoder:
Builds up 0.2 0.2 0.1 0.2 0.2 0.2 0.2 -0.4 0.2 -0.1 0.2 0.3 0.2

-0.2 0.6 0.3 0.6 -0.8 0.6 -0.1 0.6 0.6 0.6 0.4 0.6 0.6
-0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1 -0.1

sentence Decoder
0.1 -0.7 -0.7 -0.4 -0.5 -0.7 -0.7 -0.7 0.3 0.3 0.2 -0.5 -0.7
0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1

meaning
0.2 0.4 0.2 0.2 0.4 0.2 0.2 0.2 0.2 -0.1 -0.2 -0.4 0.2
0.6 -0.6 -0.3 0.4 -0.2 0.6 0.6 0.6 0.6 0.3 0.6 0.5 0.6

-0.1 0.2 -0.1 0.1 -0.3 -0.1 -0.1 -0.1 -0.1 -0.1 0.1 -0.5 -0.1
-0.7 -0.3 -0.4 -0.5 -0.4 -0.7 -0.7 -0.7 -0.7 -0.7 0.3 0.4 -0.7
0.1 0.4 0.2 -0.2 -0.2 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1

Source Die Proteste waren am Wochenende eskaliert <EOS> The protests escalated over the weekend Feeding in 
sentence last word



Decoding: Greedy decoding

• We saw how to generate (or “decode”) the target sentence by taking argmax on each 
step of the decoder

he hit me with a pie <END>

<START> he hit me with a pie

• This is greedy decoding (take most probable word on each step)

15

argmax

argmax

argmax

argmax

argmax

argmax

argmax



Problems with greedy decoding

• Greedy decoding has no way to undo decisions!
• Input: il a m’entarté (he hit me with a pie)
• → he 
• → he hit 
• → he hit a (whoops! no going back now…)

• How to fix this?

16



Exhaustive search decoding

• Ideally, we want to find a (length T) translation y that maximizes

• We could try computing all possible sequences y
• This means that on each step t of the decoder, we’re tracking Vt possible partial 

translations, where V is vocab size
• This O(VT) complexity is far too expensive!

17



Beam search decoding

• Core idea: On each step of decoder, keep track of the k most probable partial 
translations (which we call hypotheses)
• k is the beam size (in practice around 5 to 10, in NMT)

• A hypothesis has a score which is its log probability:

• Scores are all negative, and higher score is better
• We search for high-scoring hypotheses, tracking top k on each step

• Beam search is not guaranteed to find optimal solution
• But much more efficient than exhaustive search!

18



Beam search decoding: example
Beam size = k = 2. Blue numbers =

<START>

Calculate prob 
dist of next word

19



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-0.7 = log PLM(he|<START>)
he

<START>

I
-0.9 = log PLM(I|<START>)

Take top k words
and compute scores

20



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-1.7 = log PLM(hit|<START> he) + -0.7
-0.7 hit
he

struck
-2.9 = log PLM(struck|<START> he) + -0.7

<START>
-1.6 = log PLM(was|<START> I) + -0.9
was

I
got

-0.9
-1.8 = log PLM(got|<START> I) + -0.9

For each of the k hypotheses, find 
10 top k next words and calculate scores



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-1.7
-0.7 hit
he

struck
-2.9

<START>
-1.6
was

I
got

-0.9
-1.8

Of these k2 hypotheses, 
11 just keep k with highest scores



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-2.8 = log PLM(a|<START> he hit) + -1.7
-1.7 a

-0.7 hit
he me

struck -2.5 = log PLM(me|<START> he hit) + -1.7
-2.9

<START> -2.9 = log PLM(hit|<START> I was) + -1.6
-1.6 hit
was

I struck
got

-0.9 -3.8 = log PLM(struck|<START> I was) + -1.6
-1.8

For each of the k hypotheses, find 
12 top k next words and calculate scores



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-2.8
-1.7 a

-0.7 hit
he me

struck -2.5
-2.9

<START> -2.9
-1.6 hit
was

I struck
got

-0.9 -3.8
-1.8

Of these k2 hypotheses, 
13 just keep k with highest scores



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-4.0
tart

-2.8
-1.7 pie

a
-0.7 hit -3.4
he me -3.3

struck -2.5 with
-2.9

<START> -2.9 on
-1.6 hit -3.5
was

I struck
got

-0.9 -3.8
-1.8

For each of the k hypotheses, find 
14 top k next words and calculate scores



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-4.0
tart

-2.8
-1.7 pie

a
-0.7 hit -3.4
he me -3.3

struck -2.5 with
-2.9

<START> -2.9 on
-1.6 hit -3.5
was

I struck
got

-0.9 -3.8
-1.8

Of these k2 hypotheses, 
15 just keep k with highest scores



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-4.0 -4.8
tart in

-2.8
-1.7 pie with

a
-0.7 hit -3.4 -4.5
he me -3.3 -3.7

struck -2.5 with a
-2.9

<START> -2.9 on one
-1.6 hit -3.5 -4.3
was

I struck
got

-0.9 -3.8
-1.8

For each of the k hypotheses, find 
16 top k next words and calculate scores



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-4.0 -4.8
tart in

-2.8
-1.7 pie with

a
-0.7 hit -3.4 -4.5
he me -3.3 -3.7

struck -2.5 with a
-2.9

<START> -2.9 on one
-1.6 hit -3.5 -4.3
was

I struck
got

-0.9 -3.8
-1.8

Of these k2 hypotheses, 
17 just keep k with highest scores



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-4.0 -4.8
tart in

-2.8
pie with -4.3

-1.7 a
-0.7 -3.4 -4.5 pie

hit
he me -3.3 -3.7 tart

struck -2.5 with a -4.6
-2.9

<START> -2.9 on one -5.0
-1.6 hit -3.5 -4.3 pie
was

I struck tart
got

-0.9 -3.8 -5.3
-1.8

For each of the k hypotheses, find 
18 top k next words and calculate scores



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-4.0 -4.8
tart in

-2.8
-1.7 pie with -4.3

a
-0.7 pie

hit -3.4 -4.5
he me -3.3 -3.7 tart

struck -2.5 with a -4.6
-2.9

<START> -2.9 on one -5.0
-1.6 hit -3.5 -4.3 pie
was

I struck tart
got

-0.9 -3.8 -5.3
-1.8

This is the top-scoring hypothesis!
19



Beam search decoding: example
Beam size = k = 2. Blue numbers =

-4.0 -4.8
tart in

-2.8
-1.7 pie with -4.3

a
-0.7 pie

hit -3.4 -4.5
he me -3.3 -3.7 tart

struck -2.5 with a -4.6
-2.9

<START> -2.9 on one -5.0
-1.6 hit -3.5 -4.3 pie
was

I struck tart
got

-0.9 -3.8 -5.3
-1.8

Backtrack to obtain the full hypothesis
20



How do we evaluate Machine Translation?

BLEU (Bilingual Evaluation Understudy)

• BLEU compares the machine-written translation to one or several human-written 
translation(s), and computes a similarity score based on:
• n-gram precision (usually for 1, 2, 3 and 4-grams)
• Plus a penalty for too-short system translations

• BLEU is useful but imperfect
• There are many valid ways to translate a sentence
• So a good translation can get a poor BLEU score because it has low n-gram overlap 

with the human translation L

Source: ”BLEU: a Method for Automatic Evaluation of Machine Translation", Papineni et al, 2002. http://aclweb.org/anthology/P02-1040



BLEU score against 4 reference translations

Reference translation 1: Reference translation 2:
The U.S. island of Guam is maintaining Guam International Airport and its 
a high state of alert after the Guam offices are maintaining a high state of 
airport and its offices both received an alert after receiving an e-mail that was 
e-mail from someone calling himself from a person claiming to be the 
the Saudi Arabian Osama bin Laden wealthy Saudi Arabian businessman 
and threatening a biological/chemical Bin Laden and that threatened to 
attack against public places such as launch a biological and chemical attack 
the airport . on the airport and other public places .

Machine translation:
The American [?] international airport 
and its the office all receives one calls 
self the sand Arab rich business [?] 
and so on electronic mail , which 
sends out ; The threat will be able 
after public place and so on the 
airport to start the biochemistry attack
, [?] highly alerts after the 
maintenance.

Reference translation 3: Reference translation 4:
The US International Airport of Guam US Guam International Airport and its 
and its office has received an email office received an email from Mr. Bin 
from a self-claimed Arabian millionaire Laden and other rich businessman 
named Laden , which threatens to from Saudi Arabia . They said there 
launch a biochemical attack on such would be biochemistry air raid to Guam 
public places as airport . Guam Airport and other public places . Guam 
authority has been on alert . [Papineni et al. 2002] needs to be in high precaution about 

this matter .



MT progress over time
[Edinburgh En-De WMT newstest2013 Cased BLEU; NMT 2015 from U. Montréal; NMT 2019 FAIR on newstest2019]

45 Phrase-based SMT
40

Syntax-based SMT
35
30 Neural MT

25
20
15

10
5
0

2013 2014 2015 2016 2017 2018 2019
Sources: http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf & http://matrix.statmt.org/



Advantages of NMT

Compared to SMT, NMT has many advantages:

• Better performance
• More fluent
• Better use of context
• Better use of phrase similarities

• A single neural network to be optimized end-to-end
• No subcomponents to be individually optimized

• Requires much less human engineering effort
• No feature engineering
• Same method for all language pairs



Disadvantages of NMT?

Compared to SMT:

• NMT is less interpretable
• Hard to debug

• NMT is difficult to control
• For example, can’t easily specify rules or guidelines for translation
• Safety concerns!



NMT: the first big success story of NLP Deep Learning

Neural Machine Translation went from a fringe research attempt in 2014 to the leading 
standard method in 2016

• 2014: First seq2seq paper published

• 2016: Google Translate switches from SMT to NMT – and by 2018 everyone has

• This is amazing!
• SMT systems, built by hundreds of engineers over many years, outperformed by 

NMT systems trained by a small group of engineers in a few months



Summary so far!

Lots of new information today and last week! What are some of the practical takeaways?

1. LSTMs are powerful 2. Clip your gradients
Translation  

The    protests escalated over       the    weekend <EOS> generated
0 0 0

0 0 0
. . . . 0 0 0

. . . . . .
4 5 2 2 2 3 4 . . .

1 2
1 2 4 3

0 0 0 0 0 0 0 0 0
0 . . 0 0 0

. . . . . . .
. 4 5 6
3 6 6 6 6 6 . . .

4
- - - 6 6 5

0 - 0 0 - -
0 - 0 0 0 - - -

. 0 . . 0

. 0
. 3 9 . . . . 0 0 0

1 1 . 
1 - - 1 1 1 1 1 . . .

- - - - 1 1 1
0 - 0 0 -

0 - 0 0 0 - - - -
. 0 . . 

. 0
. . . . 0 0 0 0

4 . 2 3
. 

7 - - 5 7 7 7 7 . . .
7

0 0 0 0 0 0 7 7 7
. 0 0 0 0

. 1 0 0 0
2 . . . . . . . . . . .

1 3 2 1 1 1 1 1
1 1 1

Encoder: 0 0 . 0 0
. . . . 2 . . 0 0 0

2 - 0
2 2 1 2 2 . . 2 . .

0 3 2
- - 0 4 . .

- 0 0 0 0 0 0
0 1 2 0 0

0 . . . . . 

Builds up . . . 0 . .
. 6 3 6 8 6

6 6 . 0 6 6
2 - - - - - 1
- 0 0 0 0 - - - 6 . - -

0 - - 4 0 0
0 0 0 0

0 . . . . . . .  
. 1 1 1 . 0

sentence 1 1 1 1 . . 
1 1 1. 1. Decoder

- - - 1 -
0 0 0 0 - 0 1 - -

0 0 - -

. . . . . 0 0 . 0 0 0 0
3 . . . .

. 2
1 7 7 4 5 7 . .

7 7 0 3 5 7
0 0 0 0 0 0

meaning 0 0 . 0 0
. . . 1 . 1 . 0 0

. . . . 1 . 1 . .
1 1 1 1 1 1 1 1 1

. 
0 0 . 0 0 0 0

0
. 4

. . 2 . . . . . - 0 . 
2 4 2 - 0 2 2 2 2 0 . 2

- .
0 - 0 0 4
. 0 . . 0 . 0 . 0 . 0 1 2 0

. . 0
6 . 3 4 2 6 6 6 6 0 0 .

. 
- 6 - 0 - - - - - . . 6 6

0 0 . 0 0 0 0 - 3 5 -
0 0 - 0

0 .
. . . . . . 0

. 1 3 0 . 
1 2 1 1 1 1 . .

1 - . 1
- - - - - 1 1

- - - 0 5
0 0 0 0 0 - 0 -

0 0 0 . 0 . 3 0 0
. . 5 . . . . . .

. .4 7 7 7 7 . 0
7 3 4 - 4 7

- 0 0 0 0 0 7
0 0 0 0 0 0

. . 1 . . . 0 .1
. . . . . .
1 4 1 1 1 .

Source Feeding in last  
Die     Proteste waren am Wochenende eskaliert <EOS> The     protests escalated over        the   weekend

sentence word

Conditioning =
Bottleneck

3. Use bidirectionality 4. Encoder-Decoder Neural Machine 
when possible Translation Systems work very well



2. Why attention? Sequence-to-sequence: the
bottleneck pEnrcoodbinlgeomf the 

source sentence.
Target sentence (output)

he hit me with a pie <END>

il a m’ entarté <START> he hit me with a pie

Source sentence (input)

Problems with this architecture?

39

Encoder RNNDecoder RNN



1. Why attention? Sequence-to-sequence: the
bottleneck pEnrcoodbinlgeomf the 

source sentence.
This needs to capture all Target sentence (output)
information about the 

source sentence. he hit me with a pie <END>
Information bottleneck!

il a m’ entarté <START> he hit me with a pie

Source sentence (input)

40

Encoder RNNDecoder RNN



Attention

• Attention provides a solution to the bottleneck problem.

• Core idea: on each step of the decoder, use direct connection to the encoder to focus 
on a particular part of the source sequence

• First, we will show via diagram (no equations), then we will show with equations

41



Sequence-to-sequence with attention
Core idea: on each step of the decoder, use direct connection to the encoder to focus on a 
particular part of the source sequence

dot product

il a m’ entarté <START>

42 Source sentence (input)

Encoder Attention 
RNN scores

Decoder RNN



Sequence-to-sequence with attention

dot product

il a m’ entarté <START>

43 Source sentence (input)

Encoder Attention 
RNN scores

Decoder RNN



Sequence-to-sequence with attention

dot product

il a m’ entarté <START>

44 Source sentence (input)

Encoder Attention 
RNN scores

Decoder RNN



Sequence-to-sequence with attention

dot product

il a m’ entarté <START>

45 Source sentence (input)

Encoder Attention 
RNN scores

Decoder RNN



Sequence-to-sequence with attention

On this decoder timestep, we’re 
mostly focusing on the first 
encoder hidden state (”he”)

Take softmax to turn the scores 
into a probability distribution

il a m’ entarté <START>

46 Source sentence (input)

Encoder Attention Attention 
RNN scores distribution

Decoder RNN



Sequence-to-sequence with attention

Attention Use the attention distribution to take a
output weighted sum of the encoder hidden states.

The attention output mostly contains 
information from the hidden states that 
received high attention.

il a m’ entarté <START>

47 Source sentence (input)

Encoder Attention Attention 
RNN scores distribution

Decoder RNN



Sequence-to-sequence with attention

Attention he
output

Concatenate attention output 
𝑦.1 with decoder hidden state, then 

use to compute 𝑦1 as before

il a m’ entarté <START>

48 Source sentence (input)

Encoder Attention Attention 
RNN scores distribution

Decoder RNN



Sequence-to-sequence with attention

Attention hit
output

𝑦2

Sometimes we take the 
attention output from the 
previous step, and also 
feed it into the decoder 

il a m’ entarté <START> he (along with the usual 
decoder input).

49 Source sentence (input)

Encoder Attention Attention 
RNN scores distribution

Decoder RNN



Sequence-to-sequence with attention

Attention me
output

𝑦3

il a m’ entarté <START> he hit

50 Source sentence (input)

Encoder Attention Attention 
RNN scores distribution

Decoder RNN



Sequence-to-sequence with attention

Attention with
output

𝑦4

il a m’ entarté <START> he hit me

51 Source sentence (input)

Encoder Attention Attention 
RNN scores distribution

Decoder RNN



Sequence-to-sequence with attention

Attention a
output

𝑦5

il a m’ entarté <START> he hit me with

52 Source sentence (input)

Encoder Attention Attention 
RNN scores distribution

Decoder RNN



Sequence-to-sequence with attention

Attention pie
output

𝑦6

il a m’ entarté <START> he hit me with a

53 Source sentence (input)

Encoder Attention Attention 
RNN scores distribution

Decoder RNN



Attention: in equations

• We have encoder hidden states
• On timestep t, we have decoder hidden state
• We get the attention scores for this step:

• We take softmax to get the attention distribution for this step (this is a probability distribution and
sums to 1)

• We use to take a weighted sum of the encoder hidden states to get the attention output

• Finally we concatenate the attention output with the decoder hidden 
state and proceed as in the non-attention seq2seq model

54



Attention is great!

• Attention significantly improves NMT performance
• It’s very useful to allow decoder to focus on certain parts of the source

• Attention provides a more “human-like” model of the MT process
• You can look back at the source sentence while translating, rather than needing to remember it all

• Attention solves the bottleneck problem
• Attention allows decoder to look directly at source; bypass bottleneck

• Attention helps with the vanishing gradient problem
• Provides shortcut to faraway states

• Attention provides some interpretability
• By inspecting attention distribution, we see what the decoder was focusing on

il
• We get (soft) alignment for free!

a
• This is cool because we never explicitly trained an alignment system

m’
• The network just learned alignment by itself

entarté

55

he

hit

me

with

a

pie



There are several attention variants

• We have some values and a query

• Attention always involves: There are 
1. Computing the attention scores multiple ways 

to do this
2. Taking softmax to get attention distribution 𝘢:

3. Using attention distribution to take weighted sum of values:

thus obtaining the attention output a (sometimes called the context vector)

56



Attention variants You’ll think about the relative 
advantages/disadvantages of these in Assignment 4!

There are several ways you can compute from and :

• Basic dot-product attention:
• Note: this assumes . This is the version we saw earlier.

• Multiplicative attention: [Luong, Pham, and Manning 2015]
• Where is a weight matrix. Perhaps better called “bilinear attention”

• Reduced-rank multiplicative attention: 𝑒𝑖 = 𝑠𝑇 𝑼𝑇𝑽 ℎ𝑖 = (𝑼𝑠)𝑇(𝑽ℎ𝑖) Remember this when we look 
at Transformers next week!

• For low rank matrices 𝑼 ∈ ℝ𝑘×𝑑2, 𝑽 ∈ ℝ𝑘×𝑑1, 𝑘 ≪ 𝑑 1, 𝑑 2

• Additive attention: [Bahdanau, Cho, and Bengio 2014]
• Where are weight matrices and is a weight vector.
• d3 (the attention dimensionality) is a hyperparameter
• “Additive” is a weird/bad name. It’s really using a feed-forward neural net layer.

More information: “Deep Learning for NLP Best Practices”, Ruder, 2017. http://ruder.io/deep-learning-nlp-best-practices/index.html#attention
“Massive Exploration of Neural Machine Translation Architectures”, Britz et al, 2017, https://arxiv.org/pdf/1703.03906.pdf

57



Attention is a general Deep Learning technique

• We’ve seen that attention is a great way to improve the sequence-to-sequence model 
for Machine Translation.

• However: You can use attention in many architectures 
(not just seq2seq) and many tasks (not just MT)

• More general definition of attention:
• Given a set of vector values, and a vector query, attention is a technique to compute 

a weighted sum of the values, dependent on the query.

• We sometimes say that the query attends to the values.
• For example, in the seq2seq + attention model, each decoder hidden state (query)

attends to all the encoder hidden states (values).

58



Attention is a general Deep Learning technique

• More general definition of attention:
• Given a set of vector values, and a vector query, attention is a technique to compute 

a weighted sum of the values, dependent on the query.

Intuition:
• The weighted sum is a selective summary of the information contained in the values, 

where the query determines which values to focus on.
• Attention is a way to obtain a fixed-size representation of an arbitrary set of 

representations (the values), dependent on some other representation (the query).

Upshot:
• Attention has become the powerful, flexible, general way pointer and memory 

manipulation in all deep learning models. A new idea from after 2010! From NMT!
59