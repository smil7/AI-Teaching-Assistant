Learning Dense Representations of Phrases at Scale

Jinhyuk Lee1,2∗ Mujeen Sung1 Jaewoo Kang1 Danqi Chen2

Korea University1 Princeton University2

{jinhyuk_lee,mujeensung,kangj}@korea.ac.kr
danqic@cs.princeton.edu

Abstract a recent new paradigm solely based on phrase re-
trieval (Seo et al., 2019; Lee et al., 2020). Phrase

Open-domain question answering can be refor- retrieval highlights the use of phrase representa-
mulated as a phrase retrieval problem, without tions and finds answers purely based on the similar-
the need for processing documents on-demand
during inference (Seo et al., 2019). However, ity search in the vector space of phrases.2 Without
current phrase retrieval models heavily depend relying on an expensive reader model for process-
on sparse representations and still underper- ing text passages, it has demonstrated great runtime
form retriever-reader approaches. In this work, efficiency at inference time.
we show for the first time that we can learn Despite great promise, it remains a formidable
dense representations of phrases alone that challenge to build vector representations for ev-
achieve much stronger performance in open- ery single phrase in a large corpus. Since phrase
domain QA. We present an effective method representations are decomposed from question rep-
to learn phrase representations from the super-
vision of reading comprehension tasks, cou- resentations, they are inherently less expressive
pled with novel negative sampling methods. than cross-attention models (Devlin et al., 2019).
We also propose a query-side fine-tuning strat- Moreover, the approach requires retrieving answers
egy, which can support transfer learning and correctly out of billions of phrases (e.g., 6× 1010
reduce the discrepancy between training and phrases in English Wikipedia), making the scale of
inference. On five popular open-domain QA the learning problem difficult. Consequently, ex-
datasets, our model DensePhrases improves isting approaches heavily rely on sparse represen-
over previous phrase retrieval models by 15%–
25% absolute accuracy and matches the perfor- tations for locating relevant documents and para-
mance of state-of-the-art retriever-reader mod- graphs while still falling behind retriever-reader
els. Our model is easy to parallelize due to models (Seo et al., 2019; Lee et al., 2020).
pure dense representations and processes more In this work, we investigate whether we can build
than 10 questions per second on CPUs. Finally, fully dense phrase representations at scale for open-
we directly use our pre-indexed dense phrase domain QA. First, we aim to learn strong phrase
representations for two slot filling tasks, show- representations from the supervision of reading
ing the promise of utilizing DensePhrases as a
dense knowledge base for downstream tasks.1 comprehension tasks. We propose to use data aug-

mentation and knowledge distillation to learn better
1 Introduction phrase representations within a single passage. We

then adopt negative sampling strategies such as in-
Open-domain question answering (QA) aims to batch negatives (Henderson et al., 2017; Karpukhin
provide answers to natural-language questions us- et al., 2020), to better discriminate the phrases at
ing a large text corpus (Voorhees et al., 1999; Fer- a larger scale. Here, we present a novel method
rucci et al., 2010; Chen and Yih, 2020). While a called pre-batch negatives, which leverages preced-
dominating approach is a two-stage retriever-reader ing mini-batches as negative examples to compen-
approach (Chen et al., 2017; Lee et al., 2019; Guu sate the need of large-batch training. Lastly, we
et al., 2020; Karpukhin et al., 2020), we focus on present a query-side fine-tuning strategy that dras-

∗Work partly done while visiting Princeton University. 2Following previous work (Seo et al., 2018), ‘phrase’ de-
1Our code is available at https://github.com/ notes any contiguous segment of text up to L words (including

princeton-nlp/DensePhrases. single words), which is not necessarily a linguistic phrase.

arXiv:2012.12624v3  [cs.CL]  2 Jun 2021



Category Model Sparse? Storage #Q/sec NQ SQuAD
(GB) (GPU, CPU) (Acc) (Acc)

DrQA (Chen et al., 2017) 3 26 1.8, 0.6 - 29.8
BERTSerini (Yang et al., 2019) 3 21 2.0, 0.4 - 38.6

Retriever-Reader ORQA (Lee et al., 2019) 7 18 8.6, 1.2 33.3 20.2
REALMNews (Guu et al., 2020) 7 18 8.4, 1.2 40.4 -
DPR-multi (Karpukhin et al., 2020) 7 76 0.9, 0.04 41.5 24.1
DenSPI (Seo et al., 2019) 3 1,200 2.9, 2.4 8.1 36.2

Phrase Retrieval DenSPI + Sparc (Lee et al., 2020) 3 1,547 2.1, 1.7 14.5 40.7
DensePhrases (Ours) 7 320 20.6, 13.6 40.9 38.0

Table 1: Retriever-reader and phrase retrieval approaches for open-domain QA. The retriever-reader approach
retrieves a small number of relevant documents or passages from which the answers are extracted. The phrase
retrieval approach retrieves an answer out of billions of phrase representations pre-indexed from the entire corpus.
Appendix B provides detailed benchmark specification. The accuracy is measured on the test sets in the open-
domain setting. NQ: Natural Questions.

tically improves phrase retrieval performance and most approaches—including ours—are generic and
allows for transfer learning to new domains, with- could be applied to other collections of documents.
out re-building billions of phrase representations. The task aims to provide an answer â for the in-

As a result, all these improvements lead to a put question q based on D. In this work, we focus
much stronger phrase retrieval model, without the on the extractive QA setting, where each answer is
use of any sparse representations (Table 1). We a segment of text, or a phrase, that can be found in
evaluate our model, DensePhrases, on five standard D. Denote the set of phrases inD as S(D) and each
open-domain QA datasets and achieve much bet- phrase sk ∈ S(D) consists of contiguous words
ter accuracies than previous phrase retrieval mod- wstart(k), . . . , wend(k) in its document ddoc(k). In
els (Seo et al., 2019; Lee et al., 2020), with 15%– practice, we consider all the phrases up to L = 20
25% absolute improvement on most datasets. Our words in D and S(D) comprises a large number of
model also matches the performance of state-of- 6× 1010 phrases. An extractive QA system returns
the-art retriever-reader models (Guu et al., 2020; a phrase ŝ = argmaxs∈S(D) f(s|D, q) where f is
Karpukhin et al., 2020). Due to the removal of a scoring function. The system finally maps ŝ to
sparse representations and careful design choices, an answer string â: TEXT(ŝ) = â and the evalua-
we further reduce the storage footprint for the full tion is typically done by comparing the predicted
English Wikipedia from 1.5TB to 320GB, as well answer â with a gold answer a∗.
as drastically improve the throughput. Although we focus on the extractive QA setting,

Finally, we envision that DensePhrases acts as a recent works propose to use a generative model as
neural interface for retrieving phrase-level knowl- the reader (Lewis et al., 2020; Izacard and Grave,
edge from a large text corpus. To showcase this 2021), or learn a closed-book QA model (Roberts
possibility, we demonstrate that we can directly et al., 2020), which directly predicts answers with-
use DensePhrases for fact extraction, without re- out using an external knowledge source. The ex-
building the phrase storage. With only fine-tuning tractive setting provides two advantages: first, the
the question encoder on a small number of subject- model directly locates the source of the answer,
relation-object triples, we achieve state-of-the-art which is more interpretable, and second, phrase-
performance on two slot filling tasks (Petroni et al., level knowledge retrieval can be uniquely adapted
2021), using less than 5% of the training data. to other NLP tasks as we show in §7.3.

2 Background Retriever-reader. A dominating paradigm in
open-domain QA is the retriever-reader ap-

We first formulate the task of open-domain ques- proach (Chen et al., 2017; Lee et al., 2019;
tion answering for a set of K documents D = Karpukhin et al., 2020), which leverages a first-
{d1, . . . , dK}. We follow the recent work (Chen stage document retriever fretr and only reads top
et al., 2017; Lee et al., 2019) and treat all of English K ′  K documents with a reader model fread.
Wikipedia as D, hence K ≈ 5 × 106. However, The scoring function f(s | D, q) is decomposed as:



• We first learn a high-quality phrase encoder
f(s | D, q) = fretr({dj1 , . . . , djK′} | D, q)

and an (initial) question encoder from the
(1) supervision of reading comprehension tasks

× fread(s | {dj1 , . . . , djK′}, q), (§4.1), as well as incorporating effective nega-
where {j1, . . . , jK′} ⊂ {1, . . . ,K} and if s ∈/ tive sampling to better discriminate phrases at
S({d scale (§4.2, §4.3).

j1 , . . . , djK′}), the score will be 0. It can eas-
ily adapt to passages and sentences (Yang et al., • Then, we fix the phrase encoder and encode
2019; Wang et al., 2019). However, this approach all the phrases s ∈ S(D) and store the phrase
suffers from error propagation when incorrect docu- indexing offline to enable efficient search (§5).
ments are retrieved and can be slow as it usually re- • Finally, we introduce an additional strategy
quires running an expensive reader model on every called query-side fine-tuning (§6) by further
retrieved document or passage at inference time. updating the question encoder.4 We find this

step to be very effective, as it can reduce
Phrase retrieval. Seo et al. (2019) introduce the the discrepancy between training (the first
phrase retrieval approach that encodes phrase and step) and inference, as well as support transfer
question representations independently and per- learning to new domains.
forms similarity search over the phrase representa-
tions to find an answer. Their scoring function Before we present the approach in detail, we first

f is
computed as follows: describe our base architecture below.

f(s | D, q) = Es(s,D)>Eq(q), (2) 3.2 Base Architecture
Our base architecture consists of a phrase encoder

where Es and Eq denote the phrase encoder and Es and a question encoder Eq. Given a passage
the question encoder respectively. As Es(·) and p = w1, . . . , wm, we denote all the phrases up to L
Eq(·) representations are decomposable, it can tokens as S(p). Each phrase sk has start and end in-
support maximum inner product search (MIPS) dicies start(k) and end(k) and the gold phrase
and improve the efficiency of open-domain QA is s∗ ∈ S(p). Following previous work on phrase
models. Previous approaches (Seo et al., 2019; or span representations (Lee et al., 2017; Seo et al.,
Lee et al., 2020) leverage both dense and sparse 2018), we first apply a pre-trained language model
vectors for phrase and question representations Mp to obtain contextualized word representations
by taking their concatenation: Es(s,D) = for each passage token: h1, . . . ,hm ∈ Rd. Then,
[Esparse(s,D), Edense(s,D)].3 However, since the we can represent each phrase sk ∈ S(p) as the con-
sparse vectors are difficult to parallelize with dense catenation of corresponding start and end vectors:
vectors, their method essentially conducts sparse
and dense vector search separately. The goal of Es(sk, p) = [hstart(k),hend(k)] ∈ R2d. (3)
this work is to only use dense representations,
i.e., Es(s,D) = Edense(s,D), which can model A great advantage of this representation is that we
f(s | D, q) solely with MIPS, as well as close the eventually only need to index and store all the word
gap in performance. vectors (we useW(D) to denote all the words in

D), instead of all the phrases S(D), which is at
3 DensePhrases least one magnitude order smaller.

Similarly, we need to learn a question encoder
3.1 Overview Eq(·) that maps a question q = w̃1, . . . , w̃n to a
We introduce DensePhrases, a phrase retrieval vector of the same dimension as Es(·). Since the
model that is built on fully dense representations. start and end representations of phrases are pro-
Our goal is to learn a phrase encoder as well as a duced by the same language model, we use an-
question encoder, so we can pre-index all the pos- other two different pre-trained encoders Mq,start
sible phrases in D, and efficiently retrieve phrases and Mq,end to differentiate the start and end po-
for any question through MIPS at testing time. We sitions. We apply Mq,start and Mq,end on q sep-
outline our approach as follows: arately and obtain representations qstart and qend

3Seo et al. (2019) use sparse representations of both para- 4In this paper, we use the term question and query inter-
graphs and documents and Lee et al. (2020) use contextualized changeably as our question encoder can be naturally extended
sparse representations conditioned on the phrase. to “unnatural” queries.



*-2- -&(
&2- Ǜǜ ƾ
*.$/$1 ' ' $/$*)') "/$1 .
 "/$1 ' ' Ǚ$)Ǒ/#ǟ+- Ǒ/#ǚ 0& 

*+Ǒƨ ''$)"/*)
-# ./-

*)ǅ//)*'*. /* #$/.*)" /# *'$ *+ǑƧ /# 
Ǜǜ Ǜǜ Ǜǜ Ǜǜ *'$ 

ƾ ƾ ƾ Ǜǜ ( .

*)ǅ/  #$/ .*)" /# *'$ -*2)

0 ./$*) #-.  0 ./$*) #-. 
)* - )* - )* - )* -

ǆ*)ǅ//)*'*. /* ǆ$.#$/.*)"
Ǜǜ#*.$)".*)ǅ//)*'*. /* ǂ 4/# -$/$.#-*&)/# *'$ ƺƺƺ Ǜǜ*)ǅ//)*'*. /* Ǜǜ.0)"4

Ǚǚ$)"' Ǒ+.." /-$)$)"2ǟ$/$*)') "/$1 . Ǚǚ0 -4Ǒ.$ Ɵ) Ǒ/0)$)"ƥ
)! - ) 

Figure 1: An overview of DensePhrases. (a) We learn dense phrase representations in a single passage (§4.1) along
with in-batch and pre-batch negatives (§4.2, §4.3). (b) With the top-k retrieved phrase representations from the
entire text corpus (§5), we further perform query-side fine-tuning to optimize the question encoder (§6). During
inference, our model simply returns the top-1 prediction.

taken from the [CLS] token representations re- answer (§4.1). Then, we introduce two different
spectively. Finally, Eq(·) simply takes their con- negative sampling methods (§4.2, §4.3), which en-
catenation: courage the phrase representations to be better dis-

Eq(q) = [qstart,qend] ∈ R2d. (4) criminated at the full Wikipedia scale. See Figure 1
for an overview of DensePhrases.

Note that we use pre-trained language models to
initialize Mp, Mq,start and Mq,end and they are 4.1 Single-passage Training
fine-tuned with the objectives that we will define To learn phrase representations in a single passage
later. In our pilot experiments, we found that Span- along with question representations, we first max-
BERT (Joshi et al., 2020) leads to superior perfor- imize the log-likelihood of the start and end posi-
mance compared to BERT (Devlin et al., 2019). tions of the gold phrase s∗ where TEXT(s∗) = a∗.
SpanBERT is designed to predict the information The training loss for predicting the start position of
in the entire span from its two endpoints, therefore a phrase given a question is computed as:
it is well suited for our phrase representations. In
our final model, we use SpanBERT-base-cased as zstart

1 , . . . , zstart
m = [h>1 q

start, . . . ,h>mqstart],
our base LMs for Es and Eq, and hence d = 768.5

P start = softmax(zstart
1 , . . . , zstart

See Table 5 for an ablation study. m ), (5)
Lstart = − logP start

start(s∗).
4 Learning Phrase Representations

We can define Lend in a similar way and the final
In this section, we start by learning dense phrase loss for the single-passage training is
representations from the supervision of reading
comprehension tasks, i.e., a single passage p con- L Lstart + Lend

single = . (6)
tains an answer a∗ to a question q. Our goal is to 2
learn strong dense representations of phrases for This essentially learns reading comprehension with-
s ∈ S(p), which can be retrieved by a dense rep- out any cross-attention between the passage and the
resentation of the question and serve as a direct question tokens, which fully decomposes phrase

5Our base model is largely inspired by DenSPI (Seo et al., and question representations.
2019), although we deviate from theirs as follows. (1) We
remove coherency scalars and don’t split any vectors. (2) Data augmentation Since the contextualized
DenSPI uses a shared encoder for phrases and questions while
we use 3 separate language models initialized from the same word representations h1, . . . ,hm are encoded in
pre-trained model. (3) We use SpanBERT instead of BERT. a query-agnostic way, they are always inferior to

ƾ



query-dependent representations in cross-attention Positive Negative

models (Devlin et al., 2019), where passages are gs Detached  in recent C batc
1tart gsta gstart

i hes
2 rt gs3tart gs4tart

fed along with the questions concatenated by a spe- qs1tart qs1tart

cial token such as [SEP]. We hypothesize that one qs2tart
qs2tart

key reason for the performance gap is that reading qs3tart

qs3tart qs4tart
comprehension datasets only provide a few anno-
tated questions in each passage, compared to the set qs4tart

of possible answer phrases. Learning from this su-
pervision is not easy to differentiate similar phrases (a) In-batch Negatives (B − 1) (b) Pre-batch Negatives (B × C )

in one passage (e.g., s∗ = Charles, Prince of Wales Figure 2: Two types of negative samples for the first
and another s = Prince George for a question q = batch item (qstart

1 ) in a mini-batch of size B = 4 and
Who is next in line to be the monarch of England?). C = 3. Note that the negative samples for the end

Following this intuition, we propose to use a sim- representations (qend
i ) are obtained in a similar manner.

ple model to generate additional questions for data See §4.2 and §4.3 for more details.
augmentation, based on a T5-large model (Raf-
fel et al., 2020). To train the question genera- at a larger scale. While Seo et al. (2019) simply
tion model, we feed a passage p with the gold sample two negative passages based on question
answer s∗ highlighted by inserting surrounding similarity, we use in-batch negatives for our dense
special tags. Then, the model is trained to max- phrase representations, which has been shown to be
imize the log-likelihood of the question words of effective in learning dense passage representations
q. After training, we extract all the named enti- before (Karpukhin et al., 2020).
ties in each training passage as candidate answers As shown in Figure 2 (a), for the i-th exam-
and feed the passage p with each candidate an- ple in a mini-batch of size B, we denote the
swer to generate questions. We keep the question- hidden representations of the gold start and end

positions hstart(s∗) and hend(s∗) as gstart
answer pairs only when a cross-attention reading i and

gend
comprehension model6 makes a correct prediction i , as well as the question representation as

[qstart
i ,qend

on the generated pair. The remaining generated QA i ]. Let Gstart,Gend,Qstart,Qend be the
pairs {(q̄1, s̄1), (q̄2, s̄2), . . . , (q̄r, s̄r)} are directly B × d matrices and each row corresponds to

gstart
i ,g end

i ,qstart
i ,qend

augmented to the original training set. i respectively. Basically, we
can treat all the gold phrases from other pas-

Distillation We also propose improving the sages in the same mini-batch as negative exam-
phrase representations by distilling knowledge ples. We compute Sstart = QstartGstartᵀ and Send =
from a cross-attention model (Hinton et al., 2015). QendGendᵀ and the i-th row of Sstart and Send return
We minimize the Kullback–Leibler divergence be- B scores each, including a positive score and B−1
tween the probability distribution from our phrase negative scores: sstart

1 , . . . , sstart
B and send

1 , . . . , send
B .

encoder and that from a standard SpanBERT-base Similar to Eq. (5), we can compute the loss func-
QA model. The loss is computed as follows: tion for the i-th example as:

P start_ib
i = softmax(sstart

1 , . . . , sstart
B ),

KL(P start||P start
c ) + KL(P end||P end

Ldistill = c )
,

2 P end_ib
i = softmax(send

1 , . . . , send
B ), (8)

(7) logP start_ib
i + logP end_ib

where P start (and P end) is defined in Eq. (5) and Lneg = − i ,
2

P start
c and P end

c denote the probability distributions
used to predict the start and end positions of an- We also attempted using non-gold phrases from
swers in the cross-attention model. other passages as negatives but did not find a mean-

ingful improvement.
4.2 In-batch Negatives

4.3 Pre-batch Negatives
Eventually, we need to build phrase representations
for billions of phrases. Therefore, a bigger chal- The in-batch negatives usually benefit from a large

lenge is to incorporate more phrases as negatives batch size (Karpukhin et al., 2020). However, it is

so the representations can be better discriminated challenging to further increase batch sizes, as they
are bounded by the size of GPU memory. Next,

6SpanBERT-large, 88.2 EM on SQuAD. we propose a novel negative sampling method



called pre-batch negatives, which can effectively size of phrase dump, we follow and modify several
utilize the representations from the preceding C techniques introduced in Seo et al. (2019) (see Ap-
mini-batches (Figure 2 (b)). In each iteration, we pendix E for details). After indexing, we can use
maintain a FIFO queue of C mini-batches to cache two rows i and j of H to represent a dense phrase
phrase representations Gstart and Gend. The cached representation [hi,hj ]. We use faiss (Johnson
phrase representations are then used as negative et al., 2017) for building a MIPS index of H.8
samples for the next iteration, providing B × C
additional negative samples in total.7

Search For a given question q, we can find the
answer ŝ as follows:

These pre-batch negatives are used together with
in-batch negatives and the training loss is the same ŝ = argmaxEs(s(i,j),D)>Eq(q),

s(i,j)
as Eq. (8), except that the gradients are not back-
propagated to the cached pre-batch negatives. After = argmax(Hqstart (10)

)i + (Hqend)j ,
s(i,j)

warming up the model with in-batch negatives, we
simply shift from in-batch negatives (B − 1 nega- where s(i,j) denotes a phrase with start and end
tives) to in-batch and pre-batch negatives (hence a indices as i and j in the index H. We can com-
total number ofB×C+B−1 negatives). For sim- pute the argmax of Hqstart and Hqend efficiently

by performing MIPS over H with qstart and qend
plicity, we use Lneg to denote the loss for both in- .
batch negatives and pre-batch negatives. Since we In practice, we search for the top-k start and top-k
do not retain the computational graph for pre-batch end positions separately and perform a constrained
negatives, the memory consumption of pre-batch search over their end and start positions respec-
negatives is much more manageable while allowing tively such that 1 ≤ i ≤ j < i+ L ≤ |W(D)|.
an increase in the number of negative samples. 6 Query-side Fine-tuning
4.4 Training Objective So far, we have created a phrase dump H that sup-
Finally, we optimize all the three losses together, on ports efficient MIPS search. In this section, we pro-
both annotated reading comprehension examples pose a novel method called query-side fine-tuning
and generated questions from §4.1: by only updating the question encoder Eq to cor-

rectly retrieve a desired answer a∗ for a question
L = λ1Lsingle + λ2Ldistill + λ3Lneg, (9) q given H. Formally speaking, we optimize the

marginal log-likelihood of the gold answer a∗where λ1, λ2, λ3 determine the importance of each for a
loss term. We found that λ1 = 1, λ question q, which resembles the weakly-supervised

2 = 2, and λ3 =
4 works well in practice. See Table 5 and Table 6 QA setting in previous work (Lee et al., 2019; Min
for an ablation study of different components. et al., 2019). For every question q, we retrieve top

k phrases and mi∑nimize the objective:
5 Indexing and Search ( )

∑ f
s∈S̃ (s|D,q)

Lquery = − log (q),TEXT(s)=(a∗ exp )
Indexing After training the phrase encoder Es,

,
s∈S̃(q) exp f(s|D,q)

we need to encode all the phrases S(D) in the en- (11)
tire English Wikipedia D and store an index of where f(s|D, q) is the score of the phrase s
the phrase dump. We segment each document (Eq. (2)) and S̃(q) denotes the top k phrases for
di ∈ D into a set of natural paragraphs, from q (Eq. (10)). In practice, we use k = 100 for all
which we obtain token representations for each the experiments.
paragraph using Es(·). Then, we build a phrase There are several advantages for doing this: (1)
dump H = [h1, . . . ,h|W(D)|] ∈ R|W(D)|×d by we find that query-side fine-tuning can reduce the
stacking the token representations from all the para- discrepancy between training and inference, and
graphs in D. Note that this process is computation- hence improve the final performance substantially
ally expensive and takes about hundreds of GPU (§8). Even with effective negative sampling, the
hours with a large disk footprint. To reduce the model only sees a small portion of passages com-

7This approach is inspired by the momentum contrast idea pared to the full scale of D and this training objec-
proposed in unsupervised visual representation learning (He tive can effectively fill in the gap. (2) This train-
et al., 2020). Contrary to their approach, we have separate ing strategy allows for transfer learning to unseen
encoders for phrases and questions and back-propagate to both
during training without a momentum update. 8We use IVFSQ4 with 1M clusters and set n-probe to 256.



domains, without rebuilding the entire phrase in- Model SQuAD NQ (Long)
dex. More specifically, the model is able to quickly EM F1 EM F1
adapt to new QA tasks (e.g., WebQuestions) when Query-Dependent
the phrase dump is built using SQuAD or Natural BERT-base 80.8 88.5 69.9 78.2
Questions. We also find that this can transfers to SpanBERT-base 85.7 92.2 73.2 81.0
non-QA tasks when the query is written in a dif- Query-Agnostic
ferent format. In §7.3, we show the possibility of DilBERT (Siblini et al., 2020) 63.0 72.0 - -
directly using DensePhrases for slot filling tasks DeFormer (Cao et al., 2020) - 72.1 - -
by using a query such as (Michael Jackson, is a DenSPI† 73.6 81.7 68.2 76.1

DenSPI + Sparc† 76.4 84.8 - -
singer of, x). In this regard, we can view our model DensePhrases (ours) 78.3 86.3 71.9 79.6
as a dense knowledge base that can be accessed
by many different types of queries and it is able to Table 2: Reading comprehension results, evaluated on
return phrase-level knowledge efficiently. the development sets of SQuAD and Natural Ques-

tions. Underlined numbers are estimated from the fig-
7 Experiments ures from the original papers. †: BERT-large model.

7.1 Setup or 10K training examples to see how rapidly our
model adapts to the new query types. See Ap-

Datasets. We use two reading comprehension pendix D for details on the hyperparameters and
datasets: SQuAD (Rajpurkar et al., 2016) and Nat- Appendix A for an analysis of computational cost.
ural Questions (NQ) (Kwiatkowski et al., 2019) to
learn phrase representations, in which a single gold 7.2 Experiments: Question Answering
passage is provided for each question. For the open-
domain QA experiments, we evaluate our approach Reading comprehension. In order to show the
on five popular open-domain QA datasets: Natu- effectiveness of our phrase representations, we first
ral Questions, WebQuestions (WQ) (Berant et al., evaluate our model in the reading comprehension
2013), CuratedTREC (TREC) (Baudiš and Šedivỳ, setting for SQuAD and NQ and report its perfor-
2015), TriviaQA (TQA) (Joshi et al., 2017), and mance with other query-agnostic models (Eq. (9)
SQuAD. Note that we only use SQuAD and/or NQ without query-side fine-tuning). This problem was
to build the phrase index and perform query-side originally formulated by Seo et al. (2018) as the
fine-tuning (§6) for other datasets. phrase-indexed question answering (PIQA) task.

We also evaluate our model on two slot filling Compared to previous query-agnostic models,
tasks, to show how to adapt our DensePhrases for our model achieves the best performance of 78.3
other knowledge-intensive NLP tasks. We focus EM on SQuAD by improving the previous phrase
on using two slot filling datasets from the KILT retrieval model (DenSPI) by 4.7% (Table 2). Al-
benchmark (Petroni et al., 2021): T-REx (Elsahar though it is still behind cross-attention models, the
et al., 2018) and zero-shot relation extraction (Levy gap has been greatly reduced and serves as a strong
et al., 2017). Each query is provided in the form starting point for the open-domain QA model.
of “{subject entity} [SEP] {relation}" and the
answer is the object entity. Appendix C provides Open-domain QA. Experimental results on
the statistics of all the datasets. open-domain QA are summarized in Table 3. With-

out any sparse representations, DensePhrases out-
Implementation details. We denote the training performs previous phrase retrieval models by a
datasets used for reading comprehension (Eq. (9)) large margin and achieves a 15%–25% absolute
as Cphrase. For open-domain QA, we train two ver- improvement on all datasets except SQuAD. Train-
sions of phrase encoders, each of which are trained ing the model of Lee et al. (2020) on Cphrase =
on Cphrase = {SQuAD} and {NQ,SQuAD}, re- {NQ,SQuAD} only increases the result from
spectively. We build the phrase dump H for the 14.5% to 16.5% on NQ, demonstrating that it does
2018-12-20 Wikipedia snapshot and perform query- not suffice to simply add more datasets for train-
side fine-tuning on each dataset using Eq. (11). For ing phrase representations. Our performance is
slot filling, we use the same phrase dump for open- also competitive with recent retriever-reader mod-
domain QA, Cphrase = {NQ,SQuAD} and perform els (Karpukhin et al., 2020), while running much
query-side fine-tuning on randomly sampled 5K faster during inference (Table 1).



Model NQ WQ TREC TQA SQuAD
Retriever-reader Cretr: (Pre-)Training
DrQA (Chen et al., 2017) - - 20.7 25.4 - 29.8
BERT + BM25 (Lee et al., 2019) - 26.5 17.7 21.3 47.1 33.2
ORQA (Lee et al., 2019) {Wiki.}† 33.3 36.4 30.1 45.0 20.2
REALMNews (Guu et al., 2020) {Wiki., CC-News}† 40.4 40.7 42.9 - -
DPR-multi (Karpukhin et al., 2020) {NQ, WQ, TREC, TQA} 41.5 42.4 49.4 56.8 24.1
Phrase retrieval Cphrase: Training
DenSPI (Seo et al., 2019) {SQuAD} 8.1∗ 11.1∗ 31.6∗ 30.7∗ 36.2
DenSPI + Sparc (Lee et al., 2020) {SQuAD} 14.5∗ 17.3∗ 35.7∗ 34.4∗ 40.7
DenSPI + Sparc (Lee et al., 2020) {NQ, SQuAD} 16.5 - - - -
DensePhrases (ours) {SQuAD} 31.2 36.3 50.3 53.6 39.4
DensePhrases (ours) {NQ, SQuAD} 40.9 37.5 51.0 50.7 38.0

Table 3: Open-domain QA results. We report exact match (EM) on the test sets. We also show the additional
training or pre-training datasets for learning the retriever models (Cretr) and creating the phrase dump (Cphrase). ∗:
no supervision using target training data (zero-shot). †: unlabeled data used for extra pre-training.

ll EM
Model T-REx ZsRE Model M Share Split QG Disti

Acc F1 Acc F1 DenSPI Bb. 3 3 7 7 70.2
Sb. 3 3 7 7 68.5

DPR + BERT - - 4.47 27.09 Bl. 3 3 7 7 73.6
DPR + BART 11.12 11.41 18.91 20.32
RAG 23.12 23.94 36.83 39.91 Dense Bb. 3 7 7 7 70.2

Phrases Bb. 7 7 7 7 71.9
DensePhrases5K 25.32 29.76 40.39 45.89 Sb. 7 7 7 7 73.2
DensePhrases10K 27.84 32.34 41.34 46.79 Sb. 7 7 3 7 76.3

Table 4: Slot filling results on the test sets of T-REx Sb. 7 7 3 3 78.3
and Zero shot RE (ZsRE) in the KILT benchmark. We
report KILT-AC and KILT-F1 (denoted as Acc and F1 Table 5: Ablation of DensePhrases on the development
in the table), which consider both span-level accuracy set of SQuAD. Bb: BERT-base, Sb: SpanBERT-base,
and correct retrieval of evidence documents. Bl: BERT-large. Share: whether question and phrase

encoders are shared or not. Split: whether the full
7.3 Experiments: Slot Filling hidden vectors are kept or split into start and end vec-

tors. QG: question generation (§4.1). Distill: distilla-
Table 4 summarizes the results on the two slot fill- tion (Eq.(7)). DenSPI (Seo et al., 2019) also included a
ing datasets, along with the baseline scores pro- coherency scalar and see their paper for more details.
vided by Petroni et al. (2021). The only extractive
baseline is DPR + BERT, which performs poorly
in zero-shot relation extraction. On the other hand, Effect of batch negatives. We further evaluate
our model achieves competitive performance on all the effectiveness of various negative sampling
datasets and achieves state-of-the-art performance methods introduced in §4.2 and §4.3. Since it is
on two datasets using only 5K training examples. computationally expensive to test each setting at
8 Analysis the full Wikipedia scale, we use a smaller text cor-

pus Dsmall of all the gold passages in the develop-
Ablation of phrase representations. Table 5 ment sets of Natural Questions, for the ablation
shows the ablation result of our model on SQuAD. study. Empirically, we find that results are gener-
Upon our choice of architecture, augmenting train- ally well correlated when we gradually increase the
ing set with generated questions (QG = 3) and size of |D|. As shown in Table 6, both in-batch
performing distillation from cross-attention mod- and pre-batch negatives bring substantial improve-
els (Distill = 3) improve performance up to EM = ments. While using a larger batch size (B = 84)
78.3. We attempted adding the generated questions is beneficial for in-batch negatives, the number of
to the training of the SpanBERT-QA model but preceding batches in pre-batch negatives is optimal
find a 0.3% improvement, which validates that data when C = 2. Surprisingly, the pre-batch negatives
sparsity is a bottleneck for query-agnostic models. also improve the performance when D = {p}.



Type B C D = {p} D = Dsmall QS NQ WQ TREC TQA SQuAD
None 48 - 70.4 35.3 Cphrase = {SQuAD}
+ In-batch 48 - 70.5 52.4 7 12.3 11.8 36.9 34.6 35.5

84 - 70.3 54.2 3 31.2 36.3 50.3 53.6 39.4
+ Pre-batch 84 1 71.6 59.8 Cphrase = {NQ}

84 2 71.9 60.4
84 4 71.2 59.8 7 32.6 21.1 32.3 32.4 20.7

3 40.9 37.1 49.7 49.2 25.7
Table 6: Effect of in-batch negatives and pre-batch neg- Cphrase = {NQ, SQuAD}
atives on the development set of Natural Questions. B:
batch size. C: number of preceding mini-batches used 7 28.9 18.9 34.9 31.9 33.2
in pre-batch negatives. Dsmall: all the gold passages in 3 40.9 37.5 51.0 50.7 38.0
the development set of NQ. {p}: single passage.

Table 7: Effect of query-side fine-tuning in
DensePhrases on each test set. We report EM of

Effect of query-side fine-tuning. We summa- each model before (QS = 7) and after (QS = 3) the
rize the effect of query-side fine-tuning in Table 7. query-side fine-tuning.
For the datasets that were not used for training the
phrase encoders (TQA, WQ, TREC), we observe 10 Conclusion
a 15% to 20% improvement after query-side fine-
tuning. Even for the datasets that have been used In this study, we show that we can learn dense repre-
(NQ, SQuAD), it leads to significant improvements sentations of phrases at the Wikipedia scale, which
(e.g., 32.6%→40.9% on NQ for Cphrase = {NQ}) are readily retrievable for open-domain QA and
and it clearly demonstrates it can effectively reduce other knowledge-intensive NLP tasks. We learn
the discrepancy between training and inference. both phrase and question encoders from the supervi-

sion of reading comprehension tasks and introduce
two batch-negative techniques to better discrimi-

9 Related Work nate phrases at scale. We also introduce query-side
fine-tuning that adapts our model to different types

Learning effective dense representations of words of queries. We achieve strong performance on five
is a long-standing goal in NLP (Bengio et al., 2003; popular open-domain QA datasets, while reducing
Collobert et al., 2011; Mikolov et al., 2013; Peters the storage footprint and improving latency signif-
et al., 2018; Devlin et al., 2019). Beyond words, icantly. We also achieve strong performance on
dense representations of many different granular- two slot filling datasets using only a small number
ities of text such as sentences (Le and Mikolov, of training examples, showing the possibility of
2014; Kiros et al., 2015) or documents (Yih et al., utilizing our DensePhrases as a knowledge base.
2011) have been explored. While dense phrase rep-
resentations have been also studied for statistical
machine translation (Cho et al., 2014) or syntactic Acknowledgments
parsing (Socher et al., 2010), our work focuses on We thank Sewon Min, Hyunjae Kim, Gyuwan
learning dense phrase representations for QA and Kim, Jungsoo Park, Zexuan Zhong, Dan Fried-
any other knowledge-intensive tasks where phrases man, Chris Sciavolino for providing valuable com-
can be easily retrieved by performing MIPS. ments and feedback. This research was supported

This type of dense retrieval has been also stud- by a grant of the Korea Health Technology R&D
ied for sentence and passage retrieval (Humeau Project through the Korea Health Industry Develop-
et al., 2019; Karpukhin et al., 2020) (see Lin et al., ment Institute (KHIDI), funded by the Ministry of
2020 for recent advances in dense retrieval). While Health & Welfare, Republic of Korea (grant num-
DensePhrases is explicitly designed to retrieve ber: HR20C0021) and National Research Foun-
phrases that can be used as an answer to given dation of Korea (NRF-2020R1A2C3010638). It
queries, retrieving phrases also naturally entails re- was also partly supported by the James Mi *91 Re-
trieving larger units of text, provided the datastore search Innovation Fund for Data Science and an
maintains the mapping between each phrase and Amazon Research Award.
the sentence and passage in which it occurs.



Ethical Considerations Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger

Our work builds on standard reading comprehen- Schwenk, and Yoshua Bengio. 2014. Learning
sion datasets such as SQuAD to build phrase rep- phrase representations using RNN encoder-decoder
resentations. SQuAD, in particular, is created for statistical machine translation. In Empirical
from a small number of Wikipedia articles sampled Methods in Natural Language Processing (EMNLP).

from top-10,000 most popular articles (measured Ronan Collobert, Jason Weston, Léon Bottou, Michael
by PageRanks), hence some of our models trained Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
only on SQuAD could be easily biased towards the Natural language processing (almost) from scratch.
small number of topics that SQuAD contains. We JMLR.

hope that excluding such datasets during training or Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
inventing an alternative pre-training procedure for Kristina Toutanova. 2019. BERT: Pre-training of
learning phrase representations could mitigate this deep bidirectional Transformers for language under-
problem. Although most of our efforts have been standing. In North American Chapter of the Associ-

ation for Computational Linguistics (NAACL).
made to reduce the computational complexity of
previous phrase retrieval models (further detailed Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci,
in Appendices A and E), leveraging our phrase re- Christophe Gravier, Jonathon Hare, Frederique
trieval model as a knowledge base will inevitably Laforest, and Elena Simperl. 2018. T-REx: A large

scale alignment of natural language with knowledge
increase the minimum requirement for the addi- base triples. In International Conference on Lan-
tional experiments. We plan to apply vector quanti- guage Resources and Evaluation (LREC).
zation techniques to reduce the additional cost of
using our model as a KB. David Ferrucci, Eric Brown, Jennifer Chu-Carroll,

James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al. 2010. Building Watson: An overview

References of the deepqa project. AI magazine, 31(3).
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,

Richard Socher, and Caiming Xiong. 2020. Learn- Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-
ing to retrieve reasoning paths over wikipedia graph supat, and Ming-Wei Chang. 2020. REALM:
for question answering. In International Conference Retrieval-augmented language model pre-training.
on Learning Representations (ICLR). In International Conference on Machine Learning

(ICML).
Petr Baudiš and Jan Šedivỳ. 2015. Modeling of the

question answering task in the YodaQA system. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
In International Conference of the Cross-Language Ross Girshick. 2020. Momentum contrast for un-
Evaluation Forum for European Languages (CLEF). supervised visual representation learning. In IEEE

Conference on Computer Vision and Pattern Recog-
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and nition (CVPR).

Christian Jauvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re- Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-
search (JMLR). Hsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Ku-

mar, Balint Miklos, and Ray Kurzweil. 2017. Effi-
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy cient natural language response suggestion for smart

Liang. 2013. Semantic parsing on Freebase from reply. arXiv preprint arXiv:1705.00652.
question-answer pairs. In Empirical Methods in Nat-
ural Language Processing (EMNLP). Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.

Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Distilling the knowledge in a neural network. arXiv
and Niranjan Balasubramanian. 2020. Deformer: preprint arXiv:1503.02531.
Decomposing pre-trained Transformers for faster
question answering. In Association for Computa- Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,
tional Linguistics (ACL). and Jason Weston. 2019. Poly-encoders: Architec-

tures and pre-training strategies for fast and accurate
Danqi Chen, Adam Fisch, Jason Weston, and Antoine multi-sentence scoring. In International Conference

Bordes. 2017. Reading Wikipedia to answer open- on Learning Representations (ICLR).
domain questions. In Association for Computa-
tional Linguistics (ACL). Gautier Izacard and Edouard Grave. 2021. Leveraging

passage retrieval with generative models for open
Danqi Chen and Wen-tau Yih. 2020. Open-domain domain question answering. In European Chap-

question answering. In Association for Computa- ter of the Association for Computational Linguistics
tional Linguistics (ACL). (EACL).



Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio
Billion-scale similarity search with GPUs. arXiv Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
preprint arXiv:1702.08734. rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-

täschel, et al. 2020. Retrieval-augmented generation
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, for knowledge-intensive nlp tasks. In Advances in

Luke Zettlemoyer, and Omer Levy. 2020. Span- Neural Information Processing Systems (NeurIPS).
BERT: Improving pre-training by representing and
predicting spans. Transactions of the Association of Jimmy Lin, Rodrigo Nogueira, and Andrew Yates.
Computational Linguistics (TACL). 2020. Pretrained Transformers for text rank-

ing: BERT and beyond. arXiv preprint
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke arXiv:2010.06467.

Zettlemoyer. 2017. TriviaQA: A large scale dis-
tantly supervised challenge dataset for reading com- Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and
prehension. In Association for Computational Lin- Nanda Kambhatla. 2003. tRuEcasIng. In Associa-
guistics (ACL). tion for Computational Linguistics (ACL).

Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, rado, and Jeff Dean. 2013. Distributed representa-
and Wen-tau Yih. 2020. Dense passage retrieval tions of words and phrases and their compositional-
for open-domain question answering. In Empirical ity. In Advances in Neural Information Processing
Methods in Natural Language Processing (EMNLP). Systems (NIPS).

Diederik P Kingma and Jimmy Ba. 2015. Adam: A Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and
method for stochastic optimization. In International Luke Zettlemoyer. 2019. A discrete hard EM ap-
Conference on Learning Representations (ICLR). proach for weakly supervised question answering.

In Empirical Methods in Natural Language Process-
Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, ing (EMNLP).

Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. Ad- Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
vances in Neural Information Processing Systems Gardner, Christopher Clark, Kenton Lee, and Luke
(NIPS). Zettlemoyer. 2018. Deep contextualized word repre-

sentations. In North American Chapter of the Asso-
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- ciation for Computational Linguistics (NAACL).

field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Kenton Lee, et al. 2019. Natural questions: a bench- Lewis, Majid Yazdani, Nicola De Cao, James
mark for question answering research. Transac- Thorne, Yacine Jernite, Vassilis Plachouras, Tim
tions of the Association of Computational Linguis- Rocktäschel, et al. 2021. KILT: a benchmark for
tics (TACL). knowledge intensive language tasks. In North Amer-

ican Chapter of the Association for Computational
Quoc Le and Tomas Mikolov. 2014. Distributed repre- Linguistics (NAACL).

sentations of sentences and documents. In Interna-
tional Conference on Machine Learning (ICML). Colin Raffel, Noam Shazeer, Adam Roberts, Katherine

Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Jinhyuk Lee, Minjoon Seo, Hannaneh Hajishirzi, and Wei Li, and Peter J Liu. 2020. Exploring the lim-

Jaewoo Kang. 2020. Contextualized sparse repre- its of transfer learning with a unified text-to-text
sentations for real-time open-domain question an- transformer. Journal of Machine Learning Research,
swering. In Association for Computational Linguis- 21(140).
tics (ACL).

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Percy Liang. 2016. SQuAD: 100,000+ questions for

2019. Latent retrieval for weakly supervised open machine comprehension of text. In Empirical Meth-
domain question answering. In Association for Com- ods in Natural Language Processing (EMNLP).
putational Linguistics (ACL).

Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
Kenton Lee, Shimi Salant, Tom Kwiatkowski, Ankur How much knowledge can you pack into the param-

Parikh, Dipanjan Das, and Jonathan Berant. 2017. eters of a language model? In Empirical Methods in
Learning recurrent span representations for extrac- Natural Language Processing (EMNLP).
tive question answering. In ICLR.

Minjoon Seo, Tom Kwiatkowski, Ankur Parikh, Ali
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Farhadi, and Hannaneh Hajishirzi. 2018. Phrase-

Zettlemoyer. 2017. Zero-shot relation extraction via indexed question answering: A new challenge for
reading comprehension. In Computational Natural scalable document comprehension. In Empirical
Language Learning (CoNLL). Methods in Natural Language Processing (EMNLP).



Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski,
Ankur P Parikh, Ali Farhadi, and Hannaneh Ha-
jishirzi. 2019. Real-time open-domain question an-
swering with dense-sparse phrase index. In Associa-
tion for Computational Linguistics (ACL).

Wissam Siblini, Mohamed Challal, and Charlotte
Pasqual. 2020. Delaying interaction layers
in transformer-based encoders for efficient open
domain question answering. arXiv preprint
arXiv:2010.08422.

Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase repre-
sentations and syntactic parsing with recursive neu-
ral networks. In Proceedings of the NIPS-2010 deep
learning and unsupervised feature learning work-
shop.

Ellen M Voorhees et al. 1999. The TREC-8 question
answering track report. In Trec.

Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallap-
ati, and Bing Xiang. 2019. Multi-passage BERT: A
globally normalized BERT model for open-domain
question answering. In Empirical Methods in Natu-
ral Language Processing (EMNLP).

Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen
Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.
End-to-end open-domain question answering with
bertserini. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).

Wen-tau Yih, Kristina Toutanova, John C Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Compu-
tational Natural Language Learning (CoNLL).



A Computational Cost Dataset Train Dev Test

We describe the resources and time spent dur- Natural Questions 79,168 8,757 3,610

ing inference (Table 1 and A.1) and indexing (Ta- WebQuestions 3,417 361 2,032
CuratedTrec 1,353 133 694

ble A.1). With our limited GPU resources (24GB TriviaQA 78,785 8,837 11,313
× 4), it takes about 20 hours for indexing the entire SQuAD 78,713 8,886 10,570
phrase representations. We also largely reduced the T-REx 2,284,168 5,000 5,000
storage from 1,547GB to 320GB by (1) removing Zero-Shot RE 147,909 3,724 4,966
sparse representations and (2) using our sharing and
split strategy. See Appendix E for the details on the Table C.3: Statistics of five open-domain QA datasets
reduction of storage footprint and Appendix B for and two slot filling datasets. We follow the same splits
the specification of our server for the benchmark. in open-domain QA for the two reading comprehension

datasets (SQuAD and Natural Questions).
Indexing Resources Storage Time
DPR 32GB GPU × 8 76GB 17h model and the batch size for the reader model is set
DenSPI + Sparc 24GB GPU × 4 1,547GB 85h to 8 to fit in the 24GB GPU (retriever batch size
DensePhrases 24GB GPU × 4 320GB 20h is still 64). For other hyperparameters, we use the
Inference RAM / GPU #Q/sec (GPU, CPU) default settings of each model. We also exclude the
DPR 86GB / 17GB 0.9, 0.04 time and the number of questions in the first five
DenSPI + Sparc 27GB / 2GB 2.1, 1.7 iterations for warming up each model. Note that
DensePhrases 12GB / 2GB 20.6, 13.6 despite our effort to match the environment of each

Table A.1: Complexity analysis of three open-domain model, their latency can be affected by various dif-
QA models during indexing and inference. For infer- ferent settings in their implementations such as the
ence, we also report the minimum requirement of RAM choice of library (PyTorch vs. Tensorflow).
and GPU memory for running each model with GPU.
For computing #Q/s for CPU, we do not use GPUs but C Data Statistics and Pre-processing
load all models on the RAM. In Table C.3, we show the statistics of five open-

domain QA datasets and two slot filling datasets.
B Server Specifications for Benchmark Pre-processed open-domain QA datasets are pro-
To compare the complexity of open-domain QA vided by Chen et al. (2017) except Natural Ques-
models, we install all models in Table 1 on the tions and TriviaQA. We use a version of Natural
same server using their public open-source code. Questions and TriviaQA provided by Min et al.
Our server has the following specifications: (2019); Lee et al. (2019), which are pre-processed

for the open-domain QA setting. Slot filling
Hardware datasets are provided by Petroni et al. (2021). We

use two reading comprehension datasets (SQuAD
Intel Xeon CPU E5-2630 v4 @ 2.20GHz and Natural Questions) for training our model on
128GB RAM Eq. (9). For SQuAD, we use the original dataset
12GB GPU (TITAN Xp) × 2
2TB 970 EVO Plus NVMe M.2 SSD × 1 provided by the authors (Rajpurkar et al., 2016).

For Natural Questions (Kwiatkowski et al., 2019),
Table B.2: Server specification for the benchmark we use the pre-processed version provided by Asai

et al. (2020).9 We use the short answer as a ground
For DPR, due to its large memory consumption, truth answer a∗ and its long answer as a gold pas-

we use a similar server with a 24GB GPU (TITAN sage p. We also match the gold passages in Natural
RTX). For all models, we use 1,000 randomly sam- Questions to the paragraphs in Wikipedia whenever
pled questions from the Natural Questions devel- possible. Since we want to check the performance
opment set for the speed benchmark and measure changes of our model with the growing number
#Q/sec. We set the batch size to 64 for all models of tokens, we follow the same split (train/dev/test)
except BERTSerini, ORQA and REALM, which used in Natural Questions-Open for the reading
do not allow a batch size of more than 1 in their comprehension setting as well. During the valida-
open-source implementations. #Q/sec for DPR in- 9https://github.com/AkariAsai/
cludes retrieving passages and running a reader learning_to_retrieve_reasoning_paths



tion of our model and baseline models, we exclude positions (trained together with Eq. (9)). We tune
samples whose answers lie in a list or a table from the threshold for the filter logits on the reading
a Wikipedia article. comprehension development set to the point where

the performance does not drop significantly while
D Hyperparameters maximally filtering tokens. In the full Wikipedia

setting, we filter about 75% of tokens and store
We use the Adam optimizer (Kingma and Ba, 2015)

770M token representations.
in all our experiments. For training our phrase and

Second, in our architecture, we use a base model
question encoders with Eq. (9), we use a learning

(SpanBERT-base) for a smaller dimension of token
rate of 3e-5 and the norm of the gradient is clipped

representations (d = 768) and does not use any
at 1. We use a batch size of B =84 and train each

sparse representations including tf-idf or contex-
model for 4 epochs for all datasets, where the loss

tualized sparse representations (Lee et al., 2020).
of pre-batch negatives is applied in the last two
epochs. We use SQuAD to train our QG model10 We also use the scalar quantization for storing

float32 vectors as int4 during indexing.
and use spaCy11 for extracting named entities in

Lastly, since the inference in Eq. (10) is purely
each training passage, which are used to generate

based on MIPS, we do not have to keep the original
questions. The number of generated questions is

start and end vectors which takes about 500GB.
327,302 and 1,126,354 for SQuAD and Natural

However, when we perform query-side fine-tuning,
Questions, respectively. The number of preceding

we need the original start and end vectors for re-
batches C is set to 2.

constructing them to compute Eq. (11) since (the
For the query-side fine-tuning with Eq. (11), we on-disk version of) MIPS index only returns the

use a learning rate of 3e-5 and the norm of the gra- top-k scores and their indices, but not the vectors.
dient is clipped at 1. We use a batch size of 12
and train each model for 10 epochs for all datasets.
The top k for the Eq. (11) is set to 100. While we
use a single 24GB GPU (TITAN RTX) for train-
ing the phrase encoders with Eq. (9), query-side
fine-tuning is relatively cheap and uses a single
12GB GPU (TITAN Xp). Using the development
set, we select the best performing model (based on
EM) for each dataset, which are then evaluated on
each test set. Since SpanBERT only supports cased
models, we also truecase the questions (Lita et al.,
2003) that are originally provided in the lowercase
(Natural Questions and WebQuestions).

E Reducing Storage Footprint
As shown in Table 1, we have reduced the stor-
age footprint from 1,547GB (Lee et al., 2020) to
320GB. We detail how we can reduce the storage
footprint in addition to the several techniques intro-
duced by Seo et al. (2019).

First, following Seo et al. (2019), we apply a
linear transformation on the passage token repre-
sentations to obtain a set of filter logits, which can
be used to filter many token representations from
W(D). This filter layer is supervised by applying
the binary cross entropy with the gold start/end

10The quality of generated questions from a QG model
trained on Natural Questions is worse due to the ambiguity of
information-seeking questions.

11https://spacy.io/