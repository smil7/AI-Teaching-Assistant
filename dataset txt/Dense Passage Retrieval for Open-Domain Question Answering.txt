Dense Passage Retrieval for Open-Domain Question Answering

Vladimir Karpukhin∗, Barlas Oğuz∗, Sewon Min†, Patrick Lewis,
Ledell Wu, Sergey Edunov, Danqi Chen‡, Wen-tau Yih

Facebook AI †University of Washington ‡Princeton University
{vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com

sewon@cs.washington.edu
danqic@cs.princeton.edu

Abstract Retrieval in open-domain QA is usually imple-
mented using TF-IDF or BM25 (Robertson and

Open-domain question answering relies on ef-
ficient passage retrieval to select candidate Zaragoza, 2009), which matches keywords effi-
contexts, where traditional sparse vector space ciently with an inverted index and can be seen
models, such as TF-IDF or BM25, are the de as representing the question and context in high-
facto method. In this work, we show that dimensional, sparse vectors (with weighting). Con-
retrieval can be practically implemented us- versely, the dense, latent semantic encoding is com-
ing dense representations alone, where em- plementary to sparse representations by design. For
beddings are learned from a small number example, synonyms or paraphrases that consist of
of questions and passages by a simple dual-
encoder framework. When evaluated on a completely different tokens may still be mapped to
wide range of open-domain QA datasets, our vectors close to each other. Consider the question
dense retriever outperforms a strong Lucene- “Who is the bad guy in lord of the rings?”, which can
BM25 system greatly by 9%-19% absolute in be answered from the context “Sala Baker is best
terms of top-20 passage retrieval accuracy, and known for portraying the villain Sauron in the Lord
helps our end-to-end QA system establish new of the Rings trilogy.” A term-based system would
state-of-the-art on multiple open-domain QA have difficulty retrieving such a context, while
benchmarks.1 a dense retrieval system would be able to better

1 Introduction match “bad guy” with “villain” and fetch the cor-
rect context. Dense encodings are also learnable

Open-domain question answering (QA) (Voorhees, by adjusting the embedding functions, which pro-
1999) is a task that answers factoid questions us- vides additional flexibility to have a task-specific
ing a large collection of documents. While early representation. With special in-memory data struc-
QA systems are often complicated and consist of tures and indexing schemes, retrieval can be done
multiple components (Ferrucci (2012); Moldovan efficiently using maximum inner product search
et al. (2003), inter alia), the advances of reading (MIPS) algorithms (e.g., Shrivastava and Li (2014);
comprehension models suggest a much simplified Guo et al. (2016)).
two-stage framework: (1) a context retriever first However, it is generally believed that learn-
selects a small subset of passages where some ing a good dense vector representation needs a
of them contain the answer to the question, and large number of labeled pairs of question and con-
then (2) a machine reader can thoroughly exam- texts. Dense retrieval methods have thus never
ine the retrieved contexts and identify the correct be shown to outperform TF-IDF/BM25 for open-
answer (Chen et al., 2017). Although reducing domain QA before ORQA (Lee et al., 2019), which
open-domain QA to machine reading is a very rea- proposes a sophisticated inverse cloze task (ICT)
sonable strategy, a huge performance degradation objective, predicting the blocks that contain the
is often observed in practice2, indicating the needs masked sentence, for additional pretraining. The
of improving retrieval. question encoder and the reader model are then fine-

∗Equal contribution tuned using pairs of questions and answers jointly.
1The code and trained models have been released at Although ORQA successfully demonstrates that

https://github.com/facebookresearch/DPR.
2For instance, the exact match score on SQuAD v1.1 drops dense retrieval can outperform BM25, setting new

from above 80% to less than 40% (Yang et al., 2019a). state-of-the-art results on multiple open-domain

arXiv:2004.04906v3  [cs.CL]  30 Sep 2020



QA datasets, it also suffers from two weaknesses. the extractive QA setting, in which the answer is
First, ICT pretraining is computationally intensive restricted to a span appearing in one or more pas-
and it is not completely clear that regular sentences sages in the corpus. Assume that our collection
are good surrogates of questions in the objective contains D documents, d1, d2, · · · , dD. We first
function. Second, because the context encoder is split each of the documents into text passages of
not fine-tuned using pairs of questions and answers, equal lengths as the basic retrieval units3 and getM
the corresponding representations could be subop- total passages in our corpus C = {p1, p2, . . . , pM},
timal. where each passage pi can be viewed as a sequence

In this paper, we address the question: can we of tokens (i) (i) (i)
w1 , w2 , · · · , w|p Given a q

i|. uestion q,
train a better dense embedding model using only the task is to find a span (i) (i) (i)

ws , w
pairs of questions and passages (or answers), with- s+1, · · · , we from

one of the passages pi that can answer the question.
out additional pretraining? By leveraging the now Notice that to cover a wide variety of domains, the
standard BERT pretrained model (Devlin et al., corpus size can easily range from millions of docu-
2019) and a dual-encoder architecture (Bromley ments (e.g., Wikipedia) to billions (e.g., the Web).
et al., 1994), we focus on developing the right As a result, any open-domain QA system needs to
training scheme using a relatively small number include an efficient retriever component that can se-
of question and passage pairs. Through a series lect a small set of relevant texts, before applying the
of careful ablation studies, our final solution is reader to extract the answer (Chen et al., 2017).4
surprisingly simple: the embedding is optimized Formally speaking, a retriever R : (q, C) → C
for maximizing inner products of the question and F

is a function that takes as input a question q and a
relevant passage vectors, with an objective compar- corpus C and returns a much smaller filter set of
ing all pairs of questions and passages in a batch. texts CF ⊂ C, where |CF | = k  |C|. For a fixed
Our Dense Passage Retriever (DPR) is exception-

k, a retriever can be evaluated in isolation on top-k
ally strong. It not only outperforms BM25 by a retrieval accuracy, which is the fraction of ques-
large margin (65.2% vs. 42.9% in Top-5 accuracy), tions for which CF contains a span that answers the
but also results in a substantial improvement on question.
the end-to-end QA accuracy compared to ORQA
(41.5% vs. 33.3%) in the open Natural Questions 3 Dense Passage Retriever (DPR)
setting (Lee et al., 2019; Kwiatkowski et al., 2019).

Our contributions are twofold. First, we demon- We focus our research in this work on improv-
strate that with the proper training setup, sim- ing the retrieval component in open-domain QA.
ply fine-tuning the question and passage encoders Given a collection of M text passages, the goal of
on existing question-passage pairs is sufficient to our dense passage retriever (DPR) is to index all
greatly outperform BM25. Our empirical results the passages in a low-dimensional and continuous
also suggest that additional pretraining may not be space, such that it can retrieve efficiently the top
needed. Second, we verify that, in the context of k passages relevant to the input question for the
open-domain question answering, a higher retrieval reader at run-time. Note that M can be very large
precision indeed translates to a higher end-to-end (e.g., 21 million passages in our experiments, de-
QA accuracy. By applying a modern reader model scribed in Section 4.1) and k is usually small, such
to the top retrieved passages, we achieve compara- as 20–100.
ble or better results on multiple QA datasets in the 3.1 Overview
open-retrieval setting, compared to several, much
complicated systems. Our dense passage retriever (DPR) uses a dense

encoder EP (·) which maps any text passage to a d-
2 Background dimensional real-valued vectors and builds an index

for all the M passages that we will use for retrieval.
The problem of open-domain QA studied in this 3
paper can be described as follows. Given a factoid The ideal size and boundary of a text passage are func-

tions of both the retriever and reader. We also experimented
question, such as “Who first voiced Meg on Family with natural paragraphs in our preliminary trials and found that
Guy?” or “Where was the 8th Dalai Lama born?”, a using fixed-length passages performs better in both retrieval

and final QA accuracy, as observed by Wang et al. (2019).
system is required to answer it using a large corpus 4Exceptions include (Seo et al., 2019) and (Roberts et al.,
of diversified topics. More specifically, we assume 2020), which retrieves and generates the answers, respectively.



At run-time, DPR applies a different encoderEQ(·) larity) than the irrelevant ones, by learning a better
that maps the input question to a d-dimensional embedding function.
vector, and retrieves k passages of which vectors Let D = {〈qi, p+i , p

−
i,1, · · · , p

−
i,n〉}mi=1 be the

are the closest to the question vector. We define training data that consists of m instances. Each
the similarity between the question and the passage instance contains one question qi and one relevant
using the dot product of their vectors: (positive) passage p+i , along with n irrelevant (neg-

ative) passages p−i,j . We optimize the loss function
sim(q, p) = EQ(q)

ᵀEP (p). (1) as the negative log likelihood of the positive pas-
sage:

Although more expressive model forms for measur-
ing the similarity between a question and a passage L(qi, p

+
i , p

−
i,1, · · · , p

−
i,n) (2)

do exist, such as networks consisting of multiple
= − esim

lo ∑(qi,p+i )
layers of cross attentions, the similarity function g .
needs to be decomposable so that the represen- esim(qi,p

+ n esim(qi,p
−

i ) + i,j)
j=1

tations of the collection of passages can be pre-
computed. Most decomposable similarity functions Positive and negative passages For retrieval
are some transformations of Euclidean distance problems, it is often the case that positive examples
(L2). For instance, cosine is equivalent to inner are available explicitly, while negative examples
product for unit vectors and the Mahalanobis dis- need to be selected from an extremely large pool.
tance is equivalent to L2 distance in a transformed For instance, passages relevant to a question may
space. Inner product search has been widely used be given in a QA dataset, or can be found using the
and studied, as well as its connection to cosine answer. All other passages in the collection, while
similarity and L2 distance (Mussmann and Ermon, not specified explicitly, can be viewed as irrelevant
2016; Ram and Gray, 2012). As our ablation study by default. In practice, how to select negative ex-
finds other similarity functions perform compara- amples is often overlooked but could be decisive
bly (Section 5.2; Appendix B), we thus choose for learning a high-quality encoder. We consider
the simpler inner product function and improve the three different types of negatives: (1) Random: any
dense passage retriever by learning better encoders. random passage from the corpus; (2) BM25: top

passages returned by BM25 which don’t contain
Encoders Although in principle the question and the answer but match most question tokens; (3)
passage encoders can be implemented by any neu- Gold: positive passages paired with other questions
ral networks, in this work we use two independent which appear in the training set. We will discuss the
BERT (Devlin et al., 2019) networks (base, un- impact of different types of negative passages and
cased) and take the representation at the [CLS] training schemes in Section 5.2. Our best model
token as the output, so d = 768. uses gold passages from the same mini-batch and

one BM25 negative passage. In particular, re-using
Inference During inference time, we apply the gold passages from the same batch as negatives
passage encoder EP to all the passages and index can make the computation efficient while achiev-
them using FAISS (Johnson et al., 2017) offline. ing great performance. We discuss this approach
FAISS is an extremely efficient, open-source li- below.
brary for similarity search and clustering of dense
vectors, which can easily be applied to billions of In-batch negatives Assume that we have B
vectors. Given a question q at run-time, we derive questions in a mini-batch and each one is asso-
its embedding vq = EQ(q) and retrieve the top k ciated with a relevant passage. Let Q and P be the
passages with embeddings closest to vq. (B×d) matrix of question and passage embeddings

in a batch of size B. S = QPT is a (B ×B) ma-
3.2 Training trix of similarity scores, where each row of which
Training the encoders so that the dot-product sim- corresponds to a question, paired with B passages.
ilarity (Eq. (1)) becomes a good ranking function In this way, we reuse computation and effectively
for retrieval is essentially a metric learning prob- train on B2 (qi, pj) question/passage pairs in each
lem (Kulis, 2013). The goal is to create a vector batch. Any (qi, pj) pair is a positive example when
space such that relevant pairs of questions and pas- i = j, and negative otherwise. This creates B train-
sages will have smaller distance (i.e., higher simi- ing instances in each batch, where there are B − 1



negative passages for each question. Dataset Train Dev Test
The trick of in-batch negatives has been used in Natural Questions 79,168 58,880 8,757 3,610

the full batch setting (Yih et al., 2011) and more TriviaQA 78,785 60,413 8,837 11,313
recently for mini-batch (Henderson et al., 2017; WebQuestions 3,417 2,474 361 2,032

CuratedTREC 1,353 1,125 133 694
Gillick et al., 2019). It has been shown to be an SQuAD 78,713 70,096 8,886 10,570
effective strategy for learning a dual-encoder model
that boosts the number of training examples. Table 1: Number of questions in each QA dataset. The

two columns of Train denote the original training ex-
4 Experimental Setup amples in the dataset and the actual questions used for

training DPR after filtering. See text for more details.
In this section, we describe the data we used for
experiments and the basic setup.

as well as various Web sources and is intended for
4.1 Wikipedia Data Pre-processing open-domain QA from unstructured corpora.

Following (Lee et al., 2019), we use the English SQuAD v1.1 (Rajpurkar et al., 2016) is a popu-

Wikipedia dump from Dec. 20, 2018 as the source lar benchmark dataset for reading comprehension.

documents for answering questions. We first apply Annotators were presented with a Wikipedia para-

the pre-processing code released in DrQA (Chen graph, and asked to write questions that could be

et al., 2017) to extract the clean, text-portion of answered from the given text. Although SQuAD

articles from the Wikipedia dump. This step re- has been used previously for open-domain QA re-

moves semi-structured data, such as tables, info- search, it is not ideal because many questions lack

boxes, lists, as well as the disambiguation pages. context in absence of the provided paragraph. We

We then split each article into multiple, disjoint text still include it in our experiments for providing

blocks of 100 words as passages, serving as our a fair comparison to previous work and we will

basic retrieval units, following (Wang et al., 2019), discuss more in Section 5.1.

which results in 21,015,324 passages in the end.5 Selection of positive passages Because only
Each passage is also prepended with the title of the pairs of questions and answers are provided in
Wikipedia article where the passage is from, along TREC, WebQuestions and TriviaQA6, we use the
with an [SEP] token. highest-ranked passage from BM25 that contains

the answer as the positive passage. If none of the
4.2 Question Answering Datasets top 100 retrieved passages has the answer, the ques-
We use the same five QA datasets and train- tion will be discarded. For SQuAD and Natural
ing/dev/testing splitting method as in previous Questions, since the original passages have been
work (Lee et al., 2019). Below we briefly describe split and processed differently than our pool of
each dataset and refer readers to their paper for the candidate passages, we match and replace each
details of data preparation. gold passage with the corresponding passage in the
Natural Questions (NQ) (Kwiatkowski et al., candidate pool.7 We discard the questions when
2019) was designed for end-to-end question an- the matching is failed due to different Wikipedia
swering. The questions were mined from real versions or pre-processing. Table 1 shows the num-
Google search queries and the answers were spans ber of questions in training/dev/test sets for all the
in Wikipedia articles identified by annotators. datasets and the actual questions used for training
TriviaQA (Joshi et al., 2017) contains a set of trivia the retriever.
questions with answers that were originally scraped
from the Web. 5 Experiments: Passage Retrieval
WebQuestions (WQ) (Berant et al., 2013) consists

In this section, we evaluate the retrieval perfor-
of questions selected using Google Suggest API,

mance of our Dense Passage Retriever (DPR),
where the answers are entities in Freebase.

along with analysis on how its output differs from
CuratedTREC (TREC) (Baudiš and Šedivỳ,
2015) sources questions from TREC QA tracks 6We use the unfiltered TriviaQA version and discard the

noisy evidence documents mined from Bing.
5However, Wang et al. (2019) also propose splitting docu- 7The improvement of using gold contexts over passages

ments into overlapping passages, which we do not find advan- that contain answers is small. See Section 5.2 and Ap-
tageous compared to the non-overlapping version. pendix A.



Training Retriever Top-20 Top-100
NQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD

None BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0

Single DPR 78.4 79.4 73.2 79.8 63.2 85.4 85.0 81.4 89.1 77.2
BM25 + DPR 76.6 79.8 71.0 85.2 71.5 83.8 84.5 80.5 92.7 81.3

Multi DPR 79.4 78.8 75.0 89.1 51.6 86.0 84.7 82.9 93.9 67.6
BM25 + DPR 78.0 79.9 74.7 88.5 66.2 83.9 84.4 82.3 94.1 78.6

Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved
passages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained
using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.

traditional retrieval methods, the effects of different 90

training schemes and the run-time efficiency.
The DPR model used in our main experiments 80

is trained using the in-batch negative setting (Sec-
tion 3.2) with a batch size of 128 and one additional 70

BM25 negative passage per question. We trained
the question and passage encoders for up to 40 60 BM25

# Train: 1k
epochs for large datasets (NQ, TriviaQA, SQuAD) # Train: 10k
and 100 epochs for small datasets (TREC, WQ), 50 # Train: 20k

# Train: 40k
with a learning rate of 10−5 using Adam, linear # Train: all (59k)
scheduling with warm-up and dropout rate 0.1. 40 20 40 60 80 100

While it is good to have the flexibility to adapt k: # of retrieved passages
the retriever to each dataset, it would also be de- Figure 1: Retriever top-k accuracy with different num-
sirable to obtain a single retriever that works well bers of training examples used in our dense passage re-
across the board. To this end, we train a multi- triever vs BM25. The results are measured on the de-
dataset encoder by combining training data from velopment set of Natural Questions. Our DPR trained
all datasets excluding SQuAD.8 In addition to DPR, using 1,000 examples already outperforms BM25.
we also present the results of BM25, the traditional
retrieval method9 and BM25+DPR, using a linear tiple datasets, TREC, the smallest dataset of the
combination of their scores as the new ranking five, benefits greatly from more training examples.
function. Specifically, we obtain two initial sets In contrast, Natural Questions and WebQuestions
of top-2000 passages based on BM25 and DPR, improve modestly and TriviaQA degrades slightly.
respectively, and rerank the union of them using Results can be improved further in some cases by
BM25(q,p) + λ · sim(q, p) as the ranking function. combining DPR with BM25 in both single- and
We used λ = 1.1 based on the retrieval accuracy in multi-dataset settings.
the development set.

We conjecture that the lower performance on
5.1 Main Results SQuAD is due to two reasons. First, the annota-
Table 2 compares different passage retrieval sys- tors wrote questions after seeing the passage. As
tems on five QA datasets, using the top-k accuracy a result, there is a high lexical overlap between
(k ∈ {20, 100}). With the exception of SQuAD, passages and questions, which gives BM25 a clear
DPR performs consistently better than BM25 on advantage. Second, the data was collected from
all datasets. The gap is especially large when k is only 500+ Wikipedia articles and thus the distribu-
small (e.g., 78.4% vs. 59.1% for top-20 accuracy tion of training examples is extremely biased, as
on Natural Questions). When training with mul- argued previously by Lee et al. (2019).

8SQuAD is limited to a small set of Wikipedia documents 5.2 Ablation Study on Model Training
and thus introduces unwanted bias. We will discuss this issue
more in Section 5.1.

9 To understand further how different model training
Lucene implementation. BM25 parameters b = 0.4 (doc-

ument length normalization) and k1 = 0.9 (term frequency options affect the results, we conduct several addi-
scaling) are tuned using development sets. tional experiments and discuss our findings below.

Top-k accuracy (%)



Sample efficiency We explore how many train- Type #N IB Top-5 Top-20 Top-100
ing examples are needed to achieve good passage Random 7 7 47.0 64.3 77.8
retrieval performance. Figure 1 illustrates the top-k BM25 7 7 50.0 63.3 74.8
retrieval accuracy with respect to different num- Gold 7 7 42.6 63.1 78.3

bers of training examples, measured on the devel- Gold 7 3 51.1 69.1 80.8
Gold 31 3 52.1 70.8 82.1

opment set of Natural Questions. As is shown, a Gold 127 3 55.8 73.0 83.1
dense passage retriever trained using only 1,000 ex-

G.+BM25(1) 31+32 3 65.0 77.3 84.4
amples already outperforms BM25. This suggests G.+BM25(2) 31+64 3 64.5 76.4 84.0
that with a general pretrained language model, it is G.+BM25(1) 127+128 3 65.8 78.0 84.9
possible to train a high-quality dense retriever with
a small number of question–passage pairs. Adding Table 3: Comparison of different training schemes,
more training examples (from 1k to 59k) further measured as top-k retrieval accuracy on Natural Ques-
improves the retrieval accuracy consistently. tions (development set). #N: number of negative

examples, IB: in-batch training. G.+BM25(1) and
G.+BM25(2)In-batch negative training We test different denote in-batch training with 1 or 2 ad-
ditional BM25 negatives, which serve as negative pas-

training schemes on the development set of Natural sages for all questions in the batch.
Questions and summarize the results in Table 3.
The top block is the standard 1-of-N training set-
ting, where each question in the batch is paired Our experiments on Natural Questions show that
with a positive passage and its own set of n neg- switching to distantly-supervised passages (using
ative passages (Eq. (2)). We find that the choice the highest-ranked BM25 passage that contains the
of negatives — random, BM25 or gold passages answer), has only a small impact: 1 point lower
(positive passages from other questions) — does top-k accuracy for retrieval. Appendix A contains
not impact the top-k accuracy much in this setting more details.
when k ≥ 20. Similarity and loss Besides dot product, cosine

The middle bock is the in-batch negative training and Euclidean L2 distance are also commonly used
(Section 3.2) setting. We find that using a similar as decomposable similarity functions. We test these
configuration (7 gold negative passages), in-batch alternatives and find that L2 performs compara-
negative training improves the results substantially. ble to dot product, and both of them are superior
The key difference between the two is whether the to cosine. Similarly, in addition to negative log-
gold negative passages come from the same batch likelihood, a popular option for ranking is triplet
or from the whole training set. Effectively, in-batch loss, which compares a positive passage and a nega-
negative training is an easy and memory-efficient tive one directly with respect to a question (Burges
way to reuse the negative examples already in the et al., 2005). Our experiments show that using
batch rather than creating new ones. It produces triplet loss does not affect the results much. More
more pairs and thus increases the number of train- details can be found in Appendix B.
ing examples, which might contribute to the good
model performance. As a result, accuracy consis- Cross-dataset generalization One interesting
tently improves as the batch size grows. question regarding DPR’s discriminative training

Finally, we explore in-batch negative training is how much performance degradation it may suf-
with additional “hard” negative passages that have fer from a non-iid setting. In other words, can
high BM25 scores given the question, but do not it still generalize well when directly applied to
contain the answer string (the bottom block). These a different dataset without additional fine-tuning?
additional passages are used as negative passages To test the cross-dataset generalization, we train
for all questions in the same batch. We find that DPR on Natural Questions only and test it directly
adding a single BM25 negative passage improves on the smaller WebQuestions and CuratedTREC
the result substantially while adding two does not datasets. We find that DPR generalizes well, with
help further. 3-5 points loss from the best performing fine-tuned

model in top-20 retrieval accuracy (69.9/86.3 vs.
Impact of gold passages We use passages that 75.0/89.1 for WebQuestions and TREC, respec-
match the gold contexts in the original datasets tively), while still greatly outperforming the BM25
(when available) as positive examples (Section 4.2). baseline (55.0/70.9).



5.3 Qualitative Analysis score is chosen as the final answer. The passage
Although DPR performs better than BM25 in gen- selection model serves as a reranker through cross-
eral, passages retrieved by these two methods dif- attention between the question and the passage. Al-
fer qualitatively. Term-matching methods like though cross-attention is not feasible for retrieving
BM25 are sensitive to highly selective keywords relevant passages in a large corpus due to its non-
and phrases, while DPR captures lexical variations decomposable nature, it has more capacity than the
or semantic relationships better. See Appendix C dual-encoder model sim(q, p) as in Eq. (1). Apply-
for examples and more discussion. ing it to selecting the passage from a small number

of retrieved candidates has been shown to work
5.4 Run-time Efficiency well (Wang et al., 2019, 2018; Lin et al., 2018).
The main reason that we require a retrieval compo- Specifically, let Pi ∈ RL×h (1 ≤ i ≤ k) be
nent for open-domain QA is to reduce the number a BERT (base, uncased in our experiments) rep-
of candidate passages that the reader needs to con- resentation for the i-th passage, where L is the
sider, which is crucial for answering user’s ques- maximum length of the passage and h the hidden
tions in real-time. We profiled the passage retrieval dimension. The probabilities of a token being the
speed on a server with Intel Xeon CPU E5-2698 v4 starting/ending positions of an answer span and a
@ 2.20GHz and 512GB memory. With the help of passage being selected are defi(ned as:
FAISS in-memory index for real-valued vectors10,
DPR can be made incredibly efficient, processing Pstart,i(s) = softmax( )

Piwstart) , (3)
s

995.0 questions per second, returning top 100 pas- Pend,i(t) = softmax(Piwend , (4)
t

sages per question. In contrast, BM25/Lucene (im- )
Pselected(i) = softmax P̂ᵀwselected , (5)

plemented in Java, using file index) processes 23.7 i

questions per second per CPU thread. where [CLS] [CLS]
P̂ = [P ×

1 , . . . ,Pk ] ∈ Rh k and
On the other hand, the time required for building wstart,wend,wselected ∈ Rh are learnable vectors.

an index for dense vectors is much longer. Com- We compute a span score of the s-th to t-th words
puting dense embeddings on 21-million passages from the i-th passage as Pstart,i(s)× Pend,i(t), and
is resource intensive, but can be easily parallelized, a passage selection score of the i-th passage as
taking roughly 8.8 hours on 8 GPUs. However, Pselected(i).
building the FAISS index on 21-million vectors During training, we sample one positive and
on a single server takes 8.5 hours. In comparison, m̃−1 negative passages from the top 100 passages
building an inverted index using Lucene is much returned by the retrieval system (BM25 or DPR)
cheaper and takes only about 30 minutes in total. for each question. m̃ is a hyper-parameter and we

6 Experiments: Question Answering use m̃ = 24 in all the experiments. The training ob-
jective is to maximize the marginal log-likelihood

In this section, we experiment with how different of all the correct answer spans in the positive pas-
passage retrievers affect the final QA accuracy. sage (the answer string may appear multiple times

in one passage), combined with the log-likelihood
6.1 End-to-end QA System of the positive passage being selected. We use the
We implement an end-to-end question answering batch size of 16 for large (NQ, TriviaQA, SQuAD)
system in which we can plug different retriever and 4 for small (TREC, WQ) datasets, and tune k
systems directly. Besides the retriever, our QA sys- on the development set. For experiments on small
tem consists of a neural reader that outputs the datasets under the Multi setting, in which using
answer to the question. Given the top k retrieved other datasets is allowed, we fine-tune the reader
passages (up to 100 in our experiments), the reader trained on Natural Questions to the target dataset.
assigns a passage selection score to each passage. All experiments were done on eight 32GB GPUs.
In addition, it extracts an answer span from each
passage and assigns a span score. The best span 6.2 Results
from the passage with the highest passage selection Table 4 summarizes our final end-to-end QA re-

10 sults, measured by exact match with the reference
FAISS configuration: we used HNSW index type on CPU,

neighbors to store per node = 512, construction time search answer after minor normalization as in (Chen et al.,
depth = 200, search depth = 128. 2017; Lee et al., 2019). From the table, we can



Training Model NQ TriviaQA WQ TREC SQuAD
Single BM25+BERT (Lee et al., 2019) 26.5 47.1 17.7 21.3 33.2
Single ORQA (Lee et al., 2019) 33.3 45.0 36.4 30.1 20.2
Single HardEM (Min et al., 2019a) 28.1 50.9 - - -
Single GraphRetriever (Min et al., 2019b) 34.5 56.0 36.4 - -
Single PathRetriever (Asai et al., 2020) 32.6 - - - 56.5
Single REALMWiki (Guu et al., 2020) 39.2 - 40.2 46.8 -
Single REALMNews (Guu et al., 2020) 40.4 - 40.7 42.9 -

BM25 32.6 52.4 29.9 24.9 38.1
Single DPR 41.5 56.8 34.6 25.9 29.8

BM25+DPR 39.0 57.0 35.2 28.0 36.7
DPR 41.5 56.8 42.4 49.4 24.1

Multi
BM25+DPR 38.8 57.9 41.1 50.6 35.8

Table 4: End-to-end QA (Exact Match) Accuracy. The first block of results are copied from their cited papers.
REALMWiki and REALMNews are the same model but pretrained on Wikipedia and CC-News, respectively. Single
and Multi denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets
(all except SQuAD). For WQ and TREC in the Multi setting, we fine-tune the reader trained on NQ.

see that higher retriever accuracy typically leads to trained, following Lee et al. (2019). This approach
better final QA results: in all cases except SQuAD, obtains a score of 39.8 EM, which suggests that our
answers extracted from the passages retrieved by strategy of training a strong retriever and reader in
DPR are more likely to be correct, compared to isolation can leverage effectively available supervi-
those from BM25. For large datasets like NQ and sion, while outperforming a comparable joint train-
TriviaQA, models trained using multiple datasets ing approach with a simpler design (Appendix D).
(Multi) perform comparably to those trained using One thing worth noticing is that our reader does
the individual training set (Single). Conversely, consider more passages compared to ORQA, al-
on smaller datasets like WQ and TREC, the multi- though it is not completely clear how much more
dataset setting has a clear advantage. Overall, our time it takes for inference. While DPR processes
DPR-based models outperform the previous state- up to 100 passages for each question, the reader
of-the-art results on four out of the five datasets, is able to fit all of them into one batch on a sin-
with 1% to 12% absolute differences in exact match gle 32GB GPU, thus the latency remains almost
accuracy. It is interesting to contrast our results to identical to the single passage case (around 20ms).
those of ORQA (Lee et al., 2019) and also the The exact impact on throughput is harder to mea-
concurrently developed approach, REALM (Guu sure: ORQA uses 2-3x longer passages compared
et al., 2020). While both methods include addi- to DPR (288 word pieces compared to our 100
tional pretraining tasks and employ an expensive tokens) and the computational complexity is super-
end-to-end training regime, DPR manages to out- linear in passage length. We also note that we
perform them on both NQ and TriviaQA, simply found k = 50 to be optimal for NQ, and k = 10
by focusing on learning a strong passage retrieval leads to only marginal loss in exact match accu-
model using pairs of questions and answers. The racy (40.8 vs. 41.5 EM on NQ), which should be
additional pretraining tasks are likely more useful roughly comparable to ORQA’s 5-passage setup.
only when the target training sets are small. Al-
though the results of DPR on WQ and TREC in the 7 Related Work
single-dataset setting are less competitive, adding Passage retrieval has been an important compo-
more question–answer pairs helps boost the perfor- nent for open-domain QA (Voorhees, 1999). It
mance, achieving the new state of the art. not only effectively reduces the search space for

To compare our pipeline training approach with answer extraction, but also identifies the support
joint learning, we run an ablation on Natural Ques- context for users to verify the answer. Strong sparse
tions where the retriever and reader are jointly vector space models like TF-IDF or BM25 have



been used as the standard method applied broadly effective solution that shows stronger empirical per-
to various QA tasks (e.g., Chen et al., 2017; Yang formance, without relying on additional pretraining
et al., 2019a,b; Nie et al., 2019; Min et al., 2019a; or complex joint training schemes.
Wolfson et al., 2020). Augmenting text-based re- DPR has also been used as an important mod-
trieval with external structured information, such ule in very recent work. For instance, extending
as knowledge graph and Wikipedia hyperlinks, has the idea of leveraging hard negatives, Xiong et al.
also been explored recently (Min et al., 2019b; Asai (2020a) use the retrieval model trained in the pre-
et al., 2020). vious iteration to discover new negatives and con-

The use of dense vector representations for re- struct a different set of examples in each training
trieval has a long history since Latent Semantic iteration. Starting from our trained DPR model,
Analysis (Deerwester et al., 1990). Using labeled they show that the retrieval performance can be
pairs of queries and documents, discriminatively further improved. Recent work (Izacard and Grave,
trained dense encoders have become popular re- 2020; Lewis et al., 2020b) have also shown that
cently (Yih et al., 2011; Huang et al., 2013; Gillick DPR can be combined with generation models
et al., 2019), with applications to cross-lingual such as BART (Lewis et al., 2020a) and T5 (Raf-
document retrieval, ad relevance prediction, Web fel et al., 2019), achieving good performance on
search and entity retrieval. Such approaches com- open-domain QA and other knowledge-intensive
plement the sparse vector methods as they can po- tasks.
tentially give high similarity scores to semantically 8 Conclusion
relevant text pairs, even without exact token match-
ing. The dense representation alone, however, is In this work, we demonstrated that dense retrieval
typically inferior to the sparse one. While not the can outperform and potentially replace the tradi-
focus of this work, dense representations from pre- tional sparse retrieval component in open-domain
trained models, along with cross-attention mecha- question answering. While a simple dual-encoder
nisms, have also been shown effective in passage approach can be made to work surprisingly well,
or dialogue re-ranking tasks (Nogueira and Cho, we showed that there are some critical ingredients
2019; Humeau et al., 2020). Finally, a concurrent to training a dense retriever successfully. Moreover,
work (Khattab and Zaharia, 2020) demonstrates our empirical analysis and ablation studies indicate
the feasibility of full dense retrieval in IR tasks. that more complex model frameworks or similarity
Instead of employing the dual-encoder framework, functions do not necessarily provide additional val-
they introduced a late-interaction operator on top ues. As a result of improved retrieval performance,
of the BERT encoders. we obtained new state-of-the-art results on multiple

open-domain question answering benchmarks.
Dense retrieval for open-domain QA has been

explored by Das et al. (2019), who propose to re- Acknowledgments
trieve relevant passages iteratively using reformu-
lated question vectors. As an alternative approach We thank the anonymous reviewers for their helpful
that skips passage retrieval, Seo et al. (2019) pro- comments and suggestions.
pose to encode candidate answer phrases as vectors
and directly retrieve the answers to the input ques- References
tions efficiently. Using additional pretraining with
the objective that matches surrogates of questions Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,
and relevant passages, Lee et al. (2019) jointly train Richard Socher, and Caiming Xiong. 2020. Learn-

ing to retrieve reasoning paths over Wikipedia graph
the question encoder and reader. Their approach for question answering. In International Conference
outperforms the BM25 plus reader paradigm on on Learning Representations (ICLR).
multiple open-domain QA datasets in QA accuracy,

Petr Baudiš and Jan Šedivỳ. 2015. Modeling of the
and is further extended by REALM (Guu et al., question answering task in the yodaqa system. In In-
2020), which includes tuning the passage encoder ternational Conference of the Cross-Language Eval-
asynchronously by re-indexing the passages dur- uation Forum for European Languages, pages 222–
ing training. The pretraining objective has also 228. Springer.
recently been improved by Xiong et al. (2020b). Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
In contrast, our model provides a simple and yet Liang. 2013. Semantic parsing on Freebase from



question-answer pairs. In Empirical Methods in Nat- clickthrough data. In ACM International Confer-
ural Language Processing (EMNLP). ence on Information and Knowledge Management

(CIKM), pages 2333–2338.
Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard

Säckinger, and Roopak Shah. 1994. Signature verifi- Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,
cation using a “Siamese” time delay neural network. and Jason Weston. 2020. Poly-encoders: Architec-
In NIPS, pages 737–744. tures and pre-training strategies for fast and accurate

multi-sentence scoring. In International Conference
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, on Learning Representations (ICLR).

Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In Gautier Izacard and Edouard Grave. 2020. Leveraging
Proceedings of the 22nd international conference on passage retrieval with generative models for open do-
Machine learning, pages 89–96. main question answering. ArXiv, abs/2007.01282.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open- Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.
domain questions. In Association for Computa- Billion-scale similarity search with GPUs. ArXiv,
tional Linguistics (ACL), pages 1870–1879. abs/1702.08734.

Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
and Andrew McCallum. 2019. Multi-step retriever- Zettlemoyer. 2017. TriviaQA: A large scale dis-
reader interaction for scalable open-domain question tantly supervised challenge dataset for reading com-
answering. In International Conference on Learn- prehension. In Association for Computational Lin-
ing Representations (ICLR). guistics (ACL), pages 1601–1611.

Scott Deerwester, Susan T Dumais, George W Fur- Omar Khattab and Matei Zaharia. 2020. ColBERT:
nas, Thomas K Landauer, and Richard Harshman. Efficient and effective passage search via contextu-
1990. Indexing by latent semantic analysis. Jour- alized late interaction over BERT. In ACM SIGIR
nal of the American society for information science, Conference on Research and Development in Infor-
41(6):391–407. mation Retrieval (SIGIR), pages 39–48.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Brian Kulis. 2013. Metric learning: A survey. Foun-
Kristina Toutanova. 2019. BERT: Pre-training of dations and Trends in Machine Learning, 5(4):287–
deep bidirectional transformers for language under- 364.
standing. In North American Association for Com-
putational Linguistics (NAACL). Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-

field, Michael Collins, Ankur Parikh, Chris Alberti,
David A Ferrucci. 2012. Introduction to “This is Wat- Danielle Epstein, Illia Polosukhin, Matthew Kelcey,

son”. IBM Journal of Research and Development, Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
56(3.4):1–1. Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob

Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
Daniel Gillick, Sayali Kulkarni, Larry Lansing, ral questions: a benchmark for question answering

Alessandro Presta, Jason Baldridge, Eugene Ie, and research. Transactions of the Association of Compu-
Diego Garcia-Olano. 2019. Learning dense repre- tational Linguistics (TACL).
sentations for entity retrieval. In Computational Nat-
ural Language Learning (CoNLL). Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.

Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and 2019. Latent retrieval for weakly supervised open
David Simcha. 2016. Quantization based fast inner domain question answering. In Association for Com-
product search. In Artificial Intelligence and Statis- putational Linguistics (ACL), pages 6086–6096.
tics, pages 482–490. Mike Lewis, Yinhan Liu, Naman Goyal, Mar-

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa- jan Ghazvininejad, Abdelrahman Mohamed, Omer
supat, and Ming-Wei Chang. 2020. REALM: Levy, Veselin Stoyanov, and Luke Zettlemoyer.
Retrieval-augmented language model pre-training. 2020a. BART: Denoising sequence-to-sequence pre-
ArXiv, abs/2002.08909. training for natural language generation, translation,

and comprehension. In Association for Computa-
Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun- tional Linguistics (ACL), pages 7871–7880.

hsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Ku-
mar, Balint Miklos, and Ray Kurzweil. 2017. Effi- Patrick Lewis, Ethan Perez, Aleksandara Piktus,
cient natural language response suggestion for smart Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
reply. ArXiv, abs/1705.00652. Heinrich Küttler, Mike Lewis, Wen-tau Yih,

Tim Rocktäschel, Sebastian Riedel, and Douwe
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Kiela. 2020b. Retrieval-augmented generation for

Alex Acero, and Larry Heck. 2013. Learning deep knowledge-intensive NLP tasks. In Advances in
structured semantic models for Web search using Neural Information Processing Systems (NeurIPS).



Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. Anshumali Shrivastava and Ping Li. 2014. Asymmet-
2018. Denoising distantly supervised open-domain ric LSH (ALSH) for sublinear time maximum inner
question answering. In Association for Computa- product search (MIPS). In Advances in Neural In-
tional Linguistics (ACL), pages 1736–1745. formation Processing Systems (NIPS), pages 2321–

2329.
Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and

Luke Zettlemoyer. 2019a. A discrete hard EM ap- Ellen M Voorhees. 1999. The TREC-8 question an-
proach for weakly supervised question answering. swering track report. In TREC, volume 99, pages
In Empirical Methods in Natural Language Process- 77–82.
ing (EMNLP).

Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,
Sewon Min, Danqi Chen, Luke Zettlemoyer, and Han- Tim Klinger, Wei Zhang, Shiyu Chang, Gerry

naneh Hajishirzi. 2019b. Knowledge guided text re- Tesauro, Bowen Zhou, and Jing Jiang. 2018. Rˆ3:
trieval and reading for open domain question answer- Reinforced ranker-reader for open-domain question
ing. ArXiv, abs/1911.03868. answering. In Conference on Artificial Intelligence

(AAAI).
Dan Moldovan, Marius Paşca, Sanda Harabagiu, and

Mihai Surdeanu. 2003. Performance issues and er- Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nalla-
ror analysis in an open-domain question answering pati, and Bing Xiang. 2019. Multi-passage BERT:
system. ACM Transactions on Information Systems A globally normalized bert model for open-domain
(TOIS), 21(2):133–154. question answering. In Empirical Methods in Natu-

Stephen Mussmann and Stefano Ermon. 2016. Learn- ral Language Processing (EMNLP).
ing and inference via maximum inner product search. Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-
In International Conference on Machine Learning ner, Yoav Goldberg, Daniel Deutch, and Jonathan
(ICML), pages 2587–2596. Berant. 2020. Break it down: A question under-

Yixin Nie, Songhe Wang, and Mohit Bansal. 2019. Re- standing benchmark. Transactions of the Associa-
vealing the importance of semantic retrieval for ma- tion of Computational Linguistics (TACL).
chine reading at scale. In Empirical Methods in Nat- Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
ural Language Processing (EMNLP). Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold

Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Overwijk. 2020a. Approximate nearest neighbor
re-ranking with BERT. ArXiv, abs/1901.04085. negative contrastive learning for dense text retrieval.

ArXiv, abs/2007.00808.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine

Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wenhan Xiong, Hankang Wang, and William Yang
Wei Li, and Peter J Liu. 2019. Exploring the limits Wang. 2020b. Progressively pretrained dense corpus
of transfer learning with a unified text-to-text trans- index for open-domain question answering. ArXiv,
former. ArXiv, abs/1910.10683. abs/2005.00038.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen
Percy Liang. 2016. SQuAD: 100,000+ questions Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a.
for machine comprehension of text. In Empirical End-to-end open-domain question answering with
Methods in Natural Language Processing (EMNLP), bertserini. In North American Association for Com-
pages 2383–2392. putational Linguistics (NAACL), pages 72–77.

Parikshit Ram and Alexander G Gray. 2012. Maximum Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming
inner-product search using cone trees. In Proceed- Li, and Jimmy Lin. 2019b. Data augmentation for
ings of the 18th ACM SIGKDD international con- bert fine-tuning in open-domain question answering.
ference on Knowledge discovery and data mining, ArXiv, abs/1904.06652.
pages 931–939.

Wen-tau Yih, Kristina Toutanova, John C Platt, and
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. Christopher Meek. 2011. Learning discriminative

How much knowledge can you pack into the param- projections for text similarity measures. In Com-
eters of a language model? ArXiv, abs/2002.08910. putational Natural Language Learning (CoNLL),

pages 247–256.
Stephen Robertson and Hugo Zaragoza. 2009. The

probabilistic relevance framework: BM25 and be-
yond. Foundations and Trends in Information Re-
trieval, 3(4):333–389.

Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur
Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.
Real-time open-domain question answering with
dense-sparse phrase index. In Association for Com-
putational Linguistics (ACL).



A Distant Supervision Top-1 Top-5 Top-20 Top-100
When training our final DPR model using Natural Gold 44.9 66.8 78.1 85.0
Questions, we use the passages in our collection Dist. Sup. 43.9 65.3 77.1 84.4
that best match the gold context as the positive
passages. As some QA datasets contain only the Table 5: Retrieval accuracy on the development set of
question and answer pairs, it is thus interesting Natural Questions, trained on passages that match the

to see when using the passages that contain the gold context (Gold) or the top BM25 passage that con-
tains the answer (Dist. Sup.).

answers as positives (i.e., the distant supervision
setting), whether there is a significant performance
degradation. Using the question and answer to- Sim Loss Retrieval Accuracy
gether as the query, we run Lucene-BM25 and pick Top-1 Top-5 Top-20 Top-100
the top passage that contains the answer as the pos- NLL 44.9 66.8 78.1 85.0
itive passage. Table 5 shows the performance of DP

Triplet 41.6 65.0 77.2 84.5
DPR when trained using the original setting and NLL 43.5 64.7 76.1 83.1
the distant supervision setting. L2

Triplet 42.2 66.0 78.1 84.9

B Alternative Similarity Functions & Table 6: Retrieval Top-k accuracy on the development
Triplet Loss set of Natural Questions using different similarity and

loss functions.
In addition to dot product (DP) and negative log-
likelihood based on softmax (NLL), we also exper-
iment with Euclidean distance (L2) and the triplet the correct answer, presumably by matching “body
loss. We negate L2 similarity scores before ap- of water” with semantic neighbors such as sea and
plying softmax and change signs of question-to- channel, even though no lexical overlap exists. The
positive and question-to-negative similarities when second example is one where BM25 does better.
applying the triplet loss on dot product scores. The The salient phrase “Thoros of Myr” is critical, and
margin value of the triplet loss is set to 1. Ta- DPR is unable to capture it.
ble 6 summarizes the results. All these additional
experiments are conducted using the same hyper- D Joint Training of Retriever and
parameters tuned for the baseline (DP, NLL). Reader

Note that the retrieval accuracy for our “baseline”
settings reported in Table 5 (Gold) and Table 6 We fix the passage encoder in our joint-training
(DP, NLL) is slightly better than those reported in scheme while allowing only the question encoder
Table 3. This is due to a better hyper-parameter to receive backpropagation signal from the com-
setting used in these analysis experiments, which bined (retriever + reader) loss function. This allows
is documented in our code release. us to leverage the HNSW-based FAISS index for

efficient low-latency retrieving, without reindexing
C Qualitative Analysis the passages during model updates. Our loss func-

tion largely follows ORQA’s approach, which uses
Although DPR performs better than BM25 in gen- log probabilities of positive passages selected from
eral, the retrieved passages of these two retrievers the retriever model, and correct spans and passages
actually differ qualitatively. Methods like BM25 selected from the reader model. Since the passage
are sensitive to highly selective keywords and encoder is fixed, we could use larger amount of
phrases, but cannot capture lexical variations or se- retrieved passages when calculating the retriever
mantic relationships well. In contrast, DPR excels loss. Specifically, we get top 100 passages for each
at semantic representation, but might lack sufficient question in a mini-batch and use the method similar
capacity to represent salient phrases which appear to in-batch negative training: all retrieved passages’
rarely. Table 7 illustrates this phenomenon with vectors participate in the loss calculation for all
two examples. In the first example, the top scor- questions in a batch. Our training batch size is set
ing passage from BM25 is irrelevant, even though to 16, which effectively gives 1,600 passages per
keywords such as England and Ireland appear mul- question to calculate retriever loss. The reader still
tiple times. In comparison, DPR is able to return uses 24 passages per question, which are selected



Question Passage received by BM25 Passage retrieved by DPR
What is the body of water Title:British Cycling Title: Irish Sea
between England and Ireland? . . . England is not recognised as a region by the UCI, and . . . Annual traffic between Great Britain and Ireland amounts

there is no English cycling team outside the Commonwealth to over 12 million passengers and of traded goods. The Irish
Games. For those occasions, British Cycling selects and sup- Sea is connected to the North Atlantic at both its northern
ports the England team. Cycling is represented on the Isle and southern ends. To the north, the connection is through
of Man by the Isle of Man Cycling Association. Cycling in the North Channel between Scotland and Northern Ireland
Northern Ireland is organised under Cycling Ulster, part of and the Malin Sea. The southern end is linked to the Atlantic
the all-Ireland governing body Cycling Ireland. Until 2006, through the St George’s Channel between Ireland and Pem-
a rival governing body existed, . . . brokeshire, and the Celtic Sea. . . .

Who plays Thoros of Myr in Title: No One (Game of Thrones) Title: Pål Sverre Hagen
Game of Thrones? . . . He may be ”no one,” but there’s still enough of a person Pål Sverre Valheim Hagen (born 6 November 1980) is a Nor-

left in him to respect, and admire who this girl is and what wegian stage and screen actor. He appeared in the Norwe-
she’s become. Arya finally tells us something that we’ve kind gian film ”Max Manus” and played Thor Heyerdahl in the
of known all along, that she’s not no one, she’s Arya Stark Oscar-nominated 2012 film ”Kon-Tiki”. Pl Hagen was born
of Winterfell.” ”No One” saw the reintroduction of Richard in Stavanger, Norway, the son of Roar Hagen, a Norwegian
Dormer and Paul Kaye, who portrayed Beric Dondarrion and cartoonist who has long been associated with Norwayś largest
Thoros of Myr, respectively, in the third season, . . . daily, ”VG”. He lived in Jtten, a neighborhood in the city of

Stavanger in south-western Norway. . . .

Table 7: Examples of passages returned from BM25 and DPR. Correct answers are written in blue and the content
words in the question are written in bold.

from the top 5 positive and top 30 negative passages
(from the set of top 100 passages retrieved from
the same question). The question encoder’s initial
state is taken from a DPR model previously trained
on the NQ dataset. The reader’s initial state is a
BERT-base model. In terms of the end-to-end QA
results, our joint-training scheme does not provide
better results compared to the usual retriever/reader
training pipeline, resulting in the same 39.8 exact
match score on NQ dev as in our regular reader
model training.