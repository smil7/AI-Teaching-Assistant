Reading Wikipedia to Answer Open-Domain Questions

Danqi Chen∗ Adam Fisch, Jason Weston & Antoine Bordes
Computer Science Facebook AI Research

Stanford University 770 Broadway
Stanford, CA 94305, USA New York, NY 10003, USA

danqi@cs.stanford.edu {afisch,jase,abordes}@fb.com

Abstract 2016), Wikipedia contains up-to-date knowledge
that humans are interested in. It is designed, how-

This paper proposes to tackle open- ever, for humans – not machines – to read.
domain question answering using Using Wikipedia articles as the knowledge
Wikipedia as the unique knowledge source causes the task of question answering (QA)
source: the answer to any factoid question to combine the challenges of both large-scale
is a text span in a Wikipedia article. open-domain QA and of machine comprehension
This task of machine reading at scale of text. In order to answer any question, one must
combines the challenges of document re- first retrieve the few relevant articles among more
trieval (finding the relevant articles) with than 5 million items, and then scan them care-
that of machine comprehension of text fully to identify the answer. We term this setting,
(identifying the answer spans from those machine reading at scale (MRS). Our work treats
articles). Our approach combines a search Wikipedia as a collection of articles and does not
component based on bigram hashing rely on its internal graph structure. As a result, our
and TF-IDF matching with a multi-layer approach is generic and could be switched to other
recurrent neural network model trained to collections of documents, books, or even daily up-
detect answers in Wikipedia paragraphs. dated newspapers.
Our experiments on multiple existing QA Large-scale QA systems like IBM’s DeepQA
datasets indicate that (1) both modules (Ferrucci et al., 2010) rely on multiple sources
are highly competitive with respect to to answer: besides Wikipedia, it is also paired
existing counterparts and (2) multitask with KBs, dictionaries, and even news articles,
learning using distant supervision on books, etc. As a result, such systems heavily rely
their combination is an effective complete on information redundancy among the sources to
system on this challenging task. answer correctly. Having a single knowledge

1 Introduction source forces the model to be very precise while
searching for an answer as the evidence might

This paper considers the problem of answering appear only once. This challenge thus encour-
factoid questions in an open-domain setting us- ages research in the ability of a machine to read,
ing Wikipedia as the unique knowledge source, a key motivation for the machine comprehen-
such as one does when looking for answers in an sion subfield and the creation of datasets such
encyclopedia. Wikipedia is a constantly evolv- as SQuAD (Rajpurkar et al., 2016), CNN/Daily
ing source of detailed information that could fa- Mail (Hermann et al., 2015) and CBT (Hill et al.,
cilitate intelligent machines — if they are able to 2016).
leverage its power. Unlike knowledge bases (KBs) However, those machine comprehension re-
such as Freebase (Bollacker et al., 2008) or DB- sources typically assume that a short piece of rel-
Pedia (Auer et al., 2007), which are easier for evant text is already identified and given to the
computers to process but too sparsely populated model, which is not realistic for building an open-
for open-domain question answering (Miller et al., domain QA system. In sharp contrast, methods

∗Most of this work was done while DC was with Face- that use KBs or information retrieval over docu-
book AI Research. ments have to employ search as an integral part of

arXiv:1704.00051v2  [cs.CL]  28 Apr 2017



the solution. Instead MRS is focused on simul- augmented neural networks (Bahdanau et al.,
taneously maintaining the challenge of machine 2015; Weston et al., 2015; Graves et al., 2014) and
comprehension, which requires the deep under- release of new training and evaluation datasets like
standing of text, while keeping the realistic con- QuizBowl (Iyyer et al., 2014), CNN/Daily Mail
straint of searching over a large open resource. based on news articles (Hermann et al., 2015),

In this paper, we show how multiple existing CBT based on children books (Hill et al., 2016), or
QA datasets can be used to evaluate MRS by re- SQuAD (Rajpurkar et al., 2016) and WikiReading
quiring an open-domain system to perform well on (Hewlett et al., 2016), both based on Wikipedia.
all of them at once. We develop DrQA, a strong An objective of this paper is to test how such
system for question answering from Wikipedia new methods can perform in an open-domain QA
composed of: (1) Document Retriever, a mod- framework.
ule using bigram hashing and TF-IDF matching QA using Wikipedia as a resource has been ex-
designed to, given a question, efficiently return plored previously. Ryu et al. (2014) perform open-
a subset of relevant articles and (2) Document domain QA using a Wikipedia-based knowledge
Reader, a multi-layer recurrent neural network model. They combine article content with multi-
machine comprehension model trained to detect ple other answer matching modules based on dif-
answer spans in those few returned documents. ferent types of semi-structured knowledge such
Figure 1 gives an illustration of DrQA. as infoboxes, article structure, category structure,

Our experiments show that Document Retriever and definitions. Similarly, Ahn et al. (2004) also
outperforms the built-in Wikipedia search engine combine Wikipedia as a text resource with other
and that Document Reader reaches state-of-the- resources, in this case with information retrieval
art results on the very competitive SQuAD bench- over other documents. Buscaldi and Rosso (2006)
mark (Rajpurkar et al., 2016). Finally, our full sys- also mine knowledge from Wikipedia for QA. In-
tem is evaluated using multiple benchmarks. In stead of using it as a resource for seeking answers
particular, we show that performance is improved to questions, they focus on validating answers re-
across all datasets through the use of multitask turned by their QA system, and use Wikipedia
learning and distant supervision compared to sin- categories for determining a set of patterns that
gle task training. should fit with the expected answer. In our work,
2 Related Work we consider the comprehension of text only, and

use Wikipedia text documents as the sole resource
Open-domain QA was originally defined as find- in order to emphasize the task of machine reading
ing answers in collections of unstructured docu- at scale, as described in the introduction.
ments, following the setting of the annual TREC There are a number of highly developed full
competitions1. With the development of KBs, pipeline QA approaches using either the Web, as
many recent innovations have occurred in the con- does QuASE (Sun et al., 2015), or Wikipedia as a
text of QA from KBs with the creation of re- resource, as do Microsoft’s AskMSR (Brill et al.,
sources like WebQuestions (Berant et al., 2013) 2002), IBM’s DeepQA (Ferrucci et al., 2010) and
and SimpleQuestions (Bordes et al., 2015) based YodaQA (Baudiš, 2015; Baudiš and Šedivỳ, 2015)
on the Freebase KB (Bollacker et al., 2008), or on — the latter of which is open source and hence
automatically extracted KBs, e.g., OpenIE triples reproducible for comparison purposes. AskMSR
and NELL (Fader et al., 2014). However, KBs is a search-engine based QA system that relies
have inherent limitations (incompleteness, fixed on “data redundancy rather than sophisticated lin-
schemas) that motivated researchers to return to guistic analyses of either questions or candidate
the original setting of answering from raw text. answers”, i.e., it does not focus on machine com-

A second motivation to cast a fresh look at prehension, as we do. DeepQA is a very sophisti-
this problem is that of machine comprehension of cated system that relies on both unstructured infor-
text, i.e., answering questions after reading a short mation including text documents as well as struc-
text or story. That subfield has made consider- tured data such as KBs, databases and ontologies
able progress recently thanks to new deep learning to generate candidate answers or vote over evi-
architectures like attention-based and memory- dence. YodaQA is an open source system mod-

1http://trec.nist.gov/data/qamain.html eled after DeepQA, similarly combining websites,



Open-domain QA  
SQuAD, TREC, WebQuestions, WikiMovies

Q:  How many of Warsaw's inhabitants  
spoke Polish in 1933?

Document Document
Retriever Reader

833,500

Figure 1: An overview of our question answering system DrQA.

information extraction, databases and Wikipedia rather than using a KB, with positive results.
in particular. Our comprehension task is made
more challenging by only using a single resource. 3 Our System: DrQA
Comparing against these methods provides a use-
ful datapoint for an “upper bound” benchmark on In the following we describe our system DrQA for
performance. MRS which consists of two components: (1) the

Document Retriever module for finding relevant
Multitask learning (Caruana, 1998) and task articles and (2) a machine comprehension model,

transfer have a rich history in machine learning Document Reader, for extracting answers from a
(e.g., using ImageNet in the computer vision com- single document or a small collection of docu-
munity (Huh et al., 2016)), as well as in NLP ments.
in particular (Collobert and Weston, 2008). Sev-
eral works have attempted to combine multiple 3.1 Document Retriever
QA training datasets via multitask learning to (i) Following classical QA systems, we use an effi-
achieve improvement across the datasets via task cient (non-machine learning) document retrieval
transfer; and (ii) provide a single general system system to first narrow our search space and focus
capable of asking different kinds of questions due on reading only articles that are likely to be rel-
to the inevitably different data distributions across evant. A simple inverted index lookup followed
the source datasets. Fader et al. (2014) used We- by term vector model scoring performs quite well
bQuestions, TREC and WikiAnswers with four on this task for many question types, compared to
KBs as knowledge sources and reported improve- the built-in ElasticSearch based Wikipedia Search
ment on the latter two datasets through multi- API (Gormley and Tong, 2015). Articles and ques-
task learning. Bordes et al. (2015) combined We- tions are compared as TF-IDF weighted bag-of-
bQuestions and SimpleQuestions using distant su- word vectors. We further improve our system by
pervision with Freebase as the KB to give slight taking local word order into account with n-gram
improvements on both datasets, although poor per- features. Our best performing system uses bigram
formance was reported when training on only one counts while preserving speed and memory effi-
dataset and testing on the other, showing that task ciency by using the hashing of (Weinberger et al.,
transfer is indeed a challenging subject; see also 2009) to map the bigrams to 224 bins with an un-
(Kadlec et al., 2016) for a similar conclusion. Our signed murmur3 hash.
work follows similar themes, but in the setting of We use Document Retriever as the first part of
having to retrieve and then read text documents, our full model, by setting it to return 5 Wikipedia



articles given any question. Those articles are then We also add a few manual features which re-
processed by Document Reader. flect some properties of token pi in its con-

text, which include its part-of-speech (POS)
3.2 Document Reader and named entity recognition (NER) tags and
Our Document Reader model is inspired by the re- its (normalized) term frequency (TF).
cent success of neural network models on machine
comprehension tasks, in a similar spirit to the At- • Aligned question embedding:
tentiveReader described in (Hermann et al., 2015; Following (Lee et al., 2016) and other re-
Chen et al., 2016). c∑ent works, the last part we incorporate is

Given a question q consisting of l tokens an aligned question embedding falign(pi) =

{q1, . . . , ql} and a document or a small set of doc- j ai,jE(qj), where the attention score ai,j
uments of n paragraphs where a single paragraph captures the similarity between pi and each
p consists of m tokens {p1, . . . , pm}, we develop question words qj . Specifically, ai,j is com-
an RNN model that we apply to each paragraph in puted by the dot products between nonlinear
turn and then finally aggregate the predicted an- mappings of word embeddings:
swers. Our method works as follows:

ai,j = ∑exp (α((E(pi)) · α(E(qj))) ) ,
Paragraph encoding We first represent all to- j′ exp α(E(pi)) · α(E(qj′))

kens pi in a paragraph p as a sequence of feature
vectors and α(·) is a single dense layer with ReLU

p̃i ∈ Rd and pass them as the input to a
recurrent neural network and thus obtain: nonlinearity. Compared to the exact match

features, these features add soft alignments
{p1, . . . ,pm} = RNN({p̃1, . . . , p̃m}), between similar but non-identical words

(e.g., car and vehicle).
where pi is expected to encode useful context Question encoding The question encoding is
information around token pi. Specifically, we simpler, as we only apply another recurrent neu-
choose to use a multi-layer bidirectional long ral network on top of the word embeddings of q
short-term memory network (LSTM), and take i

pi and c∑ombine the resulting hidden units into one
as the concatenation of each layer’s hidden units single vector: {q1, . . . ,ql} → q. We compute
in the end.

q = j bjqj where bj encodes the importance of
The feature vector p̃i is comprised of the fol- each question word:

lowing parts:

bj = ∑exp(w · qj)
• Word embeddings: femb(pi) = E(pi). We ,

j′ exp(w · qj′)use the 300-dimensional Glove word em-
beddings trained from 840B Web crawl data and w is a weight vector to learn.
(Pennington et al., 2014). We keep most of
the pre-trained word embeddings fixed and Prediction At the paragraph level, the goal is to
only fine-tune the 1000 most frequent ques- predict the span of tokens that is most likely the
tion words because the representations of correct answer. We take the the paragraph vectors
some key words such as what, how, which, {p1, . . . ,pm} and the question vector q as input,
many could be crucial for QA systems. and simply train two classifiers independently for

predicting the two ends of the span. Concretely,
• Exact match: fexact match(pi) = I(pi ∈ q). we use a bilinear term to capture the similarity be-

We use three simple binary features, indicat- tween pi and q and compute the probabilities of
ing whether pi can be exactly matched to one each token being start and end as:
question word in q, either in its original, low-
ercase or lemma form. These simple features Pstart(i) ∝ exp (piWsq)

turn out to be extremely helpful, as we will Pend(i) ∝ exp (piWeq)
show in Section 5.

During prediction, we choose the best span from
• Token features: token i to token i′ such that i ≤ i′ ≤ i + 15 and
ftoken(pi) = (POS(pi),NER(pi),TF(pi)). Pstart(i)×Pend(i

′) is maximized. To make scores



compatible across paragraphs in one or several re- evant paragraph as defined in (Rajpurkar et al.,
trieved documents, we use the unnormalized expo- 2016). For the task of evaluating open-domain
nential and take argmax over all considered para- question answering over Wikipedia, we use the
graph spans for our final prediction. SQuAD development set QA pairs only, and we

ask systems to uncover the correct answer spans
4 Data without having access to the associated para-
Our work relies on three types of data: (1) graphs. That is, a model is required to answer
Wikipedia that serves as our knowledge source for a question given the whole of Wikipedia as a re-
finding answers, (2) the SQuAD dataset which is source; it is not given the relevant paragraph as in
our main resource to train Document Reader and the standard SQuAD setting.
(3) three more QA datasets (CuratedTREC, We-
bQuestions and WikiMovies) that in addition to 4.3 Open-domain QA Evaluation Resources
SQuAD, are used to test the open-domain QA abil-
ities of our full system, and to evaluate the ability SQuAD is one of the largest general purpose QA

of our model to learn from multitask learning and datasets currently available. SQuAD questions

distant supervision. Statistics of the datasets are have been collected via a process involving show-

given in Table 2. ing a paragraph to each human annotator and ask-
ing them to write a question. As a result, their

4.1 Wikipedia (Knowledge Source) distribution is quite specific. We hence propose to
We use the 2016-12-21 dump2 of English train and evaluate our system on other datasets de-
Wikipedia for all of our full-scale experiments as veloped for open-domain QA that have been con-
the knowledge source used to answer questions. structed in different ways (not necessarily in the
For each page, only the plain text is extracted and context of answering from Wikipedia).
all structured data sections such as lists and fig-
ures are stripped.3 After discarding internal dis- CuratedTREC This dataset is based on the
ambiguation, list, index, and outline pages, we benchmarks from the TREC QA tasks that have
retain 5,075,182 articles consisting of 9,008,962 been curated by Baudiš and Šedivỳ (2015). We use
unique uncased token types. the large version, which contains a total of 2,180

questions extracted from the datasets from TREC
4.2 SQuAD 1999, 2000, 2001 and 2002.4

The Stanford Question Answering Dataset
(SQuAD) (Rajpurkar et al., 2016) is a dataset WebQuestions Introduced in (Berant et al.,
for machine comprehension based on Wikipedia. 2013), this dataset is built to answer questions
The dataset contains 87k examples for training from the Freebase KB. It was created by crawling
and 10k for development, with a large hidden questions through the Google Suggest API, and
test set which can only be accessed by the then obtaining answers using Amazon Mechani-
SQuAD creators. Each example is composed of cal Turk. We convert each answer to text by us-
a paragraph extracted from a Wikipedia article ing entity names so that the dataset does not refer-
and an associated human-generated question. The ence Freebase IDs and is purely made of plain text
answer is always a span from this paragraph and question-answer pairs.
a model is given credit if its predicted answer
matches it. Two evaluation metrics are used: exact WikiMovies This dataset, introduced in (Miller
string match (EM) and F1 score, which measures et al., 2016), contains 96k question-answer pairs in
the weighted average of precision and recall at the the domain of movies. Originally created from the
token level. OMDb and MovieLens databases, the examples

In the following, we use SQuAD for training are built such that they can also be answered by us-
and evaluating our Document Reader for the stan- ing a subset of Wikipedia as the knowledge source
dard machine comprehension task given the rel- (the title and the first section of articles from the

2 movie domain).
https://dumps.wikimedia.org/enwiki/

latest
3We use the WikiExtractor script: https://github. 4This dataset is available at https://github.com/

com/attardi/wikiextractor. brmson/dataset-factoid-curated.



Dataset Example Article / Paragraph
SQuAD Q: How many provinces did the Ottoman Article: Ottoman Empire

empire contain in the 17th century? Paragraph: ... At the beginning of the 17th century the em-
A: 32 pire contained 32 provinces and numerous vassal states. Some

of these were later absorbed into the Ottoman Empire, while
others were granted various types of autonomy during the
course of centuries.

CuratedTREC Q: What U.S. state’s motto is “Live free Article: Live Free or Die
or Die”? Paragraph: ”Live Free or Die” is the official motto of the
A: New Hampshire U.S. state of New Hampshire, adopted by the state in 1945. It

is possibly the best-known of all state mottos, partly because it
conveys an assertive independence historically found in Amer-
ican political philosophy and partly because of its contrast to
the milder sentiments found in other state mottos.

WebQuestions Q: What part of the atom did Chadwick Article: Atom
discover?† Paragraph: ... The atomic mass of these isotopes varied by
A: neutron integer amounts, called the whole number rule. The explana-

tion for these different isotopes awaited the discovery of the
neutron, an uncharged particle with a mass similar to the pro-
ton, by the physicist James Chadwick in 1932. ...

WikiMovies Q: Who wrote the film Gigli? Article: Gigli
A: Martin Brest Paragraph: Gigli is a 2003 American romantic comedy film

written and directed by Martin Brest and starring Ben Affleck,
Jennifer Lopez, Justin Bartha, Al Pacino, Christopher Walken,
and Lainie Kazan.

Table 1: Example training data from each QA dataset. In each case we show an associated paragraph
where distant supervision (DS) correctly identified the answer within it, which is highlighted.

Dataset Train Test Dataset Wiki Doc. Retriever
Plain DS Search plain +bigrams

SQuAD 87,599 71,231 10,570† SQuAD 62.7 76.1 77.8
CuratedTREC 1,486∗ 3,464 694 CuratedTREC 81.0 85.2 86.0
WebQuestions 3,778∗ 4,602 2,032 WebQuestions 73.7 75.5 74.4
WikiMovies 96,185∗ 36,301 9,952 WikiMovies 61.7 54.4 70.3

Table 2: Number of questions for each dataset Table 3: Document retrieval results. % of ques-
used in this paper. DS: distantly supervised train- tions for which the answer segment appears in one
ing data. ∗: These training sets are not used as of the top 5 pages returned by the method.
is because no paragraph is associated with each
question. †: Corresponds to SQuAD development
set. run Document Retriever on the question to re-

trieve the top 5 Wikipedia articles. All paragraphs

4.4 Distantly Supervised Data from those articles without an exact match of the
known answer are directly discarded. All para-

All the QA datasets presented above contain train- graphs shorter than 25 or longer than 1500 charac-
ing portions, but CuratedTREC, WebQuestions ters are also filtered out. If any named entities are
and WikiMovies only contain question-answer detected in the question, we remove any paragraph
pairs, and not an associated document or para- that does not contain them at all. For every remain-
graph as in SQuAD, and hence cannot be used ing paragraph in each retrieved page, we score all
for training Document Reader directly. Follow- positions that match an answer using unigram and
ing previous work on distant supervision (DS) for bigram overlap between the question and a 20 to-
relation extraction (Mintz et al., 2009), we use a ken window, keeping up to the top 5 paragraphs
procedure to automatically associate paragraphs to with the highest overlaps. If there is no paragraph
such training examples, and then add these exam- with non-zero overlap, the example is discarded;
ples to our training set. otherwise we add each found pair to our DS train-

We use the following process for each question- ing dataset. Some examples are shown in Table 1
answer pair to build our training set. First, we and data statistics are given in Table 2.



Note that we can also generate additional DS 2014). Dropout with p = 0.3 is applied to word
data for SQuAD by trying to find mentions of the embeddings and all the hidden units of LSTMs.
answers not just in the paragraph provided, but
also from other pages or the same page that the Result and analysis Table 4 presents our eval-
given paragraph was in. We observe that around uation results on both development and test sets.
half of the DS examples come from pages outside SQuAD has been a very competitive machine
of the articles used in SQuAD. comprehension benchmark since its creation and

we only list the best-performing systems in the ta-
5 Experiments ble. Our system (single model) can achieve 70.0%

exact match and 79.0% F1 scores on the test set,
This section first presents evaluations of our Doc- which surpasses all the published results and can
ument Retriever and Document Reader modules match the top performance on the SQuAD leader-
separately, and then describes tests of their com- board at the time of writing. Additionally, we
bination, DrQA, for open-domain QA on the full think that our model is conceptually simpler than
Wikipedia. most of the existing systems. We conducted an

ablation analysis on the feature vector of para-
5.1 Finding Relevant Articles graph tokens. As shown in Table 5 all the features
We first examine the performance of our Docu- contribute to the performance of our final system.
ment Retriever module on all the QA datasets. Ta- Without the aligned question embedding feature
ble 3 compares the performance of the two ap- (only word embedding and a few manual features),
proaches described in Section 3.1 with that of the our system is still able to achieve F1 over 77%.
Wikipedia Search Engine5 for the task of find- More interestingly, if we remove both faligned and
ing articles that contain the answer given a ques- fexact match, the performance drops dramatically,
tion. Specifically, we compute the ratio of ques- so we conclude that both features play a similar
tions for which the text span of any of their as- but complementary role in the feature representa-
sociated answers appear in at least one the top 5 tion related to the paraphrased nature of a question
relevant pages returned by each system. Results vs. the context around an answer.
on all datasets indicate that our simple approach
outperforms Wikipedia Search, especially with bi- 5.3 Full Wikipedia Question Answering
gram hashing. We also compare doing retrieval Finally, we assess the performance of our full sys-
with Okapi BM25 or by using cosine distance in tem DrQA for answering open-domain questions
the word embeddings space (by encoding ques- using the four datasets introduced in Section 4.
tions and articles as bag-of-embeddings), both of We compare three versions of DrQA which eval-
which we find performed worse. uate the impact of using distant supervision and

multitask learning across the training sources pro-
5.2 Reader Evaluation on SQuAD vided to Document Reader (Document Retriever
Next we evaluate our Document Reader com- remains the same for each case):
ponent on the standard SQuAD evaluation (Ra- • SQuAD: A single Document Reader model is
jpurkar et al., 2016). trained on the SQuAD training set only and
Implementation details We use 3-layer bidirec- used on all evaluation sets.
tional LSTMs with h = 128 hidden units for both • Fine-tune (DS): A Document Reader model
paragraph and question encoding. We apply the is pre-trained on SQuAD and then fine-tuned
Stanford CoreNLP toolkit (Manning et al., 2014) for each dataset independently using its dis-
for tokenization and also generating lemma, part- tant supervision (DS) training set.
of-speech, and named entity tags.

Lastly, all the training examples are sorted by • Multitask (DS): A single Document Reader
the length of paragraph and divided into mini- model is jointly trained on the SQuAD train-
batches of 32 examples each. We use Adamax ing set and all the DS sources.
for optimization as described in (Kingma and Ba, For the full Wikipedia setting we use a stream-

5We use the Wikipedia Search API https://www. lined model that does not use the CoreNLP parsed
mediawiki.org/wiki/API:Search. ftoken features or lemmas for fexact match. We



Method Dev Test
EM F1 EM F1

Dynamic Coattention Networks (Xiong et al., 2016) 65.4 75.6 66.2 75.9
Multi-Perspective Matching (Wang et al., 2016)† 66.1 75.8 65.5 75.1
BiDAF (Seo et al., 2016) 67.7 77.3 68.0 77.3
R-net† n/a n/a 71.3 79.7
DrQA (Our model, Document Reader Only) 69.5 78.8 70.0 79.0

Table 4: Evaluation results on the SQuAD dataset (single model only). †: Test results reflect the SQuAD
leaderboard (https://stanford-qa.com) as of Feb 6, 2017.

Features F1 We compare to an unconstrained QA system us-
Full 78.8 ing redundant resources (not just Wikipedia), Yo-
No ftoken 78.0 (-0.8) daQA (Baudiš, 2015), giving results which were
No fexact match 77.3 (-1.5) previously reported on CuratedTREC and We-
No faligned 77.3 (-1.5) bQuestions. Despite the increased difficulty of our
No faligned and fexact match 59.4 (-19.4) task, it is reassuring that our performance is not

too far behind on CuratedTREC (31.3 vs. 25.4).
Table 5: Feature ablation analysis of the paragraph The gap is slightly bigger on WebQuestions, likely
representations of our Document Reader. Results because this dataset was created from the specific
are reported on the SQuAD development set. structure of Freebase which YodaQA uses directly.

DrQA’s performance on SQuAD compared to
its Document Reader component on machine com-

find that while these help for more exact paragraph prehension in Table 4 shows a large drop (from
reading in SQuAD, they don’t improve results in 69.5 to 27.1) as we now are given Wikipedia to
the full setting. Additionally, WebQuestions and read, not a single paragraph. Given the correct
WikiMovies provide a list of candidate answers document (but not the paragraph) we can achieve
(e.g., 1.6 million Freebase entity strings for We- 49.4, indicating many false positives come from
bQuestions) and we restrict the answer span must highly topical sentences. This is despite the fact
be in this list during prediction. that the Document Retriever works relatively well
Results Table 6 presents the results. Despite the (77.8% of the time retrieving the answer, see Ta-
difficulty of the task compared to machine com- ble 3). It is worth noting that a large part of the
prehension (where you are given the right para- drop comes from the nature of the SQuAD ques-
graph) and unconstrained QA (using redundant re- tions. They were written with a specific para-
sources), DrQA still provides reasonable perfor- graph in mind, thus their language can be ambigu-
mance across all four datasets. ous when the context is removed. Additional re-

We are interested in a single, full system that sources other than SQuAD, specifically designed
can answer any question using Wikipedia. The for MRS, might be needed to go further.
single model trained only on SQuAD is outper- 6 Conclusion
formed on all four of the datasets by the multitask
model that uses distant supervision. However per- We studied the task of machine reading at scale, by
formance when training on SQuAD alone is not far using Wikipedia as the unique knowledge source
behind, indicating that task transfer is occurring. for open-domain QA. Our results indicate that
The majority of the improvement from SQuAD MRS is a key challenging task for researchers
to Multitask (DS) however is likely not from task to focus on. Machine comprehension systems
transfer as fine-tuning on each dataset alone using alone cannot solve the overall task. Our method
DS also gives improvements, showing that is is the integrates search, distant supervision, and mul-
introduction of extra data in the same domain that titask learning to provide an effective complete
helps. Nevertheless, the best single model that we system. Evaluating the individual components as
can find is our overall goal, and that is the Multi- well as the full system across multiple benchmarks
task (DS) system. showed the efficacy of our approach.



Dataset YodaQA DrQA
SQuAD +Fine-tune (DS) +Multitask (DS)

SQuAD (All Wikipedia) n/a 27.1 28.4 29.8
CuratedTREC 31.3 19.7 25.7 25.4
WebQuestions 39.8 11.8 19.5 20.7
WikiMovies n/a 24.5 34.3 36.5

Table 6: Full Wikipedia results. Top-1 exact-match accuracy (in %, using SQuAD eval script). +Fine-
tune (DS): Document Reader models trained on SQuAD and fine-tuned on each DS training set inde-
pendently. +Multitask (DS): Document Reader single model trained on SQuAD and all the distant su-
pervision (DS) training sets jointly. YodaQA results are extracted from https://github.com/brmson/
yodaqa/wiki/Benchmarks and use additional resources such as Freebase and DBpedia, see Section 2.

Future work should aim to improve over our from question-answer pairs. In Empirical Methods
DrQA system. Two obvious angles of attack are: in Natural Language Processing (EMNLP). pages
(i) incorporate the fact that Document Reader ag- 1533–1544.

gregates over multiple paragraphs and documents Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
directly in the training, as it currently trains on Sturge, and Jamie Taylor. 2008. Freebase: a collab-
paragraphs independently; and (ii) perform end- oratively created graph database for structuring hu-

man knowledge. In Proceedings of the 2008 ACM
to-end training across the Document Retriever and SIGMOD international conference on Management
Document Reader pipeline, rather than indepen- of data. AcM, pages 1247–1250.
dent systems. Antoine Bordes, Nicolas Usunier, Sumit Chopra, and
Acknowledgments Jason Weston. 2015. Large-scale simple question

answering with memory networks. arXiv preprint
The authors thank Pranav Rajpurkar for testing arXiv:1506.02075 .
Document Reader on the test set of SQuAD. Eric Brill, Susan Dumais, and Michele Banko. 2002.

An analysis of the AskMSR question-answering sys-
tem. In Empirical Methods in Natural Language

References Processing (EMNLP). pages 257–264.

David Ahn, Valentin Jijkoun, Gilad Mishne, Karin Davide Buscaldi and Paolo Rosso. 2006. Mining
Mller, Maarten de Rijke, and Stefan Schlobach. knowledge from Wikipedia for the question answer-
2004. Using wikipedia at the trec qa track. In Pro- ing task. In International Conference on Language
ceedings of TREC 2004. Resources and Evaluation (LREC). pages 727–730.

Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Rich Caruana. 1998. Multitask learning. In Learning
Lehmann, Richard Cyganiak, and Zachary Ives. to learn, Springer, pages 95–133.
2007. Dbpedia: A nucleus for a web of open data.
In The semantic web, Springer, pages 722–735. Danqi Chen, Jason Bolton, and Christopher D Man-

ning. 2016. A thorough examination of the
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- CNN/Daily Mail reading comprehension task. In

gio. 2015. Neural machine translation by jointly Association for Computational Linguistics (ACL).
learning to align and translate. In International Con- Ronan Collobert and Jason Weston. 2008. A unified
ference on Learning Representations (ICLR). architecture for natural language processing: deep

neural networks with multitask learning. In Interna-
Petr Baudiš. 2015. YodaQA: a modular question an- tional Conference on Machine Learning (ICML).

swering system pipeline. In POSTER 2015-19th In-
ternational Student Conference on Electrical Engi- Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
neering. pages 1156–1165. 2014. Open question answering over curated and

extracted knowledge bases. In ACM SIGKDD in-
Petr Baudiš and Jan Šedivỳ. 2015. Modeling of ternational conference on Knowledge discovery and

the question answering task in the YodaQA sys- data mining. pages 1156–1165.
tem. In International Conference of the Cross-
Language Evaluation Forum for European Lan- David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
guages. Springer, pages 222–228. James Fan, David Gondek, Aditya A Kalyanpur,

Adam Lally, J William Murdock, Eric Nyberg, John
Jonathan Berant, Andrew Chou, Roy Frostig, and Prager, et al. 2010. Building Watson: An overview

Percy Liang. 2013. Semantic parsing on freebase of the DeepQA project. AI magazine 31(3):59–79.



Clinton Gormley and Zachary Tong. 2015. Elastic- Mike Mintz, Steven Bills, Rion Snow, and Daniel
search: The Definitive Guide. ” O’Reilly Media, Jurafsky. 2009. Distant supervision for relation
Inc.”. extraction without labeled data. In Association

for Computational Linguistics and International
Alex Graves, Greg Wayne, and Ivo Danihelka. Joint Conference on Natural Language Processing

2014. Neural turing machines. arXiv preprint (ACL/IJCNLP). pages 1003–1011.
arXiv:1410.5401 .

Jeffrey Pennington, Richard Socher, and Christopher
Karl Moritz Hermann, Tomáš Kočiský, Edward Manning. 2014. Glove: Global vectors for word

Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- representation. In Empirical Methods in Natural
leyman, and Phil Blunsom. 2015. Teaching ma- Language Processing (EMNLP). pages 1532–1543.
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems (NIPS). Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and

Percy Liang. 2016. SQuAD: 100,000+ questions for
Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia machine comprehension of text. In Empirical Meth-

Polosukhin, Andrew Fandrianto, Jay Han, Matthew ods in Natural Language Processing (EMNLP).
Kelcey, and David Berthelot. 2016. Wikireading: A
novel large-scale language understanding task over Pum-Mo Ryu, Myung-Gil Jang, and Hyun-Ki Kim.
wikipedia. In Association for Computational Lin- 2014. Open domain question answering using
guistics (ACL). pages 1535–1545. Wikipedia-based knowledge model. Information

Processing & Management 50(5):683–692.
Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and

Weston. 2016. The Goldilocks Principle: Reading Hannaneh Hajishirzi. 2016. Bidirectional attention
children’s books with explicit memory representa- flow for machine comprehension. arXiv preprint
tions. In International Conference on Learning Rep- arXiv:1611.01603 .
resentations (ICLR).

Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai,
Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. Jingjing Liu, and Ming-Wei Chang. 2015. Open do-

2016. What makes ImageNet good for transfer main question answering via semantic enrichment.
learning? arXiv preprint arXiv:1608.08614 . In Proceedings of the 24th International Conference

on World Wide Web. ACM, pages 1045–1055.
Mohit Iyyer, Jordan L Boyd-Graber, Leonardo

Max Batista Claudino, Richard Socher, and Hal Zhiguo Wang, Haitao Mi, Wael Hamza, and Radu
Daumé III. 2014. A neural network for factoid ques- Florian. 2016. Multi-perspective context match-
tion answering over paragraphs. In Empirical Meth- ing for machine comprehension. arXiv preprint
ods in Natural Language Processing (EMNLP). arXiv:1612.04211 .
pages 633–644.

Kilian Weinberger, Anirban Dasgupta, John Langford,
Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Alex Smola, and Josh Attenberg. 2009. Feature

2016. From particular to general: A preliminary hashing for large scale multitask learning. In Inter-
case study of transfer learning in reading compre- national Conference on Machine Learning (ICML).
hension. Machine Intelligence Workshop, NIPS . pages 1113–1120.

Diederik Kingma and Jimmy Ba. 2014. Adam: A Jason Weston, Sumit Chopra, and Antoine Bordes.
method for stochastic optimization. arXiv preprint 2015. Memory networks. In International Confer-
arXiv:1412.6980 . ence on Learning Representations (ICLR).

Kenton Lee, Tom Kwiatkowski, Ankur Parikh, and Di- Caiming Xiong, Victor Zhong, and Richard Socher.
panjan Das. 2016. Learning recurrent span repre- 2016. Dynamic coattention networks for question
sentations for extractive question answering. answering. arXiv preprint arXiv:1611.01604 .

arXiv
preprint arXiv:1611.01436 .

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In Association for Com-
putational Linguistics (ACL). pages 55–60.

Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason We-
ston. 2016. Key-value memory networks for directly
reading documents. In Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1400–
1409.