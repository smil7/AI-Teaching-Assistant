Evaluation methods for unsupervised word embeddings

Tobias Schnabel Igor Labutov David Mimno,
Thorsten Joachims

Cornell University Cornell University Cornell University
Ithaca, NY, 14853 Ithaca, NY, 14853 mimno@cornell.edu,

tbs49@cornell.edu iil4@cornell.edu tj@cs.cornell.edu

Abstract evaluation only provides one way to specify the
goodness of an embedding, and it is not clear how

We present a comprehensive study of eval- it connects to other measures.
uation methods for unsupervised embed- Intrinsic evaluations directly test for syntactic or
ding techniques that obtain meaningful semantic relationships between words (Mikolov et
representations of words from text. Differ- al., 2013a; Baroni et al., 2014). These tasks typi-
ent evaluations result in different orderings cally involve a pre-selected set of query terms and
of embedding methods, calling into ques- semantically related target words, which we refer
tion the common assumption that there is to as a query inventory. Methods are evaluated
one single optimal vector representation. by compiling an aggregate score for each method
We present new evaluation techniques that such as a correlation coefficient, which then serves
directly compare embeddings with respect as an absolute measure of quality. Query inven-
to specific queries. These methods re- tories have so far been collected opportunistically
duce bias, provide greater insight, and from prior work in psycholinguistics, information
allow us to solicit data-driven relevance retrieval (Finkelstein et al., 2002), and image anal-
judgments rapidly and accurately through ysis (Bruni et al., 2014). Because these inventories
crowdsourcing. were not constructed for word embedding evalu-

ation, they are often idiosyncratic, dominated by
1 Introduction specific types of queries, and poorly calibrated to
Neural word embeddings represent meaning via corpus statistics.
geometry. A good embedding provides vector rep- To remedy these problems, this paper makes
resentations of words such that the relationship be- the following contributions. First, this is the first
tween two vectors mirrors the linguistic relation- paper to conduct a comprehensive study cover-
ship between the two words. Despite the growing ing a wide range of evaluation criteria and popu-
interest in vector representations of semantic in- lar embedding techniques. In particular, we study
formation, there has been relatively little work on how outcomes from three different evaluation cri-
direct evaluations of these models. In this work, teria are connected: word relatedness, coherence,
we explore several approaches to measuring the downstream performance. We show that using dif-
quality of neural word embeddings. In particu- ferent criteria results in different relative orderings
lar, we perform a comprehensive analysis of eval- of embeddings. These results indicate that embed-
uation methods and introduce novel methods that ding methods should be compared in the context
can be implemented through crowdsourcing, pro- of a specific task, e.g., linguistic insight or good
viding better insights into the relative strengths of downstream performance.
different embeddings. Second, we study the connections between di-

Existing schemes fall into two major categories: rect evaluation with real users and pre-collected
extrinsic and intrinsic evaluation. In extrinsic eval- offline data. We propose a new approach to evalu-
uation, we use word embeddings as input features ation that focuses on direct comparison of embed-
to a downstream task and measure changes in per- dings with respect to individual queries rather than
formance metrics specific to that task. Examples overall summary scores. Because we phrase all
include part-of-speech tagging and named-entity tasks as choice problems rather than ordinal rel-
recognition (Pennington et al., 2014). Extrinsic evance tasks, we can ease the burden of the an-

298
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 298–307,

Lisbon, Portugal, 17-21 September 2015. ©c 2015 Association for Computational Linguistics.



notators. We show that these evaluations can be ing evaluation methods and not embeddings them-
gathered efficiently from crowdsourcing. Our re- selves, no attempt has been made to optimize these
sults also indicate that there is in fact strong corre- embeddings. The first two embedding models,
lation between the results of automated similarity the CBOW model of word2vec (Mikolov et al.,
evaluation and direct human evaluation. This re- 2013a) and C&W embeddings (Collobert et al.,
sult justifies the use of offline data, at least for the 2011) both are motivated by a probabilistic predic-
similarity task. tion approach. Given a number of context words

Third, we propose a model- and data-driven ap- around a target word w, these models formulate
proach to constructing query inventories. Rather the embedding task as that of finding a representa-
than picking words in an ad hoc fashion, we se- tion that is good at predicting w from the context
lect query words to be diverse with respect to representations.
their frequency, parts-of-speech and abstractness. The second group of models, Hellinger PCA
To facilitate systematic evaluation and compar- (Lebret and Collobert, 2014), GloVe (Pennington
ison of new embedding models, we release a et al., 2014), TSCCA (Dhillon et al., 2012) and
new frequency-calibrated query inventory along Sparse Random Projections (Li et al., 2006) fol-
with all user judgments at http://www.cs. low a reconstruction approach: word embeddings
cornell.edu/˜schnabts/eval/. should be able to capture as much relevant infor-

Finally, we observe that word embeddings en- mation from the original co-occurrence matrix as
code a surprising degree of information about possible.
word frequency. We found this was true even in Training corpus. We tried to make the compar-
models that explicitly reserve parameters to com- ison as fair as possible. As the C&W embeddings
pensate for frequency effects. This finding may were only available pretrained on a November
explain some of the variability across embeddings 2007 snapshot of Wikipedia, we chose the closest
and across evaluation methods. It also casts doubt available Wikipedia dump (2008-03-01) for train-
on the common practice of using the vanilla co- ing the other models. We tokenized the data us-
sine similarity as a similarity measure in the em- ing the Stanford tokenizer (Manning et al., 2014).
bedding space. Like Collobert et al. (2011), we lower-cased all

It is important to note that this work is a survey words and replaced digits with zeros.
of evaluation methods not a survey of embedding Details. All models embedded words into a 50-
methods. The specific example embeddings pre- dimensional space (D = 50). As implemented,
sented here were chosen as representative samples each method uses a different vocabulary, so we
only, and may not be optimal. computed the intersection of the six vocabularies

2 Word embeddings and used the resulting set of 103,647 words for all
nearest-neighbor experiments.

We refer to a word embedding as a mapping V →
RD : w 7→ w~ that maps a word w from a vocabu- 3 Relatedness
lary V to a real-valued vector w~ in an embedding
space of dimensionality D. We begin with intrinsic evaluation of relatedness

Following previous work (Collobert et al., using both pre-collected human evaluations and a
2011; Mikolov et al., 2013a) we use the com- novel online user study. Section 3.1 introduces the
monly employed cosine similarity, defined as list of datasets that is commonly used as a bench-
similarity(w1, w2) = w~1·w~2

‖w~ o a l i i a
1‖‖w~ , f r l m l r

2‖ s - mark for embedding methods. There, embeddings
ity computations in the embedding space. The list are evaluated individually and only their final
of nearest neighbors of a word w are all words scores are compared, hence we refer to this sce-
v ∈ V \ {w}, sorted in descending order by nario as absolute intrinsic evaluation. We present
similarity(w, v). We will denote w as the query a new scenario, comparative intrinsic evaluation,
word in the remainder of this paper. in which we ask people directly for their prefer-

All experiments in this paper are carried out ences among different embeddings. We demon-
on six popular unsupervised embedding meth- strate that we can achieve the same results as of-
ods. These embeddings form a representative fline, absolute metrics using online, comparative
but incomplete subset; and since we are study- metrics.

299



3.1 Absolute intrinsic evaluation distance in the embedding space as we show later
For the absolute intrinsic evaluation, we used the and should be explicitly considered.
same datasets and tasks as Baroni et al. (2014). Metric aggregation. The main conceptual
While we present results on all tasks for complete- shortcoming of using correlation-based metrics is
ness, we will mainly focus on relatedness in this that they aggregate scores of different pairs —
section. There are four broad categories: even though these scores can vary greatly in the

embedding space. We can view the relatedness
• Relatedness: These datasets contain relat- task as the task of evaluating a set of rankings,

edness scores for pairs of words; the cosine similar to ranking evaluation in Information Re-
similarity of the embeddings for two words trieval. More specifically, we have one query for
should have high correlation (Spearman or each unique query word w and rank all remaining
Pearson) with human relatedness scores. words v in the vocabulary accordingly. The prob-

lem now is that we usually cannot directly com-
• Analogy: This task was popularized by pare scores from different rankings (Aslam and

Mikolov et al. (2013a). The goal is to find Montague, 2001) as their scores are not guaran-
a term x for a given term y so that x : y best teed to have the same ranges. An even worse case
resembles a sample relationship a : b. is the following scenario. Assume we use rank
• Categorization: Here, the goal is to re- correlation as our metric. As a consequence, we

cover a clustering of words into different cat- need our gold ranking to define an order on all
egories. To do this, the corresponding word the word pairs. However, this also means that we
vectors of all words in a dataset are clustered somehow need to order completely unrelated word
and the purity of the returned clusters is com- pairs; for example, we have to decide whether
puted with respect to the labeled dataset. (dog, cat) is more similar than (banana, apple).

• Selectional preference 3.2 Absolute results
: The goal is to deter-

mine how typical a noun is for a verb either Table 1 presents the results on 14 different datasets
as a subject or as an object (e.g., people eat, for the six embedding models. We excluded ex-
but we rarely eat people). We follow the pro- amples from datasets that contained words not in
cedure that is outlined in Baroni et al. (2014). our vocabulary. For the relatedness and selective

preference tasks, the numbers in the table indicate
Several important design questions come up the correlation coefficient of human scores and the

when designing reusable datasets for evaluating cosine similarity times 100. The numbers for the
relatedness. While we focus mainly on challenges categorization tasks reflect the purities of the re-
that arise in the relatedness evaluation task, many sulting clusters. For the analogy task, we report
of the questions discussed also apply to other sce- accuracy.
narios. CBOW outperforms other embeddings on 10 of

Query inventory. How we pick the word 14 datasets. CBOW especially excels at the relat-
pairs to evaluate affects the results of the evalu- edness and analogy tasks, but fails to surpass other
ation. The commonly-used WordSim-353 dataset models on the selective preferences tasks. Ran-
(Finkelstein et al., 2002), for example, only tries dom projection performs worst in 13 out of the
to have word pairs with a diverse set of similarity 14 tasks, being followed by Hellinger PCA. C&W
scores. The more recent MEN dataset (Bruni et and TSCCA are similar on average, but differ
al., 2014) follows a similar strategy, but restricts across datasets. Moreover, although TSCCA and
queries to words that occur as annotations in an GloVe perform similarly on most tasks, TSCCA
image dataset. However, there are more important suffers disproportionally on the analogy tasks.
criteria that should be considered in order to cre-
ate a diverse dataset: (i) the frequency of the words 3.3 Comparative intrinsic evaluation
in the English language (ii) the parts of speech of In comparative evaluation, users give direct feed-
the words and (iii) abstractness vs. concreteness back on the embeddings themselves, so we do not
of the terms. Not only is frequency important be- have to define a metric that compares scored word
cause we want to test the quality of embeddings pairs. Rather than defining both query and target
on rare words, but also because it is related with words, we need only choose query words since the

300



relatedness categorization sel. prefs analogy
rg ws wss wsr men toefl ap esslli batt. up mcrae an ansyn ansem average

CBOW 74.0 64.0 71.5 56.5 70.7 66.7 65.9 70.5 85.2 24.1 13.9 52.2 47.8 57.6 58.6
GloVe 63.7 54.8 65.8 49.6 64.6 69.4 64.1 65.9 77.8 27.0 18.4 42.2 44.2 39.7 53.4

TSCCA 57.8 54.4 64.7 43.3 56.7 58.3 57.5 70.5 64.2 31.0 14.4 15.5 19.0 11.1 44.2
C&W 48.1 49.8 60.7 40.1 57.5 66.7 60.6 61.4 80.2 28.3 16.0 10.9 12.2 9.3 43.0

H-PCA 19.8 32.9 43.6 15.1 21.3 54.2 34.1 50.0 42.0 -2.5 3.2 3.0 2.4 3.7 23.1
Rand. Proj. 17.1 19.5 24.9 16.1 11.3 51.4 21.9 38.6 29.6 -8.5 1.2 1.0 0.3 1.9 16.2

Table 1: Results on absolute intrinsic evaluation. The best result for each dataset is highlighted in bold.
The second row contains the names of the corresponding datasets.

embeddings themselves will be used to define the provided as an alternative. Table 2 shows an exam-
comparable target words. ple instance that was given to the Turkers.

Query inventory. We compiled a diverse in-
ventory of 100 query words that balance fre- Query: skillfully
quency, part of speech (POS), and concreteness. (a) swiftly (b) expertly
First, we selected 10 out of 45 broad categories (c) cleverly (d) pointedly
from WordNet (Miller, 1995). We then chose an
equal number of categories that mostly contained Table 2: Example instance of comparative in-
abstract concepts and categories that referred to trinsic evaluation task. The presented options in
concrete concepts. Among those categories, we this example are nearest neighbors to the query
had one for adjectives and adverbs each, and four word according to (a) C&W, (b) CBOW, GloVe,
for nouns and verbs each. From each category, TSCCA (c) Rand. Proj. and (d) H-PCA.
we drew ten random words with the restriction
that there be exactly three rare words (i.e., occur- The combination of 100 query words and 3
ring fewer than 2500 times in the training corpus) ranks yielded 300 items on which we solicited
among the ten. judgements by a median of 7 Turkers (min=5,

max=14). We compare embeddings by average
Details. Our experiments were performed with win ratio, where the win ratio was how many times

users from Amazon Mechanical Turk (MTurk) raters chose embedding e divided by the number
that were native speakers of English with sufficient of total ratings for item i.
experience and positive feedback on the Amazon
Mechanical Turk framework. 3.4 Comparative results

For each of the 100 query words in the dataset, Overall comparative results replicate previous re-
the nearest neighbors at ranks k ∈ {1, 5, 50} for sults. Figure 1(a) shows normalized win ratio
the six embeddings were retrieved. For each query scores for each embedding across 3 conditions
word and k, we presented the six words along with corresponding to the frequency of the query word
the query word to the users. Each Turker was re- in the training corpus. The scores were normal-
quested to evaluate between 25 and 50 items per ized to sum to one in each condition to emphasize
task, where an item corresponds to the query word relative differences. CBOW in general performed
and the set of 6 retrieved neighbor words from the best and random projection the worst (p-value
each of the 6 embeddings. The payment was be- < 0.05 for all pairs except H-PCA and C&W in
tween $0.01 and $0.02 per item. The users were comparing un-normalized score differences for the
then asked to pick the word that is most similar ac- ALL-FREQ condition with a randomized permuta-
cording to their perception (the instructions were tion test). The novel comparative evaluations cor-
almost identical to the WordSim-353 dataset in- respond both in rank and in relative margins to
structions). Duplicate words were consolidated, those shown in Table 1.
and a click was counted for all embeddings that Unlike previous results, we can now show
returned that word. An option “I don’t know the differences beyond the nearest neighbors. Fig-
meaning of one (or several) of the words” was also ure 1(b) presents the same results, but this time

301



0.40 0.40
freq  2500 1 NN

0.35
freq 0.35

> 2500 5 NN
0.30 all freq 0.30 50 NN

0.25 0.25

0.20 0.20

0.15 0.15

0.10 0.10

0.05 0.05

0.00 0.00
Rand. Proj H-PCA C&W TSCCA GloVe CBOW Rand. Proj H-PCA C&W TSCCA GloVe CBOW

(a) Normalized scores by global word frequency. (b) Normalized scores by nearest neighbor rank k.

0.40 0.40
adj POS abstract

0.35 adv POS 0.35 concrete

0.30 noun POS 0.30
verb POS

0.25 0.25

0.20 0.20

0.15 0.15

0.10 0.10

0.05 0.05

0.00 0.00
Rand. Proj H-PCA C&W TSCCA GloVe CBOW Rand. Proj H-PCA C&W TSCCA GloVe CBOW

(c) Normalized scores by part of speech. (d) Normalized scores by category.

Figure 1: Direct comparison task

broken up by the rank k of the neighbors that were variation between embedding methods in hierar-
compared. CBOW has its strengths especially at chical classification tasks (Levy et al., 2015b). We
rank k = 1. For neighbors that appear after that, take the two observations above as evidence that a
CBOW does not necessarily produce better em- more fine-grained analysis is necessary in discern-
beddings. In fact, it even does worse for k = 50 ing different embedding methods.
than GloVe. It is important to note, however, that As a by-product, we observed that there was
we cannot make absolute statements about how no embedding method that consistently performed
performance behaves across different values of k best on all of the four different absolute evaluation
since each assessment is always relative to the tasks. However, we would like to reiterate that our
quality of all other embeddings. goal is not to identify one best method, but rather

We balanced our query inventory also with re- point out that different evaluations (e.g., changing
spect to parts of speech and abstractness vs. con- the rank k of the nearest neighbors in the compar-
creteness. Figure 1(c) shows the relative per- ison task) result in different outcomes.
formances of all embeddings for the four POS 4 Coherence
classes (adjectives, adverbs, nouns and verbs).
While most embeddings show relatively homoge- In the relatedness task we measure whether a pair
neous behaviour across the four classes, GloVe of semantically similar words are near each other
suffers disproportionally on adverbs. Moving on in the embedding space. In this novel coherence
to Figure 1(d), we can see a similar behavior for task we assess whether groups of words in a small
TSCCA: Its performance is much lower on con- neighborhood in the embedding space are mutu-
crete words than on abstract ones. This differ- ally related. Previous work has used this property
ence may be important, as recent related work for qualitative evaluation using visualizations of
finds that simply differentiating between general 2D projections (Turian et al., 2010), but we are not
and specific terms explains much of the observed aware of any work using local neighborhoods for

302

Score Score

Score Score



quantitative evaluation. Good embeddings should
0.7

have coherent neighborhoods for each word, so freq  2500
freq > 2500

inserting a word not belonging to this neighbor- 0.6
all freq

hood should be easy to spot. Similar to Chang et 0.5

al. (2009), we presented Turkers with four words, 0.4

three of which are close neighbors and one of
0.3

which is an “intruder.” For each of the 100 words
in our query set of Section 3.3, we retrieved the 0.2

two nearest neighbors. These words along with the 0.1

query word defined the set of (supposedly) good 0.0
Rand. Proj H-PCA C&W GloVe CBOW TSCCA

options. Table 3 shows an example instance that
was given to the Turkers.

Figure 2: Intrusion task: average precision by
(a) finally (b) eventually global word frequency.
(c) immediately (d) put

Table 3: Example instance of intrusion task. The son task are correlated. CBOW and C&W seem
query word is option (a), intruder is (d). to do equally well on rare and frequent words,

whereas the other models’ performance suffers on
To normalize for frequency-based effects, we rare words.

computed the average frequency avg of the three Discussion. Evaluation of set-based properties
words in this set and chose the intruder word to be of embeddings may produce different results from
the first word that had a frequency of avg ± 500 item-based evaluation: rankings we got from the
starting at rank 100 of the list of nearest neighbors. intrusion task did not match the rankings we ob-

Results. tained from the relatedness task. Pairwise similar-
In total, we solicited judgments on 600

ities seem to be only part of the information that is
items (100 query words for each of the 6 em-

encoded in word embeddings, so looking at more
beddings) from a median of 7 Turkers (min=4,

global measures is necessary for a better under-
max=11) per item, where each Turker evaluated

standing of differences between embeddings.
between 25 and 50 items per task. Figure 2 shows
the results of the intrusion experiment. The evalu- We choose intruder words based on similar but

ation measure is micro-averaged precision for an lower-ranked words, so an embedding could score

embedding across 100 query words, where per- well on this task by doing an unusually bad job

item precision is defined as the number of raters at returning less-closely related words. However,

that discovered the intruder divided the total num- the results in Figure 1(b) suggest that there is lit-

ber of raters of item i. Random guessing would tle differences at higher ranks (rank 50) between

achieve an average precision of 0.25. embeddings.

All embeddings perform better than guessing, 5 Extrinsic Tasks
indicating that there is at least some coherent
structure captured in all of them. However, Extrinsic evaluations measure the contribution of
the best performing embeddings at this task are a word embedding model to a specific task. There
TSCCA, CBOW and GloVe (the precision mean is an implicit assumption in the use of such eval-
differences were not significant under a random uations that there is a consistent, global ranking
permutation test), while TSCCA attains greater of word embedding quality, and that higher qual-
precision (p < 0.05) in relation to C&W, H-PCA ity embeddings will necessarily improve results on
and random projection embeddings. These re- any downstream task. We find that this assumption
sults are in contrast to the direct comparison study, does not hold: different tasks favor different em-
where the performance of TSCCA was found to be beddings. Although these evaluations are useful
significantly worse than that of CBOW. However, in characterizing the relative strengths of different
the order of the last three embeddings remains un- models, we do not recommend that they be used as
changed, implying that performance on the intru- a proxy for a general notion of embedding quality.
sion task and performance on the direct compari-

303

Outlier Detection Precision



dev test p-value the baseline (denoted as BOW here). We expect
that this task will be more sensitive to semantic

Baseline 94.18 93.78 0.000 information than syntactic information.
Rand. Proj. 94.33 93.90 0.006

GloVe 94.28 93.93 0.015 Results. Table 4 shows the average F1-scores
H-PCA 94.48 93.96 0.029 for the chunking task. The p-values were com-

C&W 94.53 94.12 puted using randomization (Yeh, 2000) on the sen-
CBOW 94.32 93.93 0.012 tence level. First, we can observe that adding word

TSCCA 94.53 94.09 0.357 vectors as features results in performance lifts with
all embeddings when compared to the baseline.

Table 4: F1 chunking results using different word The performance of C&W and TSCCA is statis-
embeddings as features. The p-values are with re- tically not significant, and C&W does better than
spect to the best performing method. all the remaining methods at the p = 0.05 level.

Surprisingly, although the performance of Ran-
test p-value dom Projections is still last, the gap to GloVe and

BOW (baseline) 88.90 7.45·10−14 CBOW is now very small. Table 5 shows results
Rand. Proj. 62.95 7.47·10−12 on the sentiment analysis task. We recover a sim-

GloVe 74.87 5.00·10−2 ilar order of embeddings as in the absolute intrin-
H-PCA 69.45 6.06·10−11 sic evaluation, however, the order of TSCCA and

C&W 72.37 1.29·10−7 GloVe is now reversed.
CBOW 75.78 Discussion. Performance on downstream tasks

TSCCA 75.02 7.28·10−4
is not consistent across tasks, and may not be con-

Table 5: F1 sentiment analysis results using differ- sistent with intrinsic evaluations. Comparing per-
ent word embeddings as features. The p-values are formance across tasks may provide insight into
with respect to the best performing embedding. the information encoded by an embedding, but

we should not expect any specific task to act as
a proxy for abstract quality. Furthermore, if good

Noun phrase chunking. First we use a noun downstream performance is really the goal of an
phrase chunking task similar to that used by Turian embedding, we recommend that embeddings be
et al. (2010). The only difference is that we nor- trained specifically to optimize a specific objective
malize all word vectors to unit length, rather than (Lebret and Collobert, 2014).
scaling them with some custom factor, before giv-
ing them to the conditional random field (CRF) 6 Discussion
model as input. We expect that this task will be
more sensitive to syntactic information than to se- We find consistent differences between word em-
mantic information. beddings, despite the fact that they are operating

on the same input data and optimizing arguably
Sentiment classification. Second we use a re- very similar objective functions (Pennington et al.,
cently released dataset for binary sentiment clas- 2014; Levy and Goldberg, 2014). Recent work
sification by Maas et al. (2011). The dataset con- suggests that many apparent performance differ-
tains 50K movie reviews with a balanced distribu- ences on specific tasks are due to a lack of hyper-
tion of binary polarity labels. We evaluate the rel- parameter optimization (Levy et al., 2015a). Dif-
ative performance of word embeddings at this task ferent algorithms are, in fact, encoding surpris-
as follows: we generate embedding-only features ingly different information that may or may not
for each review by computing a linear combina- align with our desired use cases. For example,
tion of word embeddings weighted by the num- we find that embeddings encode differing degrees
ber of times that the word appeared in the review of information about word frequency, even after
(using the same bag-of-words features as Maas length normalization. This result is surprising for
et al. (2011)). A LIBLINEAR logistic regression two reasons. First, many algorithms reserve dis-
model (Fan et al., 2008) with the default parame- tinct “intercept” parameters to absorb frequency-
ters is trained and evaluated using 10 fold cross- based effects. Second, we expect that the ge-
validation. A vanilla bag of words feature set is ometry of the embedding space will be primar-

304



105

1.0

0.9

0.8

0.7 CCA
C&W
CBOW

0.6 GloVe
Rand. Proj
H-PCA

0.5 104

102 103 104 105
100 101 102 103

Word frequency Nearest neighbor rank

Figure 3: Embeddings can accurately predict Figure 4: Avg. word rank by frequency in train-
whether a word is frequent or rare. ing corpus vs. nearest-neighbor rank in the C&W

embedding space.

ily driven by semantics: the relatively small num-
ber of frequent words should be evenly distributed Word frequency information in the embedding
through the space, while large numbers of rare, space also affects cosine similarity. For each of the
specific words should cluster around related, but words in the WordSim-353 dataset, we queried for
more frequent, words. the k = 1000 nearest neighbors. We then looked

We trained a logistic regression model to predict up their frequency ranks in the training corpus and
word frequency categories based on word vectors. averaged those ranks over all the query words. We
The linear classifier was trained to put words ei- found a strong correlation between the frequency
ther in a frequent or rare category, with thresholds of a word and its position in the ranking of near-
varying from 100 to 50,000. At each threshold fre- est neighbors in our experiments. Figure 4 shows
quency, we sampled the training sets to ensure a a power law relationship for C&W embeddings
consistent balance of the label distribution across between a word’s nearest neighbor rank (w.r.t. a
all frequencies. We used length-normalized em- query) and the word’s frequency rank in the train-
beddings, as rare words might have shorter vec- ing corpus (nn-rank ∼ 1000 · corpus-rank0.17).
tors resulting from fewer updates during training This is a concern: the frequency of a word in the
(Turian et al., 2010). We report the mean accuracy language plays a critical role in word processing
and standard deviation (1σ) using five-fold cross- of humans as well (Cattell, 1886). As a conse-
validation at each threshold frequency in Figure 3. quence, we need to explicitly consider word fre-

All word embeddings do better than random, quency as a factor in the experiment design. Also,
suggesting that they contain some frequency in- the above results mean that the commonly-used
formation. GloVe and TSCCA achieve nearly cosine similarity in the embedding space for the
100% accuracy on thresholds up to 1000. Unlike intrinsic tasks gets polluted by frequency-based
all other embeddings, accuracy for C&W embed- effects. We believe that further research should
dings increases for larger threshold values. Fur- address how to better measure linguistic relation-
ther investigation revealed that the weight vector ships between words in the embedding space, e.g.,
direction changes gradually with the threshold fre- by learning a custom metric.
quency — indicating that frequency seems to be
encoded in a smooth way in the embedding space. 7 Related work

Although GloVe and CBOW are the two best Mikolov et al. (2013b) demonstrate that cer-
performing embeddings on the intrinsic tasks, they tain linguistic regularities exist in the embedding
differ vastly in the amount of frequency informa- space. The authors show that by doing simple
tion they encode. As a consequence, we can con- vector arithmetic in the embedding space, one
clude that most of the differences in frequency pre- can solve various syntactic and semantic analogy
diction are not due to intrinsic properties of natu- tasks. This is different to previous work, which
ral language: it is not the case that frequent words phrased the analogy task as a classification prob-
naturally have only frequent neighbors. lem (Turney, 2008). Surprisingly, word embed-

305

Accuracy

Avg. rank by corpus frequency



dings seem to capture even more complex linguis- role. Word frequency also interferes with the
tic properties. Chen et al. (2013) show that word commonly-used cosine similarity measure. We
embeddings even contain information about re- present a novel evaluation framework based on di-
gional spellings (UK vs. US), noun gender and rect comparisons between embeddings that pro-
sentiment polarity. vides more fine-grained analysis and supports sim-

Previous work in evaluation for word embed- ple, crowdsourced relevance judgments. We also
dings can be divided into intrinsic and extrin- present a novel Coherence task that measures our
sic evaluations. Intrinsic evaluations measure the intuition that neighborhoods in the embedding
quality of word vectors by directly measuring space should be semantically or syntactically re-
correlation between semantic relatedness and ge- lated. We find that extrinsic evaluations, although
ometric relatedness, usually through inventories useful for highlighting specific aspects of embed-
of query terms. Focusing on intrinsic measures, ding performance, should not be used as a proxy
Baroni et al. (2014) compare word embeddings for generic quality.
against distributional word vectors on a variety of
query inventories and tasks. Faruqui and Dyer Acknowledgments
(2014) provide a website that allows the automatic This research was funded in part through NSF
evaluation of embeddings on a number of query Award IIS-1513692. We would like to thank
inventories. Gao et al. (2014) publish an improved Alexandra Schofield, Adith Swaminathan and all
query inventory for the analogical reasoning task. other members of the NLP seminar for their help-
Finally, Tsvetkov et al. (2015) propose a new in- ful feedback.
trinsic measure that better correlates with extrinsic
performance. However, all these evaluations are References
done on precollected inventories and mostly lim-
ited to local metrics like relatedness. Jacob Andreas and Dan Klein. 2014. How much do

word embeddings encode about syntax? In ACL:
Extrinsic evaluations use embeddings as fea- Short Papers, pages 822–827.

tures in models for other tasks, such as semantic
role labeling or part-of-speech tagging (Collobert Javed Aslam and Mark Montague. 2001. Models for

metasearch. In SIGIR, pages 276–284.
et al., 2011), and improve the performance of ex-
isting systems (Turian et al., 2010). However, they Marco Baroni, Georgiana Dinu, and Germán
have been less successful at other tasks such as Kruszewski. 2014. Don’t count, predict! a

systematic comparison of context-counting vs.
parsing (Andreas and Klein, 2014). context-predicting semantic vectors. In ACL, pages

More work has been done in unsupervised se- 238–247.
mantic modeling in the context of topic models. Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
One example is the word intrusion task (Chang et 2014. Multimodal distributional semantics. JAIR,
al., 2009), in which annotators are asked to iden- 49:1–47.
tify a random word inserted into the set of high James McKeen Cattell. 1886. The time taken up by
probability words for a given topic. Word embed- cerebral operations. Mind,, (42):220–242.
dings do not produce interpretable dimensions, so
we cannot directly use this method, but we present Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,

Chong Wang, and David Blei. 2009. Reading tea
a related task based on nearest neighbors. Manual leaves: How humans interpret topic models. In
evaluation is expensive and time-consuming, but NIPS, pages 288–296.
other work establishes that automated evaluations Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, and
can closely model human intuitions (Newman et Steven Skiena. 2013. The expressive power of word
al., 2010). embeddings. arXiv preprint: 1408.3456.

8 Conclusions Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from

There are many factors that affect word embed- scratch. JLMR, 12:2493–2537.
ding quality. Standard aggregate evaluations,
while useful, do not present a complete or con- Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster,

and Lyle H. Ungar. 2012. Two step CCA: A
sistent picture. Factors such as word frequency new spectral method for estimating vector models
play a significant and previously unacknowledged of words. In ICML, pages 1551–1558.

306



Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang- David Newman, Jey Han Lau, Karl Grieser, and Tim-
Rui Wang, and Chih-Jen Lin. 2008. LIBLIN- othy Baldwin. 2010. Automatic evaluation of topic
EAR: A library for large linear classification. JLMR, coherence. In HLT-NAACL, pages 100–108.
9:1871–1874.

Jeffrey Pennington, Richard Socher, and Christo-
Manaal Faruqui and Chris Dyer. 2014. Community pher D. Manning. 2014. GloVe: Global vectors for

evaluation and exchange of word vectors at word- word representation. In EMNLP.
vectors.org. In ACL: System Demonstrations.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
Lev Finkelstein, Ehud Rivlin Zach Solan Gadi Wolf- laume Lample, and Chris Dyer. 2015. Evaluation of

man Evgeniy Gabrilovich, Yossi Matias, and Eytan word vector representations by subspace alignment.
Ruppin. 2002. Placing search in context: The con- In EMNLP.
cept revisited. TOIS, 20(1):116–131, January. Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.

Bin Gao, Jiang Bian, and Tie-Yan Liu. 2014. Wor- Word representations: a simple and general method
dRep: A benchmark for research on learning word for semi-supervised learning. In ACL, pages 384–
representations. ICML Workshop on Knowledge- 394.
Powered Deep Learning for Text Mining. Peter D. Turney. 2008. A uniform approach to analo-

gies, synonyms, antonyms, and associations. In
Rémi Lebret and Ronan Collobert. 2014. Word em- COLING, pages 905–912.

beddings through Hellinger PCA. In EACL, pages
482–490. Alexander Yeh. 2000. More accurate tests for the sta-

tistical significance of result differences. In ACL,
Omer Levy and Yoav Goldberg. 2014. Neural word pages 947–953.

embedding as implicit matrix factorization. In NIPS,
pages 2177–2185.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015a.
Improving distributional similarity with lessons
learned from word embeddings. TACL.

Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015b. Do supervised distributional meth-
ods really learn lexical inference relations? In
NAACL.

Ping Li, Trevor J Hastie, and Kenneth W Church.
2006. Very sparse random projections. In KDD,
pages 287–296.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In HLT-ACL, pages 142–150.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In ACL: System Demon-
strations, pages 55–60.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS, pages 3111–3119.

Tomas Mikolov, Wen-Tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751.

George A Miller. 1995. WordNet: a lexical
database for english. Communications of the ACM,
38(11):39–41.

307