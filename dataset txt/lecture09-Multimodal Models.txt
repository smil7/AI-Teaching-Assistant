Computational Natural Language Processing

Multimodal Models

Hamidreza Mahyar
mahyarh@mcmaster.ca



What is multimodality?

In our case, focusing on NLP: text + one or more other modality (images, speech, 
audio, olfaction, others). We’ll mostly focus on images as the other modality.



Why does multimodality matter?
A range of very good reasons:
● Faithfulness: Human experience is multimodal
● Practical: The internet & many applications are multimodal
● Data efficiency and availability:

○ Efficiency: Multimodal data is rich and “high bandwidth” (compared to language; 
quoting LeCun, “an imperfect, incomplete, and low-bandwidth serialization 
protocol for the internal data structures we call thoughts”), so better for learning?

○ Scaling: More data is better, and we’re running out of high quality text data.

Multimodality is one of the main frontiers of the new foundation model revolution.



Multimodal brains
McGurk effect (McGurk & MacDonald, 1976)
https://www.youtube.com/watch?v=2k8fHR9jKVM



Multimodal applications
Let’s say we’re dealing with two modalities – text, and images:
● Retrieval (image <> text)
● Captioning (image -> text)
● Generation (text -> image)
● Visual question answering (image+text -> text)
● Multimodal classification (image+text -> label)
● Better understanding/generation (image+text -> label/text)



Multimodal is hot right now
.. and/but has been “the next big thing” for almost a decade!



Outline
1. Early models
2. Features and fusion
3. Contrastive models
4. Multimodal foundation models
5. Evaluation
6. Beyond images: Other modalities
7. Where to next?



Cross-modal “Visual-Semantic Embeddings”
WSABI (Weston et al 2010), DeVise (Frome et al 2013), 
Cross-Modal Transfer (Socher et al 2013)

Frome et al. 2013

Socher et al. 2013



Multimodal distributional semantics (Bruni et al., 2014)
Algorithm:
● Obtain visual “word vector” via BOVW:

○ Identify keypoints and get their descriptors
○ Cluster these and map to counts

● Concatenate with textual word vector
● Apply SVD to “fuse” information

This approach was shown to lead to better 
word representations on human similarity 
judgment datasets. Bag of visual words (BOVW) illustration. Bruni et al., 2014.



Neural version (KB, 2014; Lazaridou et al., 2015)

Kiela & Bottou, 2014 Lazaridou et al., 2015



Beyond words: Sentence level alignment
Grounded Compositional Semantics (Socher et al., 2013)
Visual-Semantic Embeddings (Kiros et al., 2014; Faghri et al., 2015) 
Visual-Semantic Alignments (Karpathy & Li, 2015)
Grounded Sentence Representations (Kiela et al., 2016)
Hinge/margin-like loss as in WSABI/DeViSE.

Karpathy & Li, 2015 Kiros et al., 2014



Image to text: Captioning
Show and tell (Vinyals et al., 2015)
Show, attend and tell (Xu et al., 2016)

Vinyals et al., 2015

Xu et al., 2016



Attention as visual-semantic alignment



Text to image: Conditional image synthesis
Generative adversarial nets (Goodfellow et al. 2014)

Source: Google

Source: Google Reed et al., 2016



Outline
1. Early models
2. Features and fusion
3. Contrastive models
4. Multimodal foundation models
5. Evaluation
6. Beyond images: Other modalities
7. Where to next?



Problems with multimodality
If it’s so important, why isn’t every system multimodal from first principles?
● One modality can dominate other modalities.
● Additional modalities can add noise.
● Full coverage over modalities is not guaranteed.
● We are (were) not ready.
● It’s complicated.



Features
Featurizing text: Batch_size x Sequence_length x Hidden_size.
Featurizing images:

● Sparse “region” features:
○ Object detectors

● Dense features:
○ ConvNet layer(s) or feature maps
○ Vision Transformer layers

Anderson et al., 2018



Region features
● R-CNN (Girshick et al., 2014); Fast R-CNN (Girshick, 2015); Faster R-CNN 

(Ren et al., 2015); YOLO (you only look once) vX..
Mask R-CNN (He et al., 2018).



“Off the shelf” ConvNet features (Razavian et al., 2014)



Vision Transformers (Dosovitskiy et al., 2020)



FiLM (Perez et al., 2017)
Modulate one modality, layerwise, by the other.
γi,c= fc(xi) 

βi,c= hc(xi)

FiLM(Fi,c | βi,c,γi,c)=γi,cFi,c+βi,c



Outline
1. Early models
2. Features and fusion
3. Contrastive models
4. Multimodal foundation models
5. Evaluation
6. Beyond images: Other modalities
7. Where to next?



CLIP (Radford et al. 2021)
Exact same contrastive loss as earlier, but.. Transformers and *web data*!



CLIP Robustness
IMHO one of the best papers ever written 
in our field: extremely thorough, worth a 
close read.

Generalizes MUCH better →



ALIGN (Jia et al., 2021)
Same idea, but EVEN MORE data (JFT at 1.8B image-text pairs vs CLIP’s 300m).



Aligned datasets
HUGE open source datasets of image-text pairs now exist.
Used to train eg StableDiffusion (Rombach et al., 2022).

https://laion.ai/blog/laion-5b/



Outline
1. Early models
2. Features and fusion
3. Contrastive models
4. Multimodal foundation models
5. Evaluation
6. Beyond images: Other modalities
7. Where to next?



BERT Refresher

How do we make this multimodal? →

Alammar (2018), Illustrated Bert



Visual BERTs: VisualBERT

VisualBERT Li et al. 2019



Visual BERTs: ViLBERT

ViLBERT Lu et al. 2019



Visual BERTs: LXMERT
Learning Cross-Modality Encoder Representations from Transformers

LXMERT Tan & Bansal 2019



Visual BERTs: Supervised Multimodal Bitransformers

MMBT Kiela et al. 2019



Visual BERTs: PixelBert

PixelBert Huang et al. 2020
Misnomer: they mean segment embedding



UNITER

Chen, Yi, Lu, et al. 2020



ViLT (Kim et al. 2021)
Feeding data directly to the transformer.



So many models

Du et al. 2022 VLP Survey



Recommended paper
Bugliarello et al. (2021). Multimodal Pretraining Unmasked: A Meta-Analysis and a 
Unified Framework of Vision-and-Language BERTs. Finding: in the same 
conditions, models perform very similarly.



FLAVA (Singh et al., 2021)
Holistic approach to multimodality.
One foundation model spanning V&L, CV and NLP. 

Jointly pretrained on:

● unimodal text data (CCNews + BookCorpus)
● unimodal image data (ImageNet)
● public paired image-text data (70M)

All data/models are publicly released.



The PMD dataset
● 70M image-text pairs from public sources



Problem to solve



How does FLAVA work?

43



How does FLAVA work?

44



How does FLAVA work?

45



How well does it work?
● On average, over 35 tasks, 

FLAVA obtains impressive 
performance



SimVLM (Wang et al., 2022)
Slowly moving from contrastive/discriminative to generative.



CoCa Contrastive Captioner (Yu et al., 2022)
Best of both (contrastive and generative) worlds.



Frozen (Tsimpoukelli, Menick, Cabi, et al., 2021)
Kind of like MMBT but with a better LLM (T5) and a 
better vision encoder (NF-ResNet).

Multi-Modal Few-Shot Learners!



Flamingo (Alayrac et al., 2022)
80b param model based on Chinchilla. 
Multi-image.



Perceiver Resampler



Gated XATTN
Inject visual info directly into a frozen LM via cross-attention (remember FiLM?).



Why is this funny?
Original image from Karpathy as a 
“visual Turing test” →



BLIP/BLIP2 (Li et al., 2023)
Freeze it all (CLIP-ViT / OPT decoder / FlanT5 encoder-decoder)






Multimodal “Chain of Thought” (Zhang et al., 2023)
Providing a rationale helps give the right answer.



KOSMOS-1 (Huang et al., 2023)
LLMs => MLLMs == FMs



Outline
1. Early models
2. Features and fusion
3. Contrastive models
4. Multimodal foundation models
5. Evaluation
6. Beyond images: Other modalities
7. Where to next?



COCO - Common Objects in Context
Super impactful datasets (Lin et al. 2014; Chen et al. 2015)
Main multimodal tasks:

● Image captioning
● Image-caption retrieval

Similar datasets:
● Flickr30k, ConceptualCaptions, VisualGenome, 

cocodataset.org
SBU, RedCaps, LAION



VQA - Visual Question Answering (Antol et al., 2015)
● The dominant task in vision and language.

○ VQA/VQAv2 citations: 4305+1684
○ COCO Captions: 1647
○ Flickr30k: 1228

● At first the “V” in VQA was found to not matter all that much, so a follow-up 
VQAv2 dataset was created (Goyal et al., 2017).

● There’s also GQA (Hudson & Manning, 2019).



CLEVR (Johnson et al., 2016)
Compositional language and elementary visual reasoning diagnostics in a 
controlled setting.

Hand crafted for measuring compositionality.



Hateful Memes (Kiela et al., 2020)
Motivated by the shortcomings of other V&L datasets: we need something that is 
harder, more realistic, and requires true multimodal reasoning and understanding.

“Mean meme” examples for illustrative purposes – not actually in the dataset



Hateful Memes
Highly trained annotators, so: decent quality but small and expensive
Key concept: benign confounders

A “challenge set” for the community to do zero-shot/finetuning from pretrained



Hateful Memes
Findings in the paper:
● Big gap with human performance.
● Region features

(as opposed to grid) seem to help.
● Earlier fusion is better than middle, 

is better than late.
● Multimodal pretraining doesn’t 

really work.



Hateful Memes Competition
After the paper came a $100k competition on an unseen test set:

Winner characteristics: frameworks matter, SOTA pretrained models, ensembles, 
entities, faces and external knowledge. STILL FAR FROM SOLVED.



Winoground
How good is CLIP really?
Some relevant ideas/findings from NLP:

● Winograd schemas
“The [trophy] doesn't fit in the [suitcase] because it is too [large/small]”

● Word order may not matter all that much



Winoground
● Examples written by linguist experts
● Using Getty Images API
● Simple way to measure by comparing scores
● In some cases, very difficult and requiring 

world knowledge



Winoground Findings
SOTA models often perform below chance.



DALL-E2 on Winoground I



DALL-E2 on Winoground II

STILL NOT SOLVED



Outline
1. Early models
2. Features and fusion
3. Contrastive models
4. Multimodal foundation models
5. Evaluation
6. Beyond images: Other modalities
7. Where to next?



Speech / audio
Can EASILY do another full lecture 
just on this topic.

Recent cool example: Whisper, trained 
on 680,000 hours of multilingual 
multitask data.

We can also just treat audio as vision ;) Radford et al., 2022

Kiela et al., 2017 Bapna et al., 2022



Video and text and audio
MERLOT (Zellers et al., 2021) 
MERLOT Reserve (idem, 2022)



Grounded language learning in simulated environments

Hermann, Hill, et al. 2017 Das et al., 2018



Text to 3D
POINT-E (Nichol, Jun, et al., 2022)



Outline
1. Early models
2. Multimodal fusion
3. Contrastive models
4. Multimodal foundation models
5. Other modalities
6. Evaluation
7. Where to next?



One foundation model to rule them all
There will modality-agnostic foundation models that can read and generate many 
modalities.

These models can be bigger and be trained on vastly more data. Parameters will 
be shared in interesting ways.

Automatic alignment from unpaired unimodal data will become a big topic.



Multimodal scaling laws
We are just beginning to understand multimodal scaling laws, lots of interesting 
work to do here in understanding trade-offs.



Retrieval augmented generative multimodal models
These can be multimodal

This can be multimodal Lewis et al., 2021 This can be multimodal



Better evaluation and benchmarking
We need better measurement.

Kiela et al. 2020

Barbosa-Silva et al. 2022



Thank you!